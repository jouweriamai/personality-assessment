{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Obtaining dependency information for datasets>=2.0.0 from https://files.pythonhosted.org/packages/a0/93/da8a22a292e51ab76f969eb87bda8fd70cc3963b4dd71f67bb92a70a7992/datasets-2.16.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/b7/3a/74a609706ef4430fe6d041a3b8d209882c15440b695e373fe26d48c6f35c/xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (2023.3.0)\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
      "  Obtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/a0/0a/02ac0ae1047d97769003ff4fb8e6717024f3f174a5d13257415aa09e13d9/huggingface_hub-0.20.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from evaluate) (23.0)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
      "  Obtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/69/03/46f112e2e415926bc7bdac2f5366572de0c28cb88051537b25a586b5d881/huggingface_hub-0.20.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Obtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting fsspec[http]>=2021.05.0 (from evaluate)\n",
      "  Obtaining dependency information for fsspec[http]>=2021.05.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Collecting dill (from evaluate)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\guest_jouw\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   -------------------------------------- - 81.9/84.1 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 84.1/84.1 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
      "   ---------------------------------------- 0.0/507.1 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 61.4/507.1 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 194.6/507.1 kB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 317.4/507.1 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 440.3/507.1 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 507.1/507.1 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 92.2/330.1 kB 2.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 143.4/330.1 kB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 307.2/330.1 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 330.1/330.1 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 81.9/166.4 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/135.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 135.4/135.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 92.2/115.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, fsspec, dill, responses, multiprocess, huggingface-hub, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.3.0\n",
      "    Uninstalling fsspec-2023.3.0:\n",
      "      Successfully uninstalled fsspec-2023.3.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed datasets-2.16.0 dill-0.3.7 evaluate-0.4.1 fsspec-2023.10.0 huggingface-hub-0.20.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2023.10.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n",
    "pip install accelerate -U\n",
    "pip install transformers[torch]\n",
    "import tensorflow as tf\n",
    "pip install vader-sentiment\n",
    "pip install transformers\n",
    "pip install --upgrade transformers\n",
    "pip install asyncpraw\n",
    "pip install praw\n",
    "pip install --upgrade asyncpraw\n",
    "pip install vaderSentiment\n",
    "pip install asyncio\n",
    "pip install nest_asyncio\n",
    "pip install torch\n",
    "pip install tensorflow\n",
    "pip install scikit-learn==1.2.2\n",
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA_LAUNCH_BLOCKING\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1702974695942,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "hwKZFEXUiiQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jouweria.Hassan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import asyncpraw\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, AdamW, Trainer, TrainingArguments, Autotokenizer\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split \n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials required for reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1702974458549,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "DYHu5C1_B2EV"
   },
   "outputs": [],
   "source": [
    "#reddit for sync env\n",
    "import praw\n",
    "reddit = praw.Reddit(\n",
    "        client_id=\"Z2yiJf-YyZIUhgAfNbbNbA\", #\"YoraOgKs6olMroUDzLnKpg\",\n",
    "        client_secret=\"uQw0TEpTIZyZj4gNWKRoSD9yXGZESg\", #\"MRYT_LJackNiVH2bPEJLdmIqOSV20w\"\n",
    "        user_agent=\"Scraper 1.0 by /u/Ok-Neighborhood-7690\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Classification into five categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re adjust the sentiment scores:\n",
    "\n",
    "def classify_sentiment(x):\n",
    "    if x >= 0.5:\n",
    "        return \"very positive\"\n",
    "    elif 0.3 <= x < 0.5:\n",
    "        return \"positive\"\n",
    "    elif -0.2 <= x < 0.3:\n",
    "        return \"neutral\"\n",
    "    elif -0.4 <= x < -0.2:    \n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"very negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment graph to show sample for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sentiment_graph(value_list):\n",
    "\n",
    "  # classified_sentiments = [classify_sentiment(x) if x is not None else \"Null\" for x in value_list]\n",
    "  label_counts = Counter(value_list)\n",
    "\n",
    "  labels = list(label_counts.keys())\n",
    "  sizes = list(label_counts.values())\n",
    "\n",
    "  plt.pie(sizes,labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "  plt.axis('equal')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZMAdsDNXafL"
   },
   "source": [
    "## Dataset for sentiment analysis from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ['190tmi6','18to58a','18a5w2p'] #mildlyinfuriating, AITH, AITH\n",
    "\n",
    "def get_dataset(reddit):\n",
    "    \n",
    "    filtered_comments = []\n",
    " \n",
    "    for id in ids:\n",
    "\n",
    "        submission = reddit.submission(id=id)\n",
    "\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comments = submission.comments.list()\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        filtered_comments.extend(comment.body for comment in comments if comment)\n",
    "        print(len(filtered_comments))\n",
    "\n",
    "    sentiment_comments = [classify_sentiment(analyser.polarity_scores(comment)['compound']) for comment in filtered_comments]\n",
    "    \n",
    "    return filtered_comments, sentiment_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4808\n",
      "6768\n",
      "14861\n"
     ]
    }
   ],
   "source": [
    "filtered_comments, sentiment_comments = get_dataset(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate user list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "def get_user_list(reddit, submission_id, limit=100):\n",
    "    user_list = []\n",
    "\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    comments = submission.comments.list()\n",
    "\n",
    "    for comment in comments[:]:\n",
    "      \n",
    "        if comment.author:\n",
    "            user_list.append(comment.author.name)\n",
    "\n",
    "    user_list = list(set(user_list))\n",
    "\n",
    "    return user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = get_user_list(reddit,'18to58a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome-dog-Lucky',\n",
       " 'trust_the_engineer4',\n",
       " 'BenTheBirbs',\n",
       " 'i_was_a_person_once',\n",
       " 'danthetwinight',\n",
       " 'Danternas',\n",
       " 'Fspz',\n",
       " 'thefamousjohnny',\n",
       " 'RippedLikeRamb0',\n",
       " 'RobSpaghettio',\n",
       " 'ThereGoesBill8',\n",
       " 'craig536',\n",
       " 'Glyphron',\n",
       " '34Loafs',\n",
       " 'NakedAndAfraidFan',\n",
       " 'rockdash',\n",
       " 'santtu_',\n",
       " 'Konstantinos_gok',\n",
       " 'Incredible-Fella',\n",
       " 'spilltheteasis_',\n",
       " 'm-sims14',\n",
       " 'TheHorizonLies',\n",
       " 'Kooky_Base_5718',\n",
       " 'SuperIngaMMXXII',\n",
       " 'poonman1234',\n",
       " 'HoPe-For-ScOrE',\n",
       " 'GregorioBue',\n",
       " 'Development-Feisty',\n",
       " 'zingding212',\n",
       " 'Croanthos',\n",
       " 'RedditLostOldAccount',\n",
       " 'mr_lockwork',\n",
       " 'Blinddog2502',\n",
       " 'Burning-Skull117',\n",
       " 'fleeingcyber',\n",
       " 'No_Inside_1738',\n",
       " 'Otherwise-Parsnip-91',\n",
       " 'Previous-Ad-7339',\n",
       " 'Tay0310',\n",
       " 'thatdemigoddude',\n",
       " 'Effective-Help4293',\n",
       " 'Quintonog63',\n",
       " 'RunFromFaxai',\n",
       " 'themostreasonableman',\n",
       " 'UtahGrantWriting',\n",
       " 'tommysoprono',\n",
       " 'incorp0real13',\n",
       " 'JustGingy95',\n",
       " 'PumpkinSpice2Nice',\n",
       " 'Grouchy_Chard8522',\n",
       " 'No-Veterinarian9666',\n",
       " 'VanGoghPro',\n",
       " 'rogaldorn88888',\n",
       " 'slothpyle',\n",
       " 'Greenfire32',\n",
       " 'Middi_the_ndder',\n",
       " 'EnergyZealousideal33',\n",
       " 'PerspectiveNo1519',\n",
       " 'offbrandchaoticoats',\n",
       " 'rickoftheuniverse',\n",
       " 'AltruisticCucumber58',\n",
       " 'cs197',\n",
       " 'Foiled_Foliage',\n",
       " 'k_br3w',\n",
       " 'Stopikingonme',\n",
       " 'Jynxbrand',\n",
       " 'Waitn4ehUsername',\n",
       " 'Ok-Signature-4445',\n",
       " 'BlueLivesDontExist84',\n",
       " 'JrRiggles',\n",
       " 'RojerLockless',\n",
       " 'DrChungusM_D',\n",
       " 'BitOneZero',\n",
       " 'Rtroism',\n",
       " 'That-redhead-artist',\n",
       " 'climatelurker',\n",
       " 'Honey-and-Venom',\n",
       " 'Evipicc',\n",
       " 'LupusDeusMagnus',\n",
       " 'LUNA_FOOD',\n",
       " 'Cultural-Stick',\n",
       " 'lazylagom',\n",
       " 'iviethod',\n",
       " 'salsagev8',\n",
       " 'VeryThicknLong',\n",
       " 'tbmny',\n",
       " 'fanamana',\n",
       " 'Next_Celebration_553',\n",
       " 'MouthAvailable',\n",
       " 'wileyhammer',\n",
       " 'JohnnyRelentless',\n",
       " 'MotorTentacle',\n",
       " 'KeshaCow',\n",
       " 'Greedyfox7',\n",
       " '600DLorBust',\n",
       " 'lisabryan',\n",
       " 'Budget_Guava',\n",
       " 'Linetrash406',\n",
       " 'iamsorryy',\n",
       " 'S4Waccount']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user__list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing data from a post in one of the subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1702974462160,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "1SMoA_rDU7-U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong, so skipping to next user\n",
      "Something went wrong, so skipping to next user\n",
      "Something went wrong, so skipping to next user\n",
      "Something went wrong, so skipping to next user\n",
      "Something went wrong, so skipping to next user\n",
      "Something went wrong, so skipping to next user\n"
     ]
    }
   ],
   "source": [
    "from prawcore.exceptions import Forbidden\n",
    "\n",
    "users = {}\n",
    "\n",
    "for username in user__list:\n",
    "    \n",
    "    user = reddit.redditor(username)\n",
    "    weeks = []\n",
    "\n",
    "    for i in range(4):\n",
    "\n",
    "        timestamp_week_ago = int((datetime.utcnow() - timedelta(weeks=i + 1)).timestamp())\n",
    "        timestamp_current = int((datetime.utcnow() - timedelta(weeks=i)).timestamp())\n",
    "        \n",
    "        try:\n",
    "            comments = user.comments.new(limit=None)\n",
    "            filtered_comments = [comment.body for comment in comments if timestamp_week_ago <= comment.created_utc <= timestamp_current]\n",
    "        \n",
    "        except:\n",
    "            print(\"Something went wrong, so skipping to next user\")\n",
    "            break\n",
    "        \n",
    "        count_comments = len(filtered_comments)\n",
    "        \n",
    "        weeks.append(count_comments)\n",
    "        \n",
    "    users[username] = weeks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'awesome-dog-Lucky': [],\n",
       " 'trust_the_engineer4': [1, 0, 0, 0],\n",
       " 'BenTheBirbs': [18, 5, 23, 14],\n",
       " 'i_was_a_person_once': [88, 125, 140, 70],\n",
       " 'danthetwinight': [1, 2, 2, 0],\n",
       " 'Danternas': [29, 10, 18, 3],\n",
       " 'Fspz': [44, 19, 19, 37],\n",
       " 'thefamousjohnny': [55, 34, 93, 35],\n",
       " 'RippedLikeRamb0': [7, 4, 4, 5],\n",
       " 'RobSpaghettio': [12, 11, 18, 5],\n",
       " 'ThereGoesBill8': [7, 2, 0, 23],\n",
       " 'craig536': [97, 89, 71, 131],\n",
       " 'Glyphron': [18, 36, 10, 40],\n",
       " '34Loafs': [60, 42, 41, 43],\n",
       " 'NakedAndAfraidFan': [24, 27, 16, 18],\n",
       " 'rockdash': [24, 31, 23, 31],\n",
       " 'santtu_': [125, 94, 90, 90],\n",
       " 'Konstantinos_gok': [0, 0, 1, 10],\n",
       " 'Incredible-Fella': [95, 151, 130, 103],\n",
       " 'spilltheteasis_': [79, 46, 47, 104],\n",
       " 'm-sims14': [10, 7, 15, 18],\n",
       " 'TheHorizonLies': [103, 65, 95, 67],\n",
       " 'Kooky_Base_5718': [0, 0, 3, 0],\n",
       " 'SuperIngaMMXXII': [62, 74, 107, 116],\n",
       " 'poonman1234': [73, 86, 98, 46],\n",
       " 'HoPe-For-ScOrE': [5, 6, 8, 5],\n",
       " 'GregorioBue': [65, 55, 53, 73],\n",
       " 'Development-Feisty': [84, 111, 67, 96],\n",
       " 'zingding212': [17, 14, 21, 32],\n",
       " 'Croanthos': [3, 3, 10, 3],\n",
       " 'RedditLostOldAccount': [6, 1, 5, 2],\n",
       " 'mr_lockwork': [0, 2, 4, 0],\n",
       " 'Blinddog2502': [4, 1, 5, 0],\n",
       " 'Burning-Skull117': [190, 158, 139, 198],\n",
       " 'fleeingcyber': [44, 49, 14, 28],\n",
       " 'No_Inside_1738': [18, 20, 3, 7],\n",
       " 'Otherwise-Parsnip-91': [69, 45, 16, 4],\n",
       " 'Previous-Ad-7339': [],\n",
       " 'Tay0310': [82, 102, 104, 64],\n",
       " 'thatdemigoddude': [0, 1, 1, 6],\n",
       " 'Effective-Help4293': [93, 31, 20, 29],\n",
       " 'Quintonog63': [0, 0, 0, 1],\n",
       " 'RunFromFaxai': [9, 47, 39, 24],\n",
       " 'themostreasonableman': [31, 30, 26, 15],\n",
       " 'UtahGrantWriting': [],\n",
       " 'tommysoprono': [5, 3, 5, 3],\n",
       " 'incorp0real13': [23, 4, 2, 5],\n",
       " 'JustGingy95': [86, 114, 141, 43],\n",
       " 'PumpkinSpice2Nice': [110, 85, 69, 61],\n",
       " 'Grouchy_Chard8522': [19, 5, 6, 15],\n",
       " 'No-Veterinarian9666': [1, 0, 1, 1],\n",
       " 'VanGoghPro': [3, 6, 2, 5],\n",
       " 'rogaldorn88888': [0, 6, 3, 2],\n",
       " 'slothpyle': [],\n",
       " 'Greenfire32': [54, 37, 23, 57],\n",
       " 'Middi_the_ndder': [25, 0, 0, 3],\n",
       " 'EnergyZealousideal33': [0, 0, 0, 0],\n",
       " 'PerspectiveNo1519': [0, 1, 0, 0],\n",
       " 'offbrandchaoticoats': [0, 0, 0, 26],\n",
       " 'rickoftheuniverse': [33, 20, 31, 22],\n",
       " 'AltruisticCucumber58': [8, 40, 12, 8],\n",
       " 'cs197': [8, 4, 10, 15],\n",
       " 'Foiled_Foliage': [29, 23, 44, 43],\n",
       " 'k_br3w': [15, 23, 15, 35],\n",
       " 'Stopikingonme': [146, 199, 271, 221],\n",
       " 'Jynxbrand': [12, 4, 3, 8],\n",
       " 'Waitn4ehUsername': [24, 40, 34, 31],\n",
       " 'Ok-Signature-4445': [16, 70, 147, 30],\n",
       " 'BlueLivesDontExist84': [],\n",
       " 'JrRiggles': [30, 40, 39, 28],\n",
       " 'RojerLockless': [224, 235, 227, 185],\n",
       " 'DrChungusM_D': [27, 5, 22, 17],\n",
       " 'BitOneZero': [],\n",
       " 'Rtroism': [16, 25, 10, 18],\n",
       " 'That-redhead-artist': [28, 23, 24, 27],\n",
       " 'climatelurker': [239, 142, 167, 169],\n",
       " 'Honey-and-Venom': [243, 269, 183, 126],\n",
       " 'Evipicc': [53, 80, 67, 48],\n",
       " 'LupusDeusMagnus': [123, 103, 90, 92],\n",
       " 'LUNA_FOOD': [34, 9, 38, 21],\n",
       " 'Cultural-Stick': [64, 23, 36, 37],\n",
       " 'lazylagom': [813, 167, 0, 0],\n",
       " 'iviethod': [68, 59, 49, 70],\n",
       " 'salsagev8': [0, 0, 0, 41],\n",
       " 'VeryThicknLong': [38, 59, 68, 71],\n",
       " 'tbmny': [15, 10, 7, 22],\n",
       " 'fanamana': [63, 10, 43, 82],\n",
       " 'Next_Celebration_553': [27, 45, 93, 58],\n",
       " 'MouthAvailable': [1, 0, 3, 1],\n",
       " 'wileyhammer': [3, 3, 4, 5],\n",
       " 'JohnnyRelentless': [88, 68, 79, 130],\n",
       " 'MotorTentacle': [17, 13, 37, 31],\n",
       " 'KeshaCow': [510, 484, 0, 0],\n",
       " 'Greedyfox7': [168, 116, 148, 112],\n",
       " '600DLorBust': [46, 49, 35, 36],\n",
       " 'lisabryan': [1, 0, 0, 3],\n",
       " 'Budget_Guava': [19, 9, 5, 4],\n",
       " 'Linetrash406': [9, 27, 9, 34],\n",
       " 'iamsorryy': [10, 9, 2, 8],\n",
       " 'S4Waccount': [39, 67, 57, 87]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_averages = {}\n",
    "for key,val in users.items():\n",
    "    if len(val)==0:\n",
    "        continue\n",
    "    else:\n",
    "        monthly_averages[key] =round(np.mean(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_comments = {}\n",
    "for key,val in users.items():\n",
    "    if len(val)==0:\n",
    "        continue\n",
    "    else:\n",
    "        weekly_comments[key] = val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trust_the_engineer4': 1,\n",
       " 'BenTheBirbs': 18,\n",
       " 'i_was_a_person_once': 88,\n",
       " 'danthetwinight': 1,\n",
       " 'Danternas': 29,\n",
       " 'Fspz': 44,\n",
       " 'thefamousjohnny': 55,\n",
       " 'RippedLikeRamb0': 7,\n",
       " 'RobSpaghettio': 12,\n",
       " 'ThereGoesBill8': 7,\n",
       " 'craig536': 97,\n",
       " 'Glyphron': 18,\n",
       " '34Loafs': 60,\n",
       " 'NakedAndAfraidFan': 24,\n",
       " 'rockdash': 24,\n",
       " 'santtu_': 125,\n",
       " 'Konstantinos_gok': 0,\n",
       " 'Incredible-Fella': 95,\n",
       " 'spilltheteasis_': 79,\n",
       " 'm-sims14': 10,\n",
       " 'TheHorizonLies': 103,\n",
       " 'Kooky_Base_5718': 0,\n",
       " 'SuperIngaMMXXII': 62,\n",
       " 'poonman1234': 73,\n",
       " 'HoPe-For-ScOrE': 5,\n",
       " 'GregorioBue': 65,\n",
       " 'Development-Feisty': 84,\n",
       " 'zingding212': 17,\n",
       " 'Croanthos': 3,\n",
       " 'RedditLostOldAccount': 6,\n",
       " 'mr_lockwork': 0,\n",
       " 'Blinddog2502': 4,\n",
       " 'Burning-Skull117': 190,\n",
       " 'fleeingcyber': 44,\n",
       " 'No_Inside_1738': 18,\n",
       " 'Otherwise-Parsnip-91': 69,\n",
       " 'Tay0310': 82,\n",
       " 'thatdemigoddude': 0,\n",
       " 'Effective-Help4293': 93,\n",
       " 'Quintonog63': 0,\n",
       " 'RunFromFaxai': 9,\n",
       " 'themostreasonableman': 31,\n",
       " 'tommysoprono': 5,\n",
       " 'incorp0real13': 23,\n",
       " 'JustGingy95': 86,\n",
       " 'PumpkinSpice2Nice': 110,\n",
       " 'Grouchy_Chard8522': 19,\n",
       " 'No-Veterinarian9666': 1,\n",
       " 'VanGoghPro': 3,\n",
       " 'rogaldorn88888': 0,\n",
       " 'Greenfire32': 54,\n",
       " 'Middi_the_ndder': 25,\n",
       " 'EnergyZealousideal33': 0,\n",
       " 'PerspectiveNo1519': 0,\n",
       " 'offbrandchaoticoats': 0,\n",
       " 'rickoftheuniverse': 33,\n",
       " 'AltruisticCucumber58': 8,\n",
       " 'cs197': 8,\n",
       " 'Foiled_Foliage': 29,\n",
       " 'k_br3w': 15,\n",
       " 'Stopikingonme': 146,\n",
       " 'Jynxbrand': 12,\n",
       " 'Waitn4ehUsername': 24,\n",
       " 'Ok-Signature-4445': 16,\n",
       " 'JrRiggles': 30,\n",
       " 'RojerLockless': 224,\n",
       " 'DrChungusM_D': 27,\n",
       " 'Rtroism': 16,\n",
       " 'That-redhead-artist': 28,\n",
       " 'climatelurker': 239,\n",
       " 'Honey-and-Venom': 243,\n",
       " 'Evipicc': 53,\n",
       " 'LupusDeusMagnus': 123,\n",
       " 'LUNA_FOOD': 34,\n",
       " 'Cultural-Stick': 64,\n",
       " 'lazylagom': 813,\n",
       " 'iviethod': 68,\n",
       " 'salsagev8': 0,\n",
       " 'VeryThicknLong': 38,\n",
       " 'tbmny': 15,\n",
       " 'fanamana': 63,\n",
       " 'Next_Celebration_553': 27,\n",
       " 'MouthAvailable': 1,\n",
       " 'wileyhammer': 3,\n",
       " 'JohnnyRelentless': 88,\n",
       " 'MotorTentacle': 17,\n",
       " 'KeshaCow': 510,\n",
       " 'Greedyfox7': 168,\n",
       " '600DLorBust': 46,\n",
       " 'lisabryan': 1,\n",
       " 'Budget_Guava': 19,\n",
       " 'Linetrash406': 9,\n",
       " 'iamsorryy': 10,\n",
       " 'S4Waccount': 39}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'users': dict_keys(['trust_the_engineer4', 'BenTheBirbs', 'i_was_a_person_once', 'danthetwinight', 'Danternas', 'Fspz', 'thefamousjohnny', 'RippedLikeRamb0', 'RobSpaghettio', 'ThereGoesBill8', 'craig536', 'Glyphron', '34Loafs', 'NakedAndAfraidFan', 'rockdash', 'santtu_', 'Konstantinos_gok', 'Incredible-Fella', 'spilltheteasis_', 'm-sims14', 'TheHorizonLies', 'Kooky_Base_5718', 'SuperIngaMMXXII', 'poonman1234', 'HoPe-For-ScOrE', 'GregorioBue', 'Development-Feisty', 'zingding212', 'Croanthos', 'RedditLostOldAccount', 'mr_lockwork', 'Blinddog2502', 'Burning-Skull117', 'fleeingcyber', 'No_Inside_1738', 'Otherwise-Parsnip-91', 'Tay0310', 'thatdemigoddude', 'Effective-Help4293', 'Quintonog63', 'RunFromFaxai', 'themostreasonableman', 'tommysoprono', 'incorp0real13', 'JustGingy95', 'PumpkinSpice2Nice', 'Grouchy_Chard8522', 'No-Veterinarian9666', 'VanGoghPro', 'rogaldorn88888', 'Greenfire32', 'Middi_the_ndder', 'EnergyZealousideal33', 'PerspectiveNo1519', 'offbrandchaoticoats', 'rickoftheuniverse', 'AltruisticCucumber58', 'cs197', 'Foiled_Foliage', 'k_br3w', 'Stopikingonme', 'Jynxbrand', 'Waitn4ehUsername', 'Ok-Signature-4445', 'JrRiggles', 'RojerLockless', 'DrChungusM_D', 'Rtroism', 'That-redhead-artist', 'climatelurker', 'Honey-and-Venom', 'Evipicc', 'LupusDeusMagnus', 'LUNA_FOOD', 'Cultural-Stick', 'lazylagom', 'iviethod', 'salsagev8', 'VeryThicknLong', 'tbmny', 'fanamana', 'Next_Celebration_553', 'MouthAvailable', 'wileyhammer', 'JohnnyRelentless', 'MotorTentacle', 'KeshaCow', 'Greedyfox7', '600DLorBust', 'lisabryan', 'Budget_Guava', 'Linetrash406', 'iamsorryy', 'S4Waccount']),\n",
       " 'monthly_averages': dict_values([0, 15, 106, 1, 15, 30, 54, 5, 12, 8, 97, 26, 46, 21, 27, 100, 3, 120, 69, 12, 82, 1, 90, 76, 6, 62, 90, 21, 5, 4, 2, 2, 171, 34, 12, 34, 88, 2, 43, 0, 30, 26, 4, 8, 96, 81, 11, 1, 4, 3, 43, 7, 0, 0, 6, 26, 17, 9, 35, 22, 209, 7, 32, 66, 34, 218, 18, 17, 26, 179, 205, 62, 102, 26, 40, 245, 62, 10, 59, 14, 50, 56, 1, 4, 91, 24, 248, 136, 42, 1, 9, 20, 7, 62])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = monthly_averages.keys()\n",
    "values = monthly_averages.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "print(len(monthly_averages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = list(monthly_averages.keys())\n",
    "values = list(monthly_averages.values())\n",
    "\n",
    "plt.bar(x=range(len(keys)), height=values)\n",
    "plt.xlabel('Users')\n",
    "plt.ylabel('Monthly Averages')\n",
    "plt.title('Avergae number of comments made by users in a given week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average user makes 47 comments on average for any given week\n"
     ]
    }
   ],
   "source": [
    "average_overall = round(np.mean(values))\n",
    "print(f'the average user makes {average_overall} comments on average for any given week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = list(weekly_comments.keys())\n",
    "values = list(weekly_comments.values())\n",
    "\n",
    "plt.bar(x=range(len(keys)), height=values)\n",
    "plt.xlabel('Users')\n",
    "plt.ylabel('Weekly_comments')\n",
    "plt.title('Number of comments made by users in a given week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average user has made 57 comments on this given week\n"
     ]
    }
   ],
   "source": [
    "average_weekly_overall = round(np.mean(values))\n",
    "print(f'the average user has made {average_weekly_overall} comments on this given week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4808\n",
      "6768\n",
      "14861\n"
     ]
    }
   ],
   "source": [
    "filtered_comments, sentiment_comments = get_dataset(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are looking for mods!\\nFeel free to apply b...</td>\n",
       "      <td>very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make a loud sneezing sound as you flick water ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I would crush that foot so hard with my elbow</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holy shit, those are some sharp nails!  Do the...</td>\n",
       "      <td>very negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay thats when I accidentally crush some t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yeah the air hostess is handling that and if n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ask for hot coffee. Accidently spill hot coffee</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ask for some markers or pens and start your tr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm sorry, but I would just say out loud \"lady...</td>\n",
       "      <td>very negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments     sentiments\n",
       "0  We are looking for mods!\\nFeel free to apply b...  very positive\n",
       "1  Make a loud sneezing sound as you flick water ...        neutral\n",
       "2      I would crush that foot so hard with my elbow       negative\n",
       "3  Holy shit, those are some sharp nails!  Do the...  very negative\n",
       "4  Okay thats when I accidentally crush some t...        neutral\n",
       "5                                          [removed]        neutral\n",
       "6  Yeah the air hostess is handling that and if n...        neutral\n",
       "7    Ask for hot coffee. Accidently spill hot coffee        neutral\n",
       "8  Ask for some markers or pens and start your tr...       positive\n",
       "9  I'm sorry, but I would just say out loud \"lady...  very negative"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14861"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How diverse is the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_graph(sentiment_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07932754, 0.51548342, 0.33288454, 0.01211746, 0.06018704])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiments'].value_counts(normalize=True).sort_index().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assigning class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94576408, 0.44855662, 0.88069444, 0.85721015, 0.86777471])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (1-(df['sentiments'].value_counts().sort_index()/len(df))).values\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9207, 0.4845, 0.6671, 0.9879, 0.9398], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = torch.from_numpy(class_weights).float().to('cuda')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uqdENdXxo87"
   },
   "source": [
    "One-hot encoding the labels in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1702975100149,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "EKPhE6u1xoLK",
    "outputId": "a89f5c68-786d-454a-9234-bed0da77aa4c"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "array_predictions = np.array(predictions)\n",
    "array_predictions = array_predictions.reshape(-1, 1)\n",
    "\n",
    "one_hot_encoded = encoder.fit_transform(array_predictions).toarray()\n",
    "\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 3, 1, 1, 1, 1, 2, 3, 2, 1, 1, 4, 2, 1, 4, 1, 2, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 0, 3, 1, 1, 3, 0, 3, 1, 1, 3, 3, 1, 1, 1, 3, 0, 1, 1, 1, 4, 1, 4, 4, 1, 4, 0, 0, 4, 3, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 0, 1, 1, 4, 1, 4, 1, 1, 3, 1, 1, 4, 3, 0, 0, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "class_labels = ['negative','neutral','positive','very negative','very positive']\n",
    "labels = [class_labels.index(pred) for pred in sentiment_comments]\n",
    "print(labels[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(filtered_comments, labels, test_size=0.2, random_state = 100)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9510\n",
      "9510\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9510\n",
      "2973\n",
      "2378\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(testX))\n",
    "print(len(valX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deadlines!',\n",
       " '****',\n",
       " 'thankyou',\n",
       " 'wow yo my boy is really gone! Smh yo! Its bout to be 3 weeks on Sunday!  R.I.P VON',\n",
       " 'happy Mothers day',\n",
       " 'hardly',\n",
       " 'Just discovered a painful sunburn on my right sholder  hopefully it just turns into a tan!',\n",
       " 'love',\n",
       " '->       almost better than th']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize_text(texts, max_length=64):\n",
    "    encoded_texts = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'  \n",
    "    )\n",
    "\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_encoded = tokenize_text(trainX)\n",
    "val_encoded = tokenize_text(valX)\n",
    "test_encoded = tokenize_text(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2026, 24115,  ...,     0,     0,     0],\n",
       "        [  101,  4647,  4903,  ...,     0,     0,     0],\n",
       "        [  101,  1037, 11414,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
       "        [  101,  4165,  2066,  ...,  2054,  2019,   102],\n",
       "        [  101,  4937,  5430,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2378, 64])\n"
     ]
    }
   ],
   "source": [
    "print(val_encoded['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9510\n",
      "9510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15536,  8873,  ...,     0,     0,     0],\n",
       "        [  101,  2008,  1521,  ...,     0,     0,     0],\n",
       "        [  101,  3021, 15536,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3198,  2005,  ...,     0,     0,     0],\n",
       "        [  101, 26202,  2068,  ...,     0,     0,     0],\n",
       "        [  101,  2196,  5293,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training_encoded['input_ids']))\n",
    "print(len(trainY))\n",
    "training_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pytorch and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2331,
     "status": "ok",
     "timestamp": 1702979324556,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "yskIkzZTujod",
    "outputId": "5ad83cd4-8152-4e18-89c7-9acb7eab631b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch: 0, batch: 0, loss: 1.5987275838851929\n",
      "epoch: 0, batch: 1, loss: 1.635967493057251\n",
      "epoch: 0, batch: 2, loss: 1.5382704734802246\n",
      "epoch: 0, batch: 3, loss: 1.607931137084961\n",
      "epoch: 0, batch: 4, loss: 1.6034467220306396\n",
      "epoch: 0, batch: 5, loss: 1.579662561416626\n",
      "epoch: 0, batch: 6, loss: 1.5620733499526978\n",
      "epoch: 0, batch: 7, loss: 1.4721156358718872\n",
      "epoch: 0, batch: 8, loss: 1.4504377841949463\n",
      "epoch: 0, batch: 9, loss: 1.5912206172943115\n",
      "epoch: 0, batch: 10, loss: 1.496316909790039\n",
      "epoch: 0, batch: 11, loss: 1.663092017173767\n",
      "epoch: 0, batch: 12, loss: 1.4262878894805908\n",
      "epoch: 0, batch: 13, loss: 1.6340324878692627\n",
      "epoch: 0, batch: 14, loss: 1.076088309288025\n",
      "epoch: 0, batch: 15, loss: 1.47311270236969\n",
      "epoch: 0, batch: 16, loss: 1.4650251865386963\n",
      "epoch: 0, batch: 17, loss: 2.0912773609161377\n",
      "epoch: 0, batch: 18, loss: 1.4915395975112915\n",
      "epoch: 0, batch: 19, loss: 1.6655323505401611\n",
      "epoch: 0, batch: 20, loss: 1.5129262208938599\n",
      "epoch: 0, batch: 21, loss: 1.308459997177124\n",
      "epoch: 0, batch: 22, loss: 1.708818793296814\n",
      "epoch: 0, batch: 23, loss: 1.4075050354003906\n",
      "epoch: 0, batch: 24, loss: 1.721374750137329\n",
      "epoch: 0, batch: 25, loss: 1.5416014194488525\n",
      "epoch: 0, batch: 26, loss: 1.4025204181671143\n",
      "epoch: 0, batch: 27, loss: 1.450929045677185\n",
      "epoch: 0, batch: 28, loss: 1.4209425449371338\n",
      "epoch: 0, batch: 29, loss: 1.5611721277236938\n",
      "epoch: 0, batch: 30, loss: 1.622804880142212\n",
      "epoch: 0, batch: 31, loss: 1.6986758708953857\n",
      "epoch: 0, batch: 32, loss: 1.5817149877548218\n",
      "epoch: 0, batch: 33, loss: 1.412024974822998\n",
      "epoch: 0, batch: 34, loss: 1.6483867168426514\n",
      "epoch: 0, batch: 35, loss: 1.7256015539169312\n",
      "epoch: 0, batch: 36, loss: 1.4374990463256836\n",
      "epoch: 0, batch: 37, loss: 1.4315969944000244\n",
      "epoch: 0, batch: 38, loss: 1.4248197078704834\n",
      "epoch: 0, batch: 39, loss: 1.6571811437606812\n",
      "epoch: 0, batch: 40, loss: 1.7808796167373657\n",
      "epoch: 0, batch: 41, loss: 1.4907994270324707\n",
      "epoch: 0, batch: 42, loss: 1.3364074230194092\n",
      "epoch: 0, batch: 43, loss: 1.7148841619491577\n",
      "epoch: 0, batch: 44, loss: 1.7186672687530518\n",
      "epoch: 0, batch: 45, loss: 1.1062595844268799\n",
      "epoch: 0, batch: 46, loss: 1.731461763381958\n",
      "epoch: 0, batch: 47, loss: 1.6367391347885132\n",
      "epoch: 0, batch: 48, loss: 1.28167724609375\n",
      "epoch: 0, batch: 49, loss: 1.612378716468811\n",
      "epoch: 0, batch: 50, loss: 1.6280744075775146\n",
      "epoch: 0, batch: 51, loss: 1.6416209936141968\n",
      "epoch: 0, batch: 52, loss: 1.3626811504364014\n",
      "epoch: 0, batch: 53, loss: 1.2838084697723389\n",
      "epoch: 0, batch: 54, loss: 1.4587583541870117\n",
      "epoch: 0, batch: 55, loss: 1.6754868030548096\n",
      "epoch: 0, batch: 56, loss: 1.3600876331329346\n",
      "epoch: 0, batch: 57, loss: 1.5185762643814087\n",
      "epoch: 0, batch: 58, loss: 2.0526976585388184\n",
      "epoch: 0, batch: 59, loss: 1.3763850927352905\n",
      "epoch: 0, batch: 60, loss: 1.3347164392471313\n",
      "epoch: 0, batch: 61, loss: 1.3137462139129639\n",
      "epoch: 0, batch: 62, loss: 1.5882244110107422\n",
      "epoch: 0, batch: 63, loss: 1.09269380569458\n",
      "epoch: 0, batch: 64, loss: 1.368969202041626\n",
      "epoch: 0, batch: 65, loss: 1.3921971321105957\n",
      "epoch: 0, batch: 66, loss: 1.6354548931121826\n",
      "epoch: 0, batch: 67, loss: 1.6984174251556396\n",
      "epoch: 0, batch: 68, loss: 1.492221713066101\n",
      "epoch: 0, batch: 69, loss: 1.326941728591919\n",
      "epoch: 0, batch: 70, loss: 1.716353178024292\n",
      "epoch: 0, batch: 71, loss: 1.2764394283294678\n",
      "epoch: 0, batch: 72, loss: 2.110516309738159\n",
      "epoch: 0, batch: 73, loss: 1.3152247667312622\n",
      "epoch: 0, batch: 74, loss: 2.1136550903320312\n",
      "epoch: 0, batch: 75, loss: 1.2281808853149414\n",
      "epoch: 0, batch: 76, loss: 1.5514442920684814\n",
      "epoch: 0, batch: 77, loss: 1.3503211736679077\n",
      "epoch: 0, batch: 78, loss: 1.386391282081604\n",
      "epoch: 0, batch: 79, loss: 1.7069776058197021\n",
      "epoch: 0, batch: 80, loss: 1.4054378271102905\n",
      "epoch: 0, batch: 81, loss: 1.9214563369750977\n",
      "epoch: 0, batch: 82, loss: 1.4633369445800781\n",
      "epoch: 0, batch: 83, loss: 1.812591552734375\n",
      "epoch: 0, batch: 84, loss: 1.7881691455841064\n",
      "epoch: 0, batch: 85, loss: 1.3204305171966553\n",
      "epoch: 0, batch: 86, loss: 1.5827327966690063\n",
      "epoch: 0, batch: 87, loss: 1.1958640813827515\n",
      "epoch: 0, batch: 88, loss: 1.6584951877593994\n",
      "epoch: 0, batch: 89, loss: 1.4682731628417969\n",
      "epoch: 0, batch: 90, loss: 1.446592092514038\n",
      "epoch: 0, batch: 91, loss: 1.795714020729065\n",
      "epoch: 0, batch: 92, loss: 1.499901533126831\n",
      "epoch: 0, batch: 93, loss: 2.1944689750671387\n",
      "epoch: 0, batch: 94, loss: 2.0572781562805176\n",
      "epoch: 0, batch: 95, loss: 1.1468408107757568\n",
      "epoch: 0, batch: 96, loss: 1.3267813920974731\n",
      "epoch: 0, batch: 97, loss: 1.6982300281524658\n",
      "epoch: 0, batch: 98, loss: 1.5677216053009033\n",
      "epoch: 0, batch: 99, loss: 1.3287137746810913\n",
      "epoch: 0, batch: 100, loss: 1.1103482246398926\n",
      "epoch: 0, batch: 101, loss: 1.2788865566253662\n",
      "epoch: 0, batch: 102, loss: 1.6378284692764282\n",
      "epoch: 0, batch: 103, loss: 1.299667239189148\n",
      "epoch: 0, batch: 104, loss: 1.2258981466293335\n",
      "epoch: 0, batch: 105, loss: 1.4059513807296753\n",
      "epoch: 0, batch: 106, loss: 1.5496716499328613\n",
      "epoch: 0, batch: 107, loss: 1.8051347732543945\n",
      "epoch: 0, batch: 108, loss: 1.3976513147354126\n",
      "epoch: 0, batch: 109, loss: 1.728205919265747\n",
      "epoch: 0, batch: 110, loss: 1.1236698627471924\n",
      "epoch: 0, batch: 111, loss: 1.325299859046936\n",
      "epoch: 0, batch: 112, loss: 1.2335283756256104\n",
      "epoch: 0, batch: 113, loss: 1.645416021347046\n",
      "epoch: 0, batch: 114, loss: 1.8555352687835693\n",
      "epoch: 0, batch: 115, loss: 1.063565969467163\n",
      "epoch: 0, batch: 116, loss: 1.8060033321380615\n",
      "epoch: 0, batch: 117, loss: 1.5666308403015137\n",
      "epoch: 0, batch: 118, loss: 1.4489210844039917\n",
      "epoch: 0, batch: 119, loss: 1.7004039287567139\n",
      "epoch: 0, batch: 120, loss: 1.9923900365829468\n",
      "epoch: 0, batch: 121, loss: 1.8991016149520874\n",
      "epoch: 0, batch: 122, loss: 1.8191474676132202\n",
      "epoch: 0, batch: 123, loss: 1.292222261428833\n",
      "epoch: 0, batch: 124, loss: 1.6615936756134033\n",
      "epoch: 0, batch: 125, loss: 1.8825080394744873\n",
      "epoch: 0, batch: 126, loss: 1.2515604496002197\n",
      "epoch: 0, batch: 127, loss: 1.296478033065796\n",
      "epoch: 1, batch: 0, loss: 1.3951187133789062\n",
      "epoch: 1, batch: 1, loss: 1.5437418222427368\n",
      "epoch: 1, batch: 2, loss: 1.4428857564926147\n",
      "epoch: 1, batch: 3, loss: 1.4142361879348755\n",
      "epoch: 1, batch: 4, loss: 1.3267496824264526\n",
      "epoch: 1, batch: 5, loss: 1.5870590209960938\n",
      "epoch: 1, batch: 6, loss: 1.5714902877807617\n",
      "epoch: 1, batch: 7, loss: 1.3816016912460327\n",
      "epoch: 1, batch: 8, loss: 1.5825679302215576\n",
      "epoch: 1, batch: 9, loss: 1.582806944847107\n",
      "epoch: 1, batch: 10, loss: 1.755368947982788\n",
      "epoch: 1, batch: 11, loss: 1.4624643325805664\n",
      "epoch: 1, batch: 12, loss: 1.7833051681518555\n",
      "epoch: 1, batch: 13, loss: 1.2826231718063354\n",
      "epoch: 1, batch: 14, loss: 1.373460292816162\n",
      "epoch: 1, batch: 15, loss: 1.4254767894744873\n",
      "epoch: 1, batch: 16, loss: 1.6394710540771484\n",
      "epoch: 1, batch: 17, loss: 1.0834572315216064\n",
      "epoch: 1, batch: 18, loss: 1.584938645362854\n",
      "epoch: 1, batch: 19, loss: 1.362432837486267\n",
      "epoch: 1, batch: 20, loss: 1.4572937488555908\n",
      "epoch: 1, batch: 21, loss: 1.6783443689346313\n",
      "epoch: 1, batch: 22, loss: 1.686950445175171\n",
      "epoch: 1, batch: 23, loss: 2.420192003250122\n",
      "epoch: 1, batch: 24, loss: 2.5292603969573975\n",
      "epoch: 1, batch: 25, loss: 1.6764816045761108\n",
      "epoch: 1, batch: 26, loss: 1.337991714477539\n",
      "epoch: 1, batch: 27, loss: 1.8372701406478882\n",
      "epoch: 1, batch: 28, loss: 1.6614677906036377\n",
      "epoch: 1, batch: 29, loss: 1.309608817100525\n",
      "epoch: 1, batch: 30, loss: 1.3837254047393799\n",
      "epoch: 1, batch: 31, loss: 1.800541639328003\n",
      "epoch: 1, batch: 32, loss: 1.6998770236968994\n",
      "epoch: 1, batch: 33, loss: 1.4591615200042725\n",
      "epoch: 1, batch: 34, loss: 1.3107582330703735\n",
      "epoch: 1, batch: 35, loss: 1.6000697612762451\n",
      "epoch: 1, batch: 36, loss: 1.671558141708374\n",
      "epoch: 1, batch: 37, loss: 1.6803325414657593\n",
      "epoch: 1, batch: 38, loss: 1.8124595880508423\n",
      "epoch: 1, batch: 39, loss: 1.6567665338516235\n",
      "epoch: 1, batch: 40, loss: 1.80721116065979\n",
      "epoch: 1, batch: 41, loss: 1.8985751867294312\n",
      "epoch: 1, batch: 42, loss: 1.5114336013793945\n",
      "epoch: 1, batch: 43, loss: 1.574715495109558\n",
      "epoch: 1, batch: 44, loss: 1.45268976688385\n",
      "epoch: 1, batch: 45, loss: 1.6906534433364868\n",
      "epoch: 1, batch: 46, loss: 1.8798424005508423\n",
      "epoch: 1, batch: 47, loss: 1.392939805984497\n",
      "epoch: 1, batch: 48, loss: 1.7495399713516235\n",
      "epoch: 1, batch: 49, loss: 1.8496145009994507\n",
      "epoch: 1, batch: 50, loss: 1.6266820430755615\n",
      "epoch: 1, batch: 51, loss: 2.5203053951263428\n",
      "epoch: 1, batch: 52, loss: 1.9169642925262451\n",
      "epoch: 1, batch: 53, loss: 1.7392065525054932\n",
      "epoch: 1, batch: 54, loss: 1.7121403217315674\n",
      "epoch: 1, batch: 55, loss: 1.6582448482513428\n",
      "epoch: 1, batch: 56, loss: 1.8712940216064453\n",
      "epoch: 1, batch: 57, loss: 1.6911067962646484\n",
      "epoch: 1, batch: 58, loss: 1.502514362335205\n",
      "epoch: 1, batch: 59, loss: 1.655521035194397\n",
      "epoch: 1, batch: 60, loss: 1.7058721780776978\n",
      "epoch: 1, batch: 61, loss: 1.5164859294891357\n",
      "epoch: 1, batch: 62, loss: 1.4742028713226318\n",
      "epoch: 1, batch: 63, loss: 1.648263931274414\n",
      "epoch: 1, batch: 64, loss: 1.5230433940887451\n",
      "epoch: 1, batch: 65, loss: 1.4688056707382202\n",
      "epoch: 1, batch: 66, loss: 1.5578148365020752\n",
      "epoch: 1, batch: 67, loss: 1.5155348777770996\n",
      "epoch: 1, batch: 68, loss: 1.60104501247406\n",
      "epoch: 1, batch: 69, loss: 1.7254638671875\n",
      "epoch: 1, batch: 70, loss: 1.6110550165176392\n",
      "epoch: 1, batch: 71, loss: 1.3629944324493408\n",
      "epoch: 1, batch: 72, loss: 1.756856918334961\n",
      "epoch: 1, batch: 73, loss: 1.167380928993225\n",
      "epoch: 1, batch: 74, loss: 1.3788137435913086\n",
      "epoch: 1, batch: 75, loss: 1.7004998922348022\n",
      "epoch: 1, batch: 76, loss: 1.8723666667938232\n",
      "epoch: 1, batch: 77, loss: 1.4697802066802979\n",
      "epoch: 1, batch: 78, loss: 1.0394537448883057\n",
      "epoch: 1, batch: 79, loss: 2.0352306365966797\n",
      "epoch: 1, batch: 80, loss: 1.101150393486023\n",
      "epoch: 1, batch: 81, loss: 1.7403290271759033\n",
      "epoch: 1, batch: 82, loss: 2.354034423828125\n",
      "epoch: 1, batch: 83, loss: 2.1884467601776123\n",
      "epoch: 1, batch: 84, loss: 1.9542906284332275\n",
      "epoch: 1, batch: 85, loss: 1.587256669998169\n",
      "epoch: 1, batch: 86, loss: 2.2591490745544434\n",
      "epoch: 1, batch: 87, loss: 1.5613024234771729\n",
      "epoch: 1, batch: 88, loss: 1.5980031490325928\n",
      "epoch: 1, batch: 89, loss: 0.9073628187179565\n",
      "epoch: 1, batch: 90, loss: 1.666577696800232\n",
      "epoch: 1, batch: 91, loss: 2.0322163105010986\n",
      "epoch: 1, batch: 92, loss: 2.0102763175964355\n",
      "epoch: 1, batch: 93, loss: 1.6327130794525146\n",
      "epoch: 1, batch: 94, loss: 1.2247352600097656\n",
      "epoch: 1, batch: 95, loss: 1.3989408016204834\n",
      "epoch: 1, batch: 96, loss: 1.699262261390686\n",
      "epoch: 1, batch: 97, loss: 1.9631913900375366\n",
      "epoch: 1, batch: 98, loss: 1.8938497304916382\n",
      "epoch: 1, batch: 99, loss: 1.2813544273376465\n",
      "epoch: 1, batch: 100, loss: 2.0376367568969727\n",
      "epoch: 1, batch: 101, loss: 1.4956690073013306\n",
      "epoch: 1, batch: 102, loss: 2.004286766052246\n",
      "epoch: 1, batch: 103, loss: 1.3518476486206055\n",
      "epoch: 1, batch: 104, loss: 2.144634485244751\n",
      "epoch: 1, batch: 105, loss: 1.0313538312911987\n",
      "epoch: 1, batch: 106, loss: 1.6401197910308838\n",
      "epoch: 1, batch: 107, loss: 1.1967579126358032\n",
      "epoch: 1, batch: 108, loss: 1.4440603256225586\n",
      "epoch: 1, batch: 109, loss: 1.4474395513534546\n",
      "epoch: 1, batch: 110, loss: 1.431227445602417\n",
      "epoch: 1, batch: 111, loss: 1.8769614696502686\n",
      "epoch: 1, batch: 112, loss: 1.6606149673461914\n",
      "epoch: 1, batch: 113, loss: 1.628535509109497\n",
      "epoch: 1, batch: 114, loss: 1.2137603759765625\n",
      "epoch: 1, batch: 115, loss: 1.4202696084976196\n",
      "epoch: 1, batch: 116, loss: 1.5480167865753174\n",
      "epoch: 1, batch: 117, loss: 1.702911138534546\n",
      "epoch: 1, batch: 118, loss: 1.4643937349319458\n",
      "epoch: 1, batch: 119, loss: 1.5781705379486084\n",
      "epoch: 1, batch: 120, loss: 1.8157577514648438\n",
      "epoch: 1, batch: 121, loss: 1.0260132551193237\n",
      "epoch: 1, batch: 122, loss: 1.7680116891860962\n",
      "epoch: 1, batch: 123, loss: 1.665138602256775\n",
      "epoch: 1, batch: 124, loss: 1.9823106527328491\n",
      "epoch: 1, batch: 125, loss: 1.248423457145691\n",
      "epoch: 1, batch: 126, loss: 1.494354248046875\n",
      "epoch: 1, batch: 127, loss: 1.3132946491241455\n",
      "epoch: 2, batch: 0, loss: 1.4682271480560303\n",
      "epoch: 2, batch: 1, loss: 1.4285855293273926\n",
      "epoch: 2, batch: 2, loss: 1.1968499422073364\n",
      "epoch: 2, batch: 3, loss: 2.02875018119812\n",
      "epoch: 2, batch: 4, loss: 1.2532302141189575\n",
      "epoch: 2, batch: 5, loss: 1.1998224258422852\n",
      "epoch: 2, batch: 6, loss: 1.9613109827041626\n",
      "epoch: 2, batch: 7, loss: 1.5445047616958618\n",
      "epoch: 2, batch: 8, loss: 1.9731624126434326\n",
      "epoch: 2, batch: 9, loss: 1.6543575525283813\n",
      "epoch: 2, batch: 10, loss: 1.4373340606689453\n",
      "epoch: 2, batch: 11, loss: 1.8945629596710205\n",
      "epoch: 2, batch: 12, loss: 1.7633291482925415\n",
      "epoch: 2, batch: 13, loss: 1.6652549505233765\n",
      "epoch: 2, batch: 14, loss: 1.4844353199005127\n",
      "epoch: 2, batch: 15, loss: 1.7094749212265015\n",
      "epoch: 2, batch: 16, loss: 1.6684879064559937\n",
      "epoch: 2, batch: 17, loss: 2.003173351287842\n",
      "epoch: 2, batch: 18, loss: 1.3039439916610718\n",
      "epoch: 2, batch: 19, loss: 1.56919264793396\n",
      "epoch: 2, batch: 20, loss: 1.3300930261611938\n",
      "epoch: 2, batch: 21, loss: 2.2308058738708496\n",
      "epoch: 2, batch: 22, loss: 1.7812646627426147\n",
      "epoch: 2, batch: 23, loss: 1.7385542392730713\n",
      "epoch: 2, batch: 24, loss: 1.9232978820800781\n",
      "epoch: 2, batch: 25, loss: 1.953527808189392\n",
      "epoch: 2, batch: 26, loss: 1.826935052871704\n",
      "epoch: 2, batch: 27, loss: 1.9953845739364624\n",
      "epoch: 2, batch: 28, loss: 1.3873473405838013\n",
      "epoch: 2, batch: 29, loss: 1.4997200965881348\n",
      "epoch: 2, batch: 30, loss: 1.7662664651870728\n",
      "epoch: 2, batch: 31, loss: 1.4566128253936768\n",
      "epoch: 2, batch: 32, loss: 1.4560099840164185\n",
      "epoch: 2, batch: 33, loss: 1.823004126548767\n",
      "epoch: 2, batch: 34, loss: 1.7742048501968384\n",
      "epoch: 2, batch: 35, loss: 1.7115743160247803\n",
      "epoch: 2, batch: 36, loss: 1.5676953792572021\n",
      "epoch: 2, batch: 37, loss: 1.78836989402771\n",
      "epoch: 2, batch: 38, loss: 1.6165058612823486\n",
      "epoch: 2, batch: 39, loss: 1.608022689819336\n",
      "epoch: 2, batch: 40, loss: 1.8050832748413086\n",
      "epoch: 2, batch: 41, loss: 1.6774219274520874\n",
      "epoch: 2, batch: 42, loss: 1.6923803091049194\n",
      "epoch: 2, batch: 43, loss: 1.525565505027771\n",
      "epoch: 2, batch: 44, loss: 1.481741189956665\n",
      "epoch: 2, batch: 45, loss: 1.6748040914535522\n",
      "epoch: 2, batch: 46, loss: 1.6240625381469727\n",
      "epoch: 2, batch: 47, loss: 1.4998271465301514\n",
      "epoch: 2, batch: 48, loss: 1.609310507774353\n",
      "epoch: 2, batch: 49, loss: 1.576294183731079\n",
      "epoch: 2, batch: 50, loss: 1.6805267333984375\n",
      "epoch: 2, batch: 51, loss: 1.7190990447998047\n",
      "epoch: 2, batch: 52, loss: 1.7258994579315186\n",
      "epoch: 2, batch: 53, loss: 1.6364200115203857\n",
      "epoch: 2, batch: 54, loss: 1.5985249280929565\n",
      "epoch: 2, batch: 55, loss: 1.673217535018921\n",
      "epoch: 2, batch: 56, loss: 1.5543328523635864\n",
      "epoch: 2, batch: 57, loss: 1.5943620204925537\n",
      "epoch: 2, batch: 58, loss: 1.471897840499878\n",
      "epoch: 2, batch: 59, loss: 1.7920020818710327\n",
      "epoch: 2, batch: 60, loss: 1.5594947338104248\n",
      "epoch: 2, batch: 61, loss: 1.8682537078857422\n",
      "epoch: 2, batch: 62, loss: 1.5886681079864502\n",
      "epoch: 2, batch: 63, loss: 1.4778802394866943\n",
      "epoch: 2, batch: 64, loss: 1.6874589920043945\n",
      "epoch: 2, batch: 65, loss: 1.7370630502700806\n",
      "epoch: 2, batch: 66, loss: 1.592513918876648\n",
      "epoch: 2, batch: 67, loss: 1.7493396997451782\n",
      "epoch: 2, batch: 68, loss: 1.5600899457931519\n",
      "epoch: 2, batch: 69, loss: 1.6942644119262695\n",
      "epoch: 2, batch: 70, loss: 1.6414315700531006\n",
      "epoch: 2, batch: 71, loss: 1.551149845123291\n",
      "epoch: 2, batch: 72, loss: 1.6575231552124023\n",
      "epoch: 2, batch: 73, loss: 1.77155339717865\n",
      "epoch: 2, batch: 74, loss: 1.8072614669799805\n",
      "epoch: 2, batch: 75, loss: 1.559242606163025\n",
      "epoch: 2, batch: 76, loss: 1.6233112812042236\n",
      "epoch: 2, batch: 77, loss: 1.5124471187591553\n",
      "epoch: 2, batch: 78, loss: 1.5720493793487549\n",
      "epoch: 2, batch: 79, loss: 1.5798311233520508\n",
      "epoch: 2, batch: 80, loss: 1.378070592880249\n",
      "epoch: 2, batch: 81, loss: 1.542189359664917\n",
      "epoch: 2, batch: 82, loss: 1.6087020635604858\n",
      "epoch: 2, batch: 83, loss: 1.6278902292251587\n",
      "epoch: 2, batch: 84, loss: 1.574899673461914\n",
      "epoch: 2, batch: 85, loss: 1.6599994897842407\n",
      "epoch: 2, batch: 86, loss: 1.5338220596313477\n",
      "epoch: 2, batch: 87, loss: 1.5117460489273071\n",
      "epoch: 2, batch: 88, loss: 1.555271029472351\n",
      "epoch: 2, batch: 89, loss: 1.5937659740447998\n",
      "epoch: 2, batch: 90, loss: 1.542559266090393\n",
      "epoch: 2, batch: 91, loss: 1.6862165927886963\n",
      "epoch: 2, batch: 92, loss: 1.5189982652664185\n",
      "epoch: 2, batch: 93, loss: 1.4718754291534424\n",
      "epoch: 2, batch: 94, loss: 1.5422470569610596\n",
      "epoch: 2, batch: 95, loss: 1.6550346612930298\n",
      "epoch: 2, batch: 96, loss: 1.6573116779327393\n",
      "epoch: 2, batch: 97, loss: 1.320833444595337\n",
      "epoch: 2, batch: 98, loss: 1.5392194986343384\n",
      "epoch: 2, batch: 99, loss: 1.893734335899353\n",
      "epoch: 2, batch: 100, loss: 1.3412401676177979\n",
      "epoch: 2, batch: 101, loss: 1.5212527513504028\n",
      "epoch: 2, batch: 102, loss: 1.8974641561508179\n",
      "epoch: 2, batch: 103, loss: 1.3425447940826416\n",
      "epoch: 2, batch: 104, loss: 1.2553346157073975\n",
      "epoch: 2, batch: 105, loss: 1.5737491846084595\n",
      "epoch: 2, batch: 106, loss: 1.620701789855957\n",
      "epoch: 2, batch: 107, loss: 1.5127086639404297\n",
      "epoch: 2, batch: 108, loss: 1.6408348083496094\n",
      "epoch: 2, batch: 109, loss: 1.1506626605987549\n",
      "epoch: 2, batch: 110, loss: 1.3768035173416138\n",
      "epoch: 2, batch: 111, loss: 1.5118290185928345\n",
      "epoch: 2, batch: 112, loss: 1.5967941284179688\n",
      "epoch: 2, batch: 113, loss: 1.1581637859344482\n",
      "epoch: 2, batch: 114, loss: 1.6091747283935547\n",
      "epoch: 2, batch: 115, loss: 1.193236231803894\n",
      "epoch: 2, batch: 116, loss: 1.6591469049453735\n",
      "epoch: 2, batch: 117, loss: 1.7930562496185303\n",
      "epoch: 2, batch: 118, loss: 1.9312469959259033\n",
      "epoch: 2, batch: 119, loss: 1.291290283203125\n",
      "epoch: 2, batch: 120, loss: 1.724107027053833\n",
      "epoch: 2, batch: 121, loss: 1.5244925022125244\n",
      "epoch: 2, batch: 122, loss: 2.0741326808929443\n",
      "epoch: 2, batch: 123, loss: 1.6462570428848267\n",
      "epoch: 2, batch: 124, loss: 1.9772170782089233\n",
      "epoch: 2, batch: 125, loss: 1.5594663619995117\n",
      "epoch: 2, batch: 126, loss: 2.0216450691223145\n",
      "epoch: 2, batch: 127, loss: 1.554100751876831\n",
      "epoch: 3, batch: 0, loss: 1.7038761377334595\n",
      "epoch: 3, batch: 1, loss: 0.8822858929634094\n",
      "epoch: 3, batch: 2, loss: 1.2798194885253906\n",
      "epoch: 3, batch: 3, loss: 0.9341801404953003\n",
      "epoch: 3, batch: 4, loss: 2.0142159461975098\n",
      "epoch: 3, batch: 5, loss: 1.995424509048462\n",
      "epoch: 3, batch: 6, loss: 1.879643201828003\n",
      "epoch: 3, batch: 7, loss: 1.6196033954620361\n",
      "epoch: 3, batch: 8, loss: 1.6281133890151978\n",
      "epoch: 3, batch: 9, loss: 1.8243589401245117\n",
      "epoch: 3, batch: 10, loss: 1.9702043533325195\n",
      "epoch: 3, batch: 11, loss: 1.652040719985962\n",
      "epoch: 3, batch: 12, loss: 1.249692678451538\n",
      "epoch: 3, batch: 13, loss: 1.943926215171814\n",
      "epoch: 3, batch: 14, loss: 0.9534111022949219\n",
      "epoch: 3, batch: 15, loss: 1.4717048406600952\n",
      "epoch: 3, batch: 16, loss: 1.352655053138733\n",
      "epoch: 3, batch: 17, loss: 1.8405803442001343\n",
      "epoch: 3, batch: 18, loss: 0.6246391534805298\n",
      "epoch: 3, batch: 19, loss: 1.0756460428237915\n",
      "epoch: 3, batch: 20, loss: 1.197324514389038\n",
      "epoch: 3, batch: 21, loss: 1.5208382606506348\n",
      "epoch: 3, batch: 22, loss: 0.9586120843887329\n",
      "epoch: 3, batch: 23, loss: 1.6194798946380615\n",
      "epoch: 3, batch: 24, loss: 1.2041261196136475\n",
      "epoch: 3, batch: 25, loss: 1.700893759727478\n",
      "epoch: 3, batch: 26, loss: 1.8188464641571045\n",
      "epoch: 3, batch: 27, loss: 1.1201107501983643\n",
      "epoch: 3, batch: 28, loss: 1.6655057668685913\n",
      "epoch: 3, batch: 29, loss: 1.5318214893341064\n",
      "epoch: 3, batch: 30, loss: 1.5471279621124268\n",
      "epoch: 3, batch: 31, loss: 1.6856892108917236\n",
      "epoch: 3, batch: 32, loss: 2.0461206436157227\n",
      "epoch: 3, batch: 33, loss: 1.417883276939392\n",
      "epoch: 3, batch: 34, loss: 1.6856178045272827\n",
      "epoch: 3, batch: 35, loss: 1.885552167892456\n",
      "epoch: 3, batch: 36, loss: 1.3298360109329224\n",
      "epoch: 3, batch: 37, loss: 1.537186622619629\n",
      "epoch: 3, batch: 38, loss: 1.7950623035430908\n",
      "epoch: 3, batch: 39, loss: 1.0907539129257202\n",
      "epoch: 3, batch: 40, loss: 1.9710609912872314\n",
      "epoch: 3, batch: 41, loss: 1.2814360857009888\n",
      "epoch: 3, batch: 42, loss: 1.6367108821868896\n",
      "epoch: 3, batch: 43, loss: 1.9128425121307373\n",
      "epoch: 3, batch: 44, loss: 1.8440723419189453\n",
      "epoch: 3, batch: 45, loss: 1.7924010753631592\n",
      "epoch: 3, batch: 46, loss: 1.3067137002944946\n",
      "epoch: 3, batch: 47, loss: 1.2301781177520752\n",
      "epoch: 3, batch: 48, loss: 1.6390613317489624\n",
      "epoch: 3, batch: 49, loss: 1.3799527883529663\n",
      "epoch: 3, batch: 50, loss: 1.685887336730957\n",
      "epoch: 3, batch: 51, loss: 1.8153263330459595\n",
      "epoch: 3, batch: 52, loss: 1.413299322128296\n",
      "epoch: 3, batch: 53, loss: 1.5291924476623535\n",
      "epoch: 3, batch: 54, loss: 1.8113586902618408\n",
      "epoch: 3, batch: 55, loss: 1.3993206024169922\n",
      "epoch: 3, batch: 56, loss: 1.476914882659912\n",
      "epoch: 3, batch: 57, loss: 1.3166834115982056\n",
      "epoch: 3, batch: 58, loss: 1.7256778478622437\n",
      "epoch: 3, batch: 59, loss: 1.912293791770935\n",
      "epoch: 3, batch: 60, loss: 1.6285232305526733\n",
      "epoch: 3, batch: 61, loss: 1.811470627784729\n",
      "epoch: 3, batch: 62, loss: 1.437762975692749\n",
      "epoch: 3, batch: 63, loss: 1.5229156017303467\n",
      "epoch: 3, batch: 64, loss: 1.796242117881775\n",
      "epoch: 3, batch: 65, loss: 1.637166976928711\n",
      "epoch: 3, batch: 66, loss: 1.4155309200286865\n",
      "epoch: 3, batch: 67, loss: 1.3714287281036377\n",
      "epoch: 3, batch: 68, loss: 1.3382341861724854\n",
      "epoch: 3, batch: 69, loss: 1.4263224601745605\n",
      "epoch: 3, batch: 70, loss: 1.5211493968963623\n",
      "epoch: 3, batch: 71, loss: 1.6108753681182861\n",
      "epoch: 3, batch: 72, loss: 1.6379470825195312\n",
      "epoch: 3, batch: 73, loss: 1.4215342998504639\n",
      "epoch: 3, batch: 74, loss: 1.3154587745666504\n",
      "epoch: 3, batch: 75, loss: 1.4766838550567627\n",
      "epoch: 3, batch: 76, loss: 1.5729666948318481\n",
      "epoch: 3, batch: 77, loss: 1.442361831665039\n",
      "epoch: 3, batch: 78, loss: 1.354996919631958\n",
      "epoch: 3, batch: 79, loss: 1.5172035694122314\n",
      "epoch: 3, batch: 80, loss: 1.7427585124969482\n",
      "epoch: 3, batch: 81, loss: 1.3334639072418213\n",
      "epoch: 3, batch: 82, loss: 1.8372762203216553\n",
      "epoch: 3, batch: 83, loss: 1.5672301054000854\n",
      "epoch: 3, batch: 84, loss: 1.2018731832504272\n",
      "epoch: 3, batch: 85, loss: 1.4602292776107788\n",
      "epoch: 3, batch: 86, loss: 1.5527244806289673\n",
      "epoch: 3, batch: 87, loss: 1.5422534942626953\n",
      "epoch: 3, batch: 88, loss: 1.3485043048858643\n",
      "epoch: 3, batch: 89, loss: 1.564458966255188\n",
      "epoch: 3, batch: 90, loss: 1.6226354837417603\n",
      "epoch: 3, batch: 91, loss: 1.2040666341781616\n",
      "epoch: 3, batch: 92, loss: 1.6290099620819092\n",
      "epoch: 3, batch: 93, loss: 1.4077844619750977\n",
      "epoch: 3, batch: 94, loss: 1.5833454132080078\n",
      "epoch: 3, batch: 95, loss: 1.6401838064193726\n",
      "epoch: 3, batch: 96, loss: 1.511855125427246\n",
      "epoch: 3, batch: 97, loss: 1.7379093170166016\n",
      "epoch: 3, batch: 98, loss: 1.045019507408142\n",
      "epoch: 3, batch: 99, loss: 1.7386432886123657\n",
      "epoch: 3, batch: 100, loss: 1.7004668712615967\n",
      "epoch: 3, batch: 101, loss: 1.6232242584228516\n",
      "epoch: 3, batch: 102, loss: 1.1594014167785645\n",
      "epoch: 3, batch: 103, loss: 1.4509680271148682\n",
      "epoch: 3, batch: 104, loss: 1.1498146057128906\n",
      "epoch: 3, batch: 105, loss: 1.7067381143569946\n",
      "epoch: 3, batch: 106, loss: 2.0512447357177734\n",
      "epoch: 3, batch: 107, loss: 2.1386208534240723\n",
      "epoch: 3, batch: 108, loss: 1.4623229503631592\n",
      "epoch: 3, batch: 109, loss: 1.7324415445327759\n",
      "epoch: 3, batch: 110, loss: 1.8957058191299438\n",
      "epoch: 3, batch: 111, loss: 2.228210687637329\n",
      "epoch: 3, batch: 112, loss: 2.2079319953918457\n",
      "epoch: 3, batch: 113, loss: 1.2552821636199951\n",
      "epoch: 3, batch: 114, loss: 1.928837776184082\n",
      "epoch: 3, batch: 115, loss: 1.6033509969711304\n",
      "epoch: 3, batch: 116, loss: 1.4398233890533447\n",
      "epoch: 3, batch: 117, loss: 1.9636189937591553\n",
      "epoch: 3, batch: 118, loss: 1.5004944801330566\n",
      "epoch: 3, batch: 119, loss: 1.2861813306808472\n",
      "epoch: 3, batch: 120, loss: 1.8380239009857178\n",
      "epoch: 3, batch: 121, loss: 1.588034987449646\n",
      "epoch: 3, batch: 122, loss: 1.684457778930664\n",
      "epoch: 3, batch: 123, loss: 1.315924882888794\n",
      "epoch: 3, batch: 124, loss: 1.3451331853866577\n",
      "epoch: 3, batch: 125, loss: 1.4846456050872803\n",
      "epoch: 3, batch: 126, loss: 1.6461349725723267\n",
      "epoch: 3, batch: 127, loss: 1.7067070007324219\n",
      "epoch: 4, batch: 0, loss: 1.3340871334075928\n",
      "epoch: 4, batch: 1, loss: 1.5941746234893799\n",
      "epoch: 4, batch: 2, loss: 1.4450063705444336\n",
      "epoch: 4, batch: 3, loss: 2.0117459297180176\n",
      "epoch: 4, batch: 4, loss: 1.3944830894470215\n",
      "epoch: 4, batch: 5, loss: 1.4523077011108398\n",
      "epoch: 4, batch: 6, loss: 1.3799350261688232\n",
      "epoch: 4, batch: 7, loss: 1.6796659231185913\n",
      "epoch: 4, batch: 8, loss: 1.488856554031372\n",
      "epoch: 4, batch: 9, loss: 1.4437662363052368\n",
      "epoch: 4, batch: 10, loss: 1.452199101448059\n",
      "epoch: 4, batch: 11, loss: 1.7684329748153687\n",
      "epoch: 4, batch: 12, loss: 1.6914355754852295\n",
      "epoch: 4, batch: 13, loss: 1.7726936340332031\n",
      "epoch: 4, batch: 14, loss: 1.392793893814087\n",
      "epoch: 4, batch: 15, loss: 1.8785508871078491\n",
      "epoch: 4, batch: 16, loss: 1.677025556564331\n",
      "epoch: 4, batch: 17, loss: 1.4280378818511963\n",
      "epoch: 4, batch: 18, loss: 1.413944959640503\n",
      "epoch: 4, batch: 19, loss: 1.3098915815353394\n",
      "epoch: 4, batch: 20, loss: 2.0099713802337646\n",
      "epoch: 4, batch: 21, loss: 1.465247392654419\n",
      "epoch: 4, batch: 22, loss: 1.3270800113677979\n",
      "epoch: 4, batch: 23, loss: 1.9204736948013306\n",
      "epoch: 4, batch: 24, loss: 1.463393211364746\n",
      "epoch: 4, batch: 25, loss: 1.332908272743225\n",
      "epoch: 4, batch: 26, loss: 1.6973192691802979\n",
      "epoch: 4, batch: 27, loss: 2.023463726043701\n",
      "epoch: 4, batch: 28, loss: 1.3442964553833008\n",
      "epoch: 4, batch: 29, loss: 1.7257301807403564\n",
      "epoch: 4, batch: 30, loss: 1.876879334449768\n",
      "epoch: 4, batch: 31, loss: 1.2057697772979736\n",
      "epoch: 4, batch: 32, loss: 1.915771484375\n",
      "epoch: 4, batch: 33, loss: 1.2007662057876587\n",
      "epoch: 4, batch: 34, loss: 1.9111093282699585\n",
      "epoch: 4, batch: 35, loss: 1.3267810344696045\n",
      "epoch: 4, batch: 36, loss: 1.5986812114715576\n",
      "epoch: 4, batch: 37, loss: 1.5754454135894775\n",
      "epoch: 4, batch: 38, loss: 1.8545866012573242\n",
      "epoch: 4, batch: 39, loss: 1.9982261657714844\n",
      "epoch: 4, batch: 40, loss: 1.8447425365447998\n",
      "epoch: 4, batch: 41, loss: 1.4245834350585938\n",
      "epoch: 4, batch: 42, loss: 1.124699354171753\n",
      "epoch: 4, batch: 43, loss: 1.2799259424209595\n",
      "epoch: 4, batch: 44, loss: 1.4752285480499268\n",
      "epoch: 4, batch: 45, loss: 1.5034282207489014\n",
      "epoch: 4, batch: 46, loss: 1.8374073505401611\n",
      "epoch: 4, batch: 47, loss: 1.839752197265625\n",
      "epoch: 4, batch: 48, loss: 1.496415138244629\n",
      "epoch: 4, batch: 49, loss: 1.4533336162567139\n",
      "epoch: 4, batch: 50, loss: 1.63069748878479\n",
      "epoch: 4, batch: 51, loss: 1.6781730651855469\n",
      "epoch: 4, batch: 52, loss: 1.0987571477890015\n",
      "epoch: 4, batch: 53, loss: 1.3820829391479492\n",
      "epoch: 4, batch: 54, loss: 1.4939898252487183\n",
      "epoch: 4, batch: 55, loss: 1.7025654315948486\n",
      "epoch: 4, batch: 56, loss: 1.6943248510360718\n",
      "epoch: 4, batch: 57, loss: 1.246590256690979\n",
      "epoch: 4, batch: 58, loss: 1.2285687923431396\n",
      "epoch: 4, batch: 59, loss: 1.5580692291259766\n",
      "epoch: 4, batch: 60, loss: 2.006206512451172\n",
      "epoch: 4, batch: 61, loss: 1.593157172203064\n",
      "epoch: 4, batch: 62, loss: 1.3790955543518066\n",
      "epoch: 4, batch: 63, loss: 1.4637655019760132\n",
      "epoch: 4, batch: 64, loss: 1.6613273620605469\n",
      "epoch: 4, batch: 65, loss: 1.670058250427246\n",
      "epoch: 4, batch: 66, loss: 1.4891221523284912\n",
      "epoch: 4, batch: 67, loss: 1.237719178199768\n",
      "epoch: 4, batch: 68, loss: 1.7257534265518188\n",
      "epoch: 4, batch: 69, loss: 1.562393069267273\n",
      "epoch: 4, batch: 70, loss: 1.4451508522033691\n",
      "epoch: 4, batch: 71, loss: 1.6055305004119873\n",
      "epoch: 4, batch: 72, loss: 1.4648189544677734\n",
      "epoch: 4, batch: 73, loss: 1.4322245121002197\n",
      "epoch: 4, batch: 74, loss: 1.490371823310852\n",
      "epoch: 4, batch: 75, loss: 1.4622900485992432\n",
      "epoch: 4, batch: 76, loss: 1.7680164575576782\n",
      "epoch: 4, batch: 77, loss: 1.6003106832504272\n",
      "epoch: 4, batch: 78, loss: 1.4122576713562012\n",
      "epoch: 4, batch: 79, loss: 1.4640495777130127\n",
      "epoch: 4, batch: 80, loss: 1.4844403266906738\n",
      "epoch: 4, batch: 81, loss: 1.6004054546356201\n",
      "epoch: 4, batch: 82, loss: 1.6461302042007446\n",
      "epoch: 4, batch: 83, loss: 1.888200044631958\n",
      "epoch: 4, batch: 84, loss: 1.0837384462356567\n",
      "epoch: 4, batch: 85, loss: 1.187132716178894\n",
      "epoch: 4, batch: 86, loss: 2.015918493270874\n",
      "epoch: 4, batch: 87, loss: 1.4256665706634521\n",
      "epoch: 4, batch: 88, loss: 1.6470777988433838\n",
      "epoch: 4, batch: 89, loss: 1.8575780391693115\n",
      "epoch: 4, batch: 90, loss: 1.1867311000823975\n",
      "epoch: 4, batch: 91, loss: 1.435218095779419\n",
      "epoch: 4, batch: 92, loss: 1.4316575527191162\n",
      "epoch: 4, batch: 93, loss: 1.3240848779678345\n",
      "epoch: 4, batch: 94, loss: 1.6358951330184937\n",
      "epoch: 4, batch: 95, loss: 1.7567981481552124\n",
      "epoch: 4, batch: 96, loss: 1.699573278427124\n",
      "epoch: 4, batch: 97, loss: 1.860260009765625\n",
      "epoch: 4, batch: 98, loss: 1.4063589572906494\n",
      "epoch: 4, batch: 99, loss: 2.00054931640625\n",
      "epoch: 4, batch: 100, loss: 1.282064437866211\n",
      "epoch: 4, batch: 101, loss: 1.360177755355835\n",
      "epoch: 4, batch: 102, loss: 1.4679107666015625\n",
      "epoch: 4, batch: 103, loss: 1.4354608058929443\n",
      "epoch: 4, batch: 104, loss: 1.1313378810882568\n",
      "epoch: 4, batch: 105, loss: 1.4154174327850342\n",
      "epoch: 4, batch: 106, loss: 1.626955270767212\n",
      "epoch: 4, batch: 107, loss: 1.2408034801483154\n",
      "epoch: 4, batch: 108, loss: 1.5065408945083618\n",
      "epoch: 4, batch: 109, loss: 1.873011589050293\n",
      "epoch: 4, batch: 110, loss: 1.7256062030792236\n",
      "epoch: 4, batch: 111, loss: 1.5438120365142822\n",
      "epoch: 4, batch: 112, loss: 1.4993640184402466\n",
      "epoch: 4, batch: 113, loss: 1.700136423110962\n",
      "epoch: 4, batch: 114, loss: 1.2505130767822266\n",
      "epoch: 4, batch: 115, loss: 1.407592535018921\n",
      "epoch: 4, batch: 116, loss: 1.7718591690063477\n",
      "epoch: 4, batch: 117, loss: 1.5459967851638794\n",
      "epoch: 4, batch: 118, loss: 1.5869511365890503\n",
      "epoch: 4, batch: 119, loss: 1.6674238443374634\n",
      "epoch: 4, batch: 120, loss: 1.7520679235458374\n",
      "epoch: 4, batch: 121, loss: 1.8412754535675049\n",
      "epoch: 4, batch: 122, loss: 1.3448126316070557\n",
      "epoch: 4, batch: 123, loss: 1.4634908437728882\n",
      "epoch: 4, batch: 124, loss: 1.7144784927368164\n",
      "epoch: 4, batch: 125, loss: 1.8454935550689697\n",
      "epoch: 4, batch: 126, loss: 1.4518215656280518\n",
      "epoch: 4, batch: 127, loss: 2.0253639221191406\n",
      "epoch: 5, batch: 0, loss: 1.4617011547088623\n",
      "epoch: 5, batch: 1, loss: 2.0064773559570312\n",
      "epoch: 5, batch: 2, loss: 1.5755589008331299\n",
      "epoch: 5, batch: 3, loss: 1.6373611688613892\n",
      "epoch: 5, batch: 4, loss: 1.8745527267456055\n",
      "epoch: 5, batch: 5, loss: 1.4784533977508545\n",
      "epoch: 5, batch: 6, loss: 1.717433214187622\n",
      "epoch: 5, batch: 7, loss: 1.8584076166152954\n",
      "epoch: 5, batch: 8, loss: 1.3719385862350464\n",
      "epoch: 5, batch: 9, loss: 1.5477205514907837\n",
      "epoch: 5, batch: 10, loss: 1.5871100425720215\n",
      "epoch: 5, batch: 11, loss: 1.4469550848007202\n",
      "epoch: 5, batch: 12, loss: 1.4997813701629639\n",
      "epoch: 5, batch: 13, loss: 1.634892463684082\n",
      "epoch: 5, batch: 14, loss: 1.53714919090271\n",
      "epoch: 5, batch: 15, loss: 1.5993974208831787\n",
      "epoch: 5, batch: 16, loss: 1.565301537513733\n",
      "epoch: 5, batch: 17, loss: 1.7572548389434814\n",
      "epoch: 5, batch: 18, loss: 1.5098676681518555\n",
      "epoch: 5, batch: 19, loss: 1.602429747581482\n",
      "epoch: 5, batch: 20, loss: 1.4612762928009033\n",
      "epoch: 5, batch: 21, loss: 1.5500386953353882\n",
      "epoch: 5, batch: 22, loss: 1.756479263305664\n",
      "epoch: 5, batch: 23, loss: 1.550887107849121\n",
      "epoch: 5, batch: 24, loss: 1.44135320186615\n",
      "epoch: 5, batch: 25, loss: 1.282012701034546\n",
      "epoch: 5, batch: 26, loss: 1.4849941730499268\n",
      "epoch: 5, batch: 27, loss: 1.531474232673645\n",
      "epoch: 5, batch: 28, loss: 1.8416194915771484\n",
      "epoch: 5, batch: 29, loss: 1.6766424179077148\n",
      "epoch: 5, batch: 30, loss: 1.8150787353515625\n",
      "epoch: 5, batch: 31, loss: 1.900811791419983\n",
      "epoch: 5, batch: 32, loss: 1.8660125732421875\n",
      "epoch: 5, batch: 33, loss: 1.4050453901290894\n",
      "epoch: 5, batch: 34, loss: 1.1868075132369995\n",
      "epoch: 5, batch: 35, loss: 1.4392181634902954\n",
      "epoch: 5, batch: 36, loss: 1.7208716869354248\n",
      "epoch: 5, batch: 37, loss: 1.4909356832504272\n",
      "epoch: 5, batch: 38, loss: 1.5424710512161255\n",
      "epoch: 5, batch: 39, loss: 1.4742107391357422\n",
      "epoch: 5, batch: 40, loss: 1.7020628452301025\n",
      "epoch: 5, batch: 41, loss: 1.4288685321807861\n",
      "epoch: 5, batch: 42, loss: 1.3012317419052124\n",
      "epoch: 5, batch: 43, loss: 1.6107661724090576\n",
      "epoch: 5, batch: 44, loss: 1.5756815671920776\n",
      "epoch: 5, batch: 45, loss: 1.6629291772842407\n",
      "epoch: 5, batch: 46, loss: 1.432511568069458\n",
      "epoch: 5, batch: 47, loss: 1.7667220830917358\n",
      "epoch: 5, batch: 48, loss: 1.498957872390747\n",
      "epoch: 5, batch: 49, loss: 1.7347466945648193\n",
      "epoch: 5, batch: 50, loss: 1.4803969860076904\n",
      "epoch: 5, batch: 51, loss: 1.3671401739120483\n",
      "epoch: 5, batch: 52, loss: 1.577197790145874\n",
      "epoch: 5, batch: 53, loss: 1.6592750549316406\n",
      "epoch: 5, batch: 54, loss: 1.5381262302398682\n",
      "epoch: 5, batch: 55, loss: 1.4301034212112427\n",
      "epoch: 5, batch: 56, loss: 1.5440540313720703\n",
      "epoch: 5, batch: 57, loss: 1.3711351156234741\n",
      "epoch: 5, batch: 58, loss: 1.6147871017456055\n",
      "epoch: 5, batch: 59, loss: 1.3621675968170166\n",
      "epoch: 5, batch: 60, loss: 1.543643593788147\n",
      "epoch: 5, batch: 61, loss: 1.378014326095581\n",
      "epoch: 5, batch: 62, loss: 1.5521661043167114\n",
      "epoch: 5, batch: 63, loss: 1.48468816280365\n",
      "epoch: 5, batch: 64, loss: 1.3837283849716187\n",
      "epoch: 5, batch: 65, loss: 1.6939105987548828\n",
      "epoch: 5, batch: 66, loss: 1.3978058099746704\n",
      "epoch: 5, batch: 67, loss: 1.611823320388794\n",
      "epoch: 5, batch: 68, loss: 1.4426233768463135\n",
      "epoch: 5, batch: 69, loss: 1.386124849319458\n",
      "epoch: 5, batch: 70, loss: 1.3703399896621704\n",
      "epoch: 5, batch: 71, loss: 1.5822995901107788\n",
      "epoch: 5, batch: 72, loss: 1.394264578819275\n",
      "epoch: 5, batch: 73, loss: 1.581138253211975\n",
      "epoch: 5, batch: 74, loss: 1.631773591041565\n",
      "epoch: 5, batch: 75, loss: 1.5707427263259888\n",
      "epoch: 5, batch: 76, loss: 1.8300682306289673\n",
      "epoch: 5, batch: 77, loss: 1.5753144025802612\n",
      "epoch: 5, batch: 78, loss: 1.6873598098754883\n",
      "epoch: 5, batch: 79, loss: 1.5331789255142212\n",
      "epoch: 5, batch: 80, loss: 1.7312864065170288\n",
      "epoch: 5, batch: 81, loss: 1.4605216979980469\n",
      "epoch: 5, batch: 82, loss: 1.9307386875152588\n",
      "epoch: 5, batch: 83, loss: 1.4598495960235596\n",
      "epoch: 5, batch: 84, loss: 1.4084621667861938\n",
      "epoch: 5, batch: 85, loss: 1.5585743188858032\n",
      "epoch: 5, batch: 86, loss: 1.4849729537963867\n",
      "epoch: 5, batch: 87, loss: 1.4688594341278076\n",
      "epoch: 5, batch: 88, loss: 1.2855066061019897\n",
      "epoch: 5, batch: 89, loss: 1.7013492584228516\n",
      "epoch: 5, batch: 90, loss: 1.3871943950653076\n",
      "epoch: 5, batch: 91, loss: 1.258161187171936\n",
      "epoch: 5, batch: 92, loss: 1.2182724475860596\n",
      "epoch: 5, batch: 93, loss: 1.2731282711029053\n",
      "epoch: 5, batch: 94, loss: 1.5891367197036743\n",
      "epoch: 5, batch: 95, loss: 1.4766396284103394\n",
      "epoch: 5, batch: 96, loss: 1.422007441520691\n",
      "epoch: 5, batch: 97, loss: 1.2564973831176758\n",
      "epoch: 5, batch: 98, loss: 1.236072301864624\n",
      "epoch: 5, batch: 99, loss: 1.5833940505981445\n",
      "epoch: 5, batch: 100, loss: 1.834938406944275\n",
      "epoch: 5, batch: 101, loss: 1.7153384685516357\n",
      "epoch: 5, batch: 102, loss: 1.5030847787857056\n",
      "epoch: 5, batch: 103, loss: 1.3229376077651978\n",
      "epoch: 5, batch: 104, loss: 1.6179863214492798\n",
      "epoch: 5, batch: 105, loss: 1.906663179397583\n",
      "epoch: 5, batch: 106, loss: 1.3027108907699585\n",
      "epoch: 5, batch: 107, loss: 1.6501308679580688\n",
      "epoch: 5, batch: 108, loss: 1.1923866271972656\n",
      "epoch: 5, batch: 109, loss: 1.4267170429229736\n",
      "epoch: 5, batch: 110, loss: 1.8863506317138672\n",
      "epoch: 5, batch: 111, loss: 1.8378944396972656\n",
      "epoch: 5, batch: 112, loss: 1.384279489517212\n",
      "epoch: 5, batch: 113, loss: 1.1159758567810059\n",
      "epoch: 5, batch: 114, loss: 1.5184085369110107\n",
      "epoch: 5, batch: 115, loss: 1.9996144771575928\n",
      "epoch: 5, batch: 116, loss: 1.3962749242782593\n",
      "epoch: 5, batch: 117, loss: 1.3837792873382568\n",
      "epoch: 5, batch: 118, loss: 1.6139070987701416\n",
      "epoch: 5, batch: 119, loss: 0.9589886665344238\n",
      "epoch: 5, batch: 120, loss: 2.0306472778320312\n",
      "epoch: 5, batch: 121, loss: 1.3919317722320557\n",
      "epoch: 5, batch: 122, loss: 2.0455193519592285\n",
      "epoch: 5, batch: 123, loss: 1.5623900890350342\n",
      "epoch: 5, batch: 124, loss: 1.85629141330719\n",
      "epoch: 5, batch: 125, loss: 1.8284438848495483\n",
      "epoch: 5, batch: 126, loss: 1.4604918956756592\n",
      "epoch: 5, batch: 127, loss: 1.3563404083251953\n",
      "epoch: 6, batch: 0, loss: 1.496827244758606\n",
      "epoch: 6, batch: 1, loss: 1.9930121898651123\n",
      "epoch: 6, batch: 2, loss: 1.2897762060165405\n",
      "epoch: 6, batch: 3, loss: 0.9266080856323242\n",
      "epoch: 6, batch: 4, loss: 1.2668087482452393\n",
      "epoch: 6, batch: 5, loss: 1.2004501819610596\n",
      "epoch: 6, batch: 6, loss: 1.1267938613891602\n",
      "epoch: 6, batch: 7, loss: 1.7544835805892944\n",
      "epoch: 6, batch: 8, loss: 1.285491943359375\n",
      "epoch: 6, batch: 9, loss: 1.4477620124816895\n",
      "epoch: 6, batch: 10, loss: 0.9623204469680786\n",
      "epoch: 6, batch: 11, loss: 1.8086780309677124\n",
      "epoch: 6, batch: 12, loss: 1.2245845794677734\n",
      "epoch: 6, batch: 13, loss: 1.8601000308990479\n",
      "epoch: 6, batch: 14, loss: 1.899814248085022\n",
      "epoch: 6, batch: 15, loss: 2.1536428928375244\n",
      "epoch: 6, batch: 16, loss: 1.6366775035858154\n",
      "epoch: 6, batch: 17, loss: 1.7631473541259766\n",
      "epoch: 6, batch: 18, loss: 1.6528396606445312\n",
      "epoch: 6, batch: 19, loss: 1.5865516662597656\n",
      "epoch: 6, batch: 20, loss: 0.9471109509468079\n",
      "epoch: 6, batch: 21, loss: 1.563049554824829\n",
      "epoch: 6, batch: 22, loss: 1.3050068616867065\n",
      "epoch: 6, batch: 23, loss: 1.266162633895874\n",
      "epoch: 6, batch: 24, loss: 1.3427989482879639\n",
      "epoch: 6, batch: 25, loss: 1.726360559463501\n",
      "epoch: 6, batch: 26, loss: 1.5683727264404297\n",
      "epoch: 6, batch: 27, loss: 1.5008487701416016\n",
      "epoch: 6, batch: 28, loss: 1.8562557697296143\n",
      "epoch: 6, batch: 29, loss: 1.7266032695770264\n",
      "epoch: 6, batch: 30, loss: 1.8019778728485107\n",
      "epoch: 6, batch: 31, loss: 2.0080246925354004\n",
      "epoch: 6, batch: 32, loss: 1.8287646770477295\n",
      "epoch: 6, batch: 33, loss: 0.8163009881973267\n",
      "epoch: 6, batch: 34, loss: 1.4824445247650146\n",
      "epoch: 6, batch: 35, loss: 1.360177755355835\n",
      "epoch: 6, batch: 36, loss: 1.3845566511154175\n",
      "epoch: 6, batch: 37, loss: 1.020732045173645\n",
      "epoch: 6, batch: 38, loss: 1.2484614849090576\n",
      "epoch: 6, batch: 39, loss: 1.443534016609192\n",
      "epoch: 6, batch: 40, loss: 1.3188034296035767\n",
      "epoch: 6, batch: 41, loss: 1.5510295629501343\n",
      "epoch: 6, batch: 42, loss: 1.461155652999878\n",
      "epoch: 6, batch: 43, loss: 1.7459999322891235\n",
      "epoch: 6, batch: 44, loss: 1.5956838130950928\n",
      "epoch: 6, batch: 45, loss: 1.5212352275848389\n",
      "epoch: 6, batch: 46, loss: 1.57086980342865\n",
      "epoch: 6, batch: 47, loss: 1.4812180995941162\n",
      "epoch: 6, batch: 48, loss: 1.9324153661727905\n",
      "epoch: 6, batch: 49, loss: 1.4561691284179688\n",
      "epoch: 6, batch: 50, loss: 1.8065379858016968\n",
      "epoch: 6, batch: 51, loss: 1.2062187194824219\n",
      "epoch: 6, batch: 52, loss: 1.3678696155548096\n",
      "epoch: 6, batch: 53, loss: 1.468858242034912\n",
      "epoch: 6, batch: 54, loss: 1.710443139076233\n",
      "epoch: 6, batch: 55, loss: 1.8576176166534424\n",
      "epoch: 6, batch: 56, loss: 1.4318883419036865\n",
      "epoch: 6, batch: 57, loss: 1.7279987335205078\n",
      "epoch: 6, batch: 58, loss: 1.3858468532562256\n",
      "epoch: 6, batch: 59, loss: 1.3088972568511963\n",
      "epoch: 6, batch: 60, loss: 1.7177470922470093\n",
      "epoch: 6, batch: 61, loss: 1.5478719472885132\n",
      "epoch: 6, batch: 62, loss: 1.4160953760147095\n",
      "epoch: 6, batch: 63, loss: 1.9070590734481812\n",
      "epoch: 6, batch: 64, loss: 1.6743080615997314\n",
      "epoch: 6, batch: 65, loss: 1.7089840173721313\n",
      "epoch: 6, batch: 66, loss: 1.6885836124420166\n",
      "epoch: 6, batch: 67, loss: 1.4523017406463623\n",
      "epoch: 6, batch: 68, loss: 1.5799353122711182\n",
      "epoch: 6, batch: 69, loss: 1.4569135904312134\n",
      "epoch: 6, batch: 70, loss: 1.5580251216888428\n",
      "epoch: 6, batch: 71, loss: 1.4509303569793701\n",
      "epoch: 6, batch: 72, loss: 1.7489917278289795\n",
      "epoch: 6, batch: 73, loss: 1.615918755531311\n",
      "epoch: 6, batch: 74, loss: 1.560880422592163\n",
      "epoch: 6, batch: 75, loss: 1.5816028118133545\n",
      "epoch: 6, batch: 76, loss: 1.6583915948867798\n",
      "epoch: 6, batch: 77, loss: 1.6031080484390259\n",
      "epoch: 6, batch: 78, loss: 1.5537707805633545\n",
      "epoch: 6, batch: 79, loss: 1.6140486001968384\n",
      "epoch: 6, batch: 80, loss: 1.6330451965332031\n",
      "epoch: 6, batch: 81, loss: 1.51839280128479\n",
      "epoch: 6, batch: 82, loss: 1.637864112854004\n",
      "epoch: 6, batch: 83, loss: 1.5820419788360596\n",
      "epoch: 6, batch: 84, loss: 1.6401126384735107\n",
      "epoch: 6, batch: 85, loss: 1.5333175659179688\n",
      "epoch: 6, batch: 86, loss: 1.6788314580917358\n",
      "epoch: 6, batch: 87, loss: 1.4157474040985107\n",
      "epoch: 6, batch: 88, loss: 1.5535838603973389\n",
      "epoch: 6, batch: 89, loss: 1.5820989608764648\n",
      "epoch: 6, batch: 90, loss: 1.4828236103057861\n",
      "epoch: 6, batch: 91, loss: 1.5042037963867188\n",
      "epoch: 6, batch: 92, loss: 1.4075565338134766\n",
      "epoch: 6, batch: 93, loss: 1.5751904249191284\n",
      "epoch: 6, batch: 94, loss: 1.6697900295257568\n",
      "epoch: 6, batch: 95, loss: 1.6176738739013672\n",
      "epoch: 6, batch: 96, loss: 1.7151117324829102\n",
      "epoch: 6, batch: 97, loss: 1.714308738708496\n",
      "epoch: 6, batch: 98, loss: 1.6747941970825195\n",
      "epoch: 6, batch: 99, loss: 1.676709771156311\n",
      "epoch: 6, batch: 100, loss: 1.6354135274887085\n",
      "epoch: 6, batch: 101, loss: 1.5308096408843994\n",
      "epoch: 6, batch: 102, loss: 1.3983418941497803\n",
      "epoch: 6, batch: 103, loss: 1.5515445470809937\n",
      "epoch: 6, batch: 104, loss: 1.4929425716400146\n",
      "epoch: 6, batch: 105, loss: 1.6713100671768188\n",
      "epoch: 6, batch: 106, loss: 1.587986707687378\n",
      "epoch: 6, batch: 107, loss: 1.5760656595230103\n",
      "epoch: 6, batch: 108, loss: 1.4551334381103516\n",
      "epoch: 6, batch: 109, loss: 1.483865737915039\n",
      "epoch: 6, batch: 110, loss: 1.6101973056793213\n",
      "epoch: 6, batch: 111, loss: 1.5173343420028687\n",
      "epoch: 6, batch: 112, loss: 1.6224501132965088\n",
      "epoch: 6, batch: 113, loss: 1.4242795705795288\n",
      "epoch: 6, batch: 114, loss: 1.5051428079605103\n",
      "epoch: 6, batch: 115, loss: 1.7963899374008179\n",
      "epoch: 6, batch: 116, loss: 1.4928126335144043\n",
      "epoch: 6, batch: 117, loss: 1.3436073064804077\n",
      "epoch: 6, batch: 118, loss: 1.5597831010818481\n",
      "epoch: 6, batch: 119, loss: 1.4843693971633911\n",
      "epoch: 6, batch: 120, loss: 1.4880491495132446\n",
      "epoch: 6, batch: 121, loss: 1.4053760766983032\n",
      "epoch: 6, batch: 122, loss: 1.481762170791626\n",
      "epoch: 6, batch: 123, loss: 1.3872504234313965\n",
      "epoch: 6, batch: 124, loss: 1.3740475177764893\n",
      "epoch: 6, batch: 125, loss: 1.6646865606307983\n",
      "epoch: 6, batch: 126, loss: 1.5650670528411865\n",
      "epoch: 6, batch: 127, loss: 1.2841699123382568\n",
      "epoch: 7, batch: 0, loss: 1.544148564338684\n",
      "epoch: 7, batch: 1, loss: 1.7422783374786377\n",
      "epoch: 7, batch: 2, loss: 1.4304020404815674\n",
      "epoch: 7, batch: 3, loss: 1.9343799352645874\n",
      "epoch: 7, batch: 4, loss: 1.682643175125122\n",
      "epoch: 7, batch: 5, loss: 1.578133225440979\n",
      "epoch: 7, batch: 6, loss: 1.209110975265503\n",
      "epoch: 7, batch: 7, loss: 1.892778992652893\n",
      "epoch: 7, batch: 8, loss: 1.7049049139022827\n",
      "epoch: 7, batch: 9, loss: 1.5678170919418335\n",
      "epoch: 7, batch: 10, loss: 1.4805777072906494\n",
      "epoch: 7, batch: 11, loss: 1.605146050453186\n",
      "epoch: 7, batch: 12, loss: 1.5463759899139404\n",
      "epoch: 7, batch: 13, loss: 1.2026503086090088\n",
      "epoch: 7, batch: 14, loss: 1.3780429363250732\n",
      "epoch: 7, batch: 15, loss: 1.7720661163330078\n",
      "epoch: 7, batch: 16, loss: 1.5274684429168701\n",
      "epoch: 7, batch: 17, loss: 1.7319285869598389\n",
      "epoch: 7, batch: 18, loss: 1.5304977893829346\n",
      "epoch: 7, batch: 19, loss: 1.6958000659942627\n",
      "epoch: 7, batch: 20, loss: 1.6448001861572266\n",
      "epoch: 7, batch: 21, loss: 1.7873560190200806\n",
      "epoch: 7, batch: 22, loss: 1.2614625692367554\n",
      "epoch: 7, batch: 23, loss: 1.688028335571289\n",
      "epoch: 7, batch: 24, loss: 1.5231748819351196\n",
      "epoch: 7, batch: 25, loss: 1.8785568475723267\n",
      "epoch: 7, batch: 26, loss: 1.3592830896377563\n",
      "epoch: 7, batch: 27, loss: 1.4170109033584595\n",
      "epoch: 7, batch: 28, loss: 1.4411964416503906\n",
      "epoch: 7, batch: 29, loss: 1.4675676822662354\n",
      "epoch: 7, batch: 30, loss: 1.0187782049179077\n",
      "epoch: 7, batch: 31, loss: 1.2139761447906494\n",
      "epoch: 7, batch: 32, loss: 1.2100530862808228\n",
      "epoch: 7, batch: 33, loss: 1.8864530324935913\n",
      "epoch: 7, batch: 34, loss: 1.5676270723342896\n",
      "epoch: 7, batch: 35, loss: 1.450286626815796\n",
      "epoch: 7, batch: 36, loss: 1.4924944639205933\n",
      "epoch: 7, batch: 37, loss: 1.566161036491394\n",
      "epoch: 7, batch: 38, loss: 1.8462984561920166\n",
      "epoch: 7, batch: 39, loss: 1.273763656616211\n",
      "epoch: 7, batch: 40, loss: 1.116652488708496\n",
      "epoch: 7, batch: 41, loss: 1.3906651735305786\n",
      "epoch: 7, batch: 42, loss: 1.5425535440444946\n",
      "epoch: 7, batch: 43, loss: 1.4577792882919312\n",
      "epoch: 7, batch: 44, loss: 1.1800696849822998\n",
      "epoch: 7, batch: 45, loss: 1.6750093698501587\n",
      "epoch: 7, batch: 46, loss: 1.4589025974273682\n",
      "epoch: 7, batch: 47, loss: 1.4516288042068481\n",
      "epoch: 7, batch: 48, loss: 1.6916589736938477\n",
      "epoch: 7, batch: 49, loss: 1.216038703918457\n",
      "epoch: 7, batch: 50, loss: 1.4311692714691162\n",
      "epoch: 7, batch: 51, loss: 1.4830378293991089\n",
      "epoch: 7, batch: 52, loss: 1.7808372974395752\n",
      "epoch: 7, batch: 53, loss: 1.2161399126052856\n",
      "epoch: 7, batch: 54, loss: 1.48805570602417\n",
      "epoch: 7, batch: 55, loss: 1.7394825220108032\n",
      "epoch: 7, batch: 56, loss: 2.1607871055603027\n",
      "epoch: 7, batch: 57, loss: 1.5423550605773926\n",
      "epoch: 7, batch: 58, loss: 1.6857086420059204\n",
      "epoch: 7, batch: 59, loss: 1.6490085124969482\n",
      "epoch: 7, batch: 60, loss: 1.7048723697662354\n",
      "epoch: 7, batch: 61, loss: 1.8608124256134033\n",
      "epoch: 7, batch: 62, loss: 1.3733488321304321\n",
      "epoch: 7, batch: 63, loss: 1.5431597232818604\n",
      "epoch: 7, batch: 64, loss: 1.4564026594161987\n",
      "epoch: 7, batch: 65, loss: 1.2029390335083008\n",
      "epoch: 7, batch: 66, loss: 1.4923086166381836\n",
      "epoch: 7, batch: 67, loss: 1.5175021886825562\n",
      "epoch: 7, batch: 68, loss: 1.5524104833602905\n",
      "epoch: 7, batch: 69, loss: 1.6349947452545166\n",
      "epoch: 7, batch: 70, loss: 1.3457142114639282\n",
      "epoch: 7, batch: 71, loss: 2.536705493927002\n",
      "epoch: 7, batch: 72, loss: 0.9894668459892273\n",
      "epoch: 7, batch: 73, loss: 1.883705735206604\n",
      "epoch: 7, batch: 74, loss: 2.1780552864074707\n",
      "epoch: 7, batch: 75, loss: 1.9675476551055908\n",
      "epoch: 7, batch: 76, loss: 1.6552015542984009\n",
      "epoch: 7, batch: 77, loss: 1.2730283737182617\n",
      "epoch: 7, batch: 78, loss: 1.821228265762329\n",
      "epoch: 7, batch: 79, loss: 1.4572257995605469\n",
      "epoch: 7, batch: 80, loss: 1.4475071430206299\n",
      "epoch: 7, batch: 81, loss: 1.8472095727920532\n",
      "epoch: 7, batch: 82, loss: 1.6329991817474365\n",
      "epoch: 7, batch: 83, loss: 1.4437336921691895\n",
      "epoch: 7, batch: 84, loss: 1.4488976001739502\n",
      "epoch: 7, batch: 85, loss: 1.639136552810669\n",
      "epoch: 7, batch: 86, loss: 1.2477442026138306\n",
      "epoch: 7, batch: 87, loss: 1.2320877313613892\n",
      "epoch: 7, batch: 88, loss: 2.0931155681610107\n",
      "epoch: 7, batch: 89, loss: 1.7709767818450928\n",
      "epoch: 7, batch: 90, loss: 1.5916533470153809\n",
      "epoch: 7, batch: 91, loss: 1.846474051475525\n",
      "epoch: 7, batch: 92, loss: 1.6637061834335327\n",
      "epoch: 7, batch: 93, loss: 1.4025039672851562\n",
      "epoch: 7, batch: 94, loss: 1.3410680294036865\n",
      "epoch: 7, batch: 95, loss: 1.3545384407043457\n",
      "epoch: 7, batch: 96, loss: 1.1601464748382568\n",
      "epoch: 7, batch: 97, loss: 2.2838289737701416\n",
      "epoch: 7, batch: 98, loss: 1.5301951169967651\n",
      "epoch: 7, batch: 99, loss: 1.8602895736694336\n",
      "epoch: 7, batch: 100, loss: 1.5185134410858154\n",
      "epoch: 7, batch: 101, loss: 1.6502711772918701\n",
      "epoch: 7, batch: 102, loss: 1.4954825639724731\n",
      "epoch: 7, batch: 103, loss: 1.8825801610946655\n",
      "epoch: 7, batch: 104, loss: 1.1788208484649658\n",
      "epoch: 7, batch: 105, loss: 1.5917706489562988\n",
      "epoch: 7, batch: 106, loss: 1.7336885929107666\n",
      "epoch: 7, batch: 107, loss: 1.2580983638763428\n",
      "epoch: 7, batch: 108, loss: 1.6307452917099\n",
      "epoch: 7, batch: 109, loss: 1.6283639669418335\n",
      "epoch: 7, batch: 110, loss: 1.2485204935073853\n",
      "epoch: 7, batch: 111, loss: 1.4264627695083618\n",
      "epoch: 7, batch: 112, loss: 1.5105383396148682\n",
      "epoch: 7, batch: 113, loss: 1.6143653392791748\n",
      "epoch: 7, batch: 114, loss: 1.3480584621429443\n",
      "epoch: 7, batch: 115, loss: 1.3411043882369995\n",
      "epoch: 7, batch: 116, loss: 1.703449010848999\n",
      "epoch: 7, batch: 117, loss: 1.585561990737915\n",
      "epoch: 7, batch: 118, loss: 1.4240643978118896\n",
      "epoch: 7, batch: 119, loss: 1.7421338558197021\n",
      "epoch: 7, batch: 120, loss: 1.4347617626190186\n",
      "epoch: 7, batch: 121, loss: 1.3497244119644165\n",
      "epoch: 7, batch: 122, loss: 1.7651084661483765\n",
      "epoch: 7, batch: 123, loss: 1.663511872291565\n",
      "epoch: 7, batch: 124, loss: 1.46232008934021\n",
      "epoch: 7, batch: 125, loss: 1.6924549341201782\n",
      "epoch: 7, batch: 126, loss: 1.775761604309082\n",
      "epoch: 7, batch: 127, loss: 1.689396619796753\n",
      "epoch: 8, batch: 0, loss: 1.5060632228851318\n",
      "epoch: 8, batch: 1, loss: 1.7522556781768799\n",
      "epoch: 8, batch: 2, loss: 1.5258023738861084\n",
      "epoch: 8, batch: 3, loss: 1.8319940567016602\n",
      "epoch: 8, batch: 4, loss: 1.4557219743728638\n",
      "epoch: 8, batch: 5, loss: 1.5996652841567993\n",
      "epoch: 8, batch: 6, loss: 1.8097816705703735\n",
      "epoch: 8, batch: 7, loss: 1.2446115016937256\n",
      "epoch: 8, batch: 8, loss: 1.4209331274032593\n",
      "epoch: 8, batch: 9, loss: 1.4252779483795166\n",
      "epoch: 8, batch: 10, loss: 1.6016342639923096\n",
      "epoch: 8, batch: 11, loss: 1.4868453741073608\n",
      "epoch: 8, batch: 12, loss: 1.253385305404663\n",
      "epoch: 8, batch: 13, loss: 1.9597371816635132\n",
      "epoch: 8, batch: 14, loss: 1.6536411046981812\n",
      "epoch: 8, batch: 15, loss: 1.5781681537628174\n",
      "epoch: 8, batch: 16, loss: 1.467260718345642\n",
      "epoch: 8, batch: 17, loss: 1.36703622341156\n",
      "epoch: 8, batch: 18, loss: 1.6089519262313843\n",
      "epoch: 8, batch: 19, loss: 1.8862206935882568\n",
      "epoch: 8, batch: 20, loss: 1.3738410472869873\n",
      "epoch: 8, batch: 21, loss: 1.613572359085083\n",
      "epoch: 8, batch: 22, loss: 1.5244715213775635\n",
      "epoch: 8, batch: 23, loss: 1.4919569492340088\n",
      "epoch: 8, batch: 24, loss: 1.484565019607544\n",
      "epoch: 8, batch: 25, loss: 1.5172961950302124\n",
      "epoch: 8, batch: 26, loss: 1.5006217956542969\n",
      "epoch: 8, batch: 27, loss: 1.627882957458496\n",
      "epoch: 8, batch: 28, loss: 1.6540400981903076\n",
      "epoch: 8, batch: 29, loss: 1.3183248043060303\n",
      "epoch: 8, batch: 30, loss: 1.4450044631958008\n",
      "epoch: 8, batch: 31, loss: 1.6413004398345947\n",
      "epoch: 8, batch: 32, loss: 1.5419433116912842\n",
      "epoch: 8, batch: 33, loss: 1.6296288967132568\n",
      "epoch: 8, batch: 34, loss: 1.470092535018921\n",
      "epoch: 8, batch: 35, loss: 1.4524930715560913\n",
      "epoch: 8, batch: 36, loss: 1.5144935846328735\n",
      "epoch: 8, batch: 37, loss: 1.5145962238311768\n",
      "epoch: 8, batch: 38, loss: 1.5787060260772705\n",
      "epoch: 8, batch: 39, loss: 1.577315092086792\n",
      "epoch: 8, batch: 40, loss: 1.6155866384506226\n",
      "epoch: 8, batch: 41, loss: 1.5248922109603882\n",
      "epoch: 8, batch: 42, loss: 1.5240157842636108\n",
      "epoch: 8, batch: 43, loss: 1.6633895635604858\n",
      "epoch: 8, batch: 44, loss: 1.5209118127822876\n",
      "epoch: 8, batch: 45, loss: 1.649935007095337\n",
      "epoch: 8, batch: 46, loss: 1.5142974853515625\n",
      "epoch: 8, batch: 47, loss: 1.693535566329956\n",
      "epoch: 8, batch: 48, loss: 1.4885743856430054\n",
      "epoch: 8, batch: 49, loss: 1.380619764328003\n",
      "epoch: 8, batch: 50, loss: 1.3482270240783691\n",
      "epoch: 8, batch: 51, loss: 1.5306974649429321\n",
      "epoch: 8, batch: 52, loss: 1.7492700815200806\n",
      "epoch: 8, batch: 53, loss: 1.6690311431884766\n",
      "epoch: 8, batch: 54, loss: 1.5706249475479126\n",
      "epoch: 8, batch: 55, loss: 1.6088365316390991\n",
      "epoch: 8, batch: 56, loss: 1.4527479410171509\n",
      "epoch: 8, batch: 57, loss: 1.647526741027832\n",
      "epoch: 8, batch: 58, loss: 1.7914928197860718\n",
      "epoch: 8, batch: 59, loss: 1.5752999782562256\n",
      "epoch: 8, batch: 60, loss: 1.6406885385513306\n",
      "epoch: 8, batch: 61, loss: 1.3750038146972656\n",
      "epoch: 8, batch: 62, loss: 1.6081628799438477\n",
      "epoch: 8, batch: 63, loss: 1.5502231121063232\n",
      "epoch: 8, batch: 64, loss: 1.57590913772583\n",
      "epoch: 8, batch: 65, loss: 1.6400645971298218\n",
      "epoch: 8, batch: 66, loss: 1.5239002704620361\n",
      "epoch: 8, batch: 67, loss: 1.547387957572937\n",
      "epoch: 8, batch: 68, loss: 1.3392671346664429\n",
      "epoch: 8, batch: 69, loss: 1.3753665685653687\n",
      "epoch: 8, batch: 70, loss: 1.5414323806762695\n",
      "epoch: 8, batch: 71, loss: 1.291331171989441\n",
      "epoch: 8, batch: 72, loss: 1.3137977123260498\n",
      "epoch: 8, batch: 73, loss: 1.7105833292007446\n",
      "epoch: 8, batch: 74, loss: 1.527265191078186\n",
      "epoch: 8, batch: 75, loss: 1.6999152898788452\n",
      "epoch: 8, batch: 76, loss: 1.1065038442611694\n",
      "epoch: 8, batch: 77, loss: 1.7628681659698486\n",
      "epoch: 8, batch: 78, loss: 1.6487627029418945\n",
      "epoch: 8, batch: 79, loss: 1.512121558189392\n",
      "epoch: 8, batch: 80, loss: 1.4928309917449951\n",
      "epoch: 8, batch: 81, loss: 1.518570065498352\n",
      "epoch: 8, batch: 82, loss: 1.2306410074234009\n",
      "epoch: 8, batch: 83, loss: 1.632311224937439\n",
      "epoch: 8, batch: 84, loss: 1.555100440979004\n",
      "epoch: 8, batch: 85, loss: 1.3715014457702637\n",
      "epoch: 8, batch: 86, loss: 1.5005983114242554\n",
      "epoch: 8, batch: 87, loss: 1.808153748512268\n",
      "epoch: 8, batch: 88, loss: 1.5591505765914917\n",
      "epoch: 8, batch: 89, loss: 1.525710105895996\n",
      "epoch: 8, batch: 90, loss: 1.52065110206604\n",
      "epoch: 8, batch: 91, loss: 1.5180925130844116\n",
      "epoch: 8, batch: 92, loss: 1.68654465675354\n",
      "epoch: 8, batch: 93, loss: 1.5561370849609375\n",
      "epoch: 8, batch: 94, loss: 1.4385573863983154\n",
      "epoch: 8, batch: 95, loss: 1.8670810461044312\n",
      "epoch: 8, batch: 96, loss: 1.3258224725723267\n",
      "epoch: 8, batch: 97, loss: 1.1045523881912231\n",
      "epoch: 8, batch: 98, loss: 1.5263303518295288\n",
      "epoch: 8, batch: 99, loss: 1.237864375114441\n",
      "epoch: 8, batch: 100, loss: 1.8334070444107056\n",
      "epoch: 8, batch: 101, loss: 1.1867661476135254\n",
      "epoch: 8, batch: 102, loss: 1.605552315711975\n",
      "epoch: 8, batch: 103, loss: 1.4530645608901978\n",
      "epoch: 8, batch: 104, loss: 1.537841558456421\n",
      "epoch: 8, batch: 105, loss: 1.177979826927185\n",
      "epoch: 8, batch: 106, loss: 2.091087818145752\n",
      "epoch: 8, batch: 107, loss: 1.638822317123413\n",
      "epoch: 8, batch: 108, loss: 2.2129364013671875\n",
      "epoch: 8, batch: 109, loss: 1.9232275485992432\n",
      "epoch: 8, batch: 110, loss: 1.911024808883667\n",
      "epoch: 8, batch: 111, loss: 1.2552090883255005\n",
      "epoch: 8, batch: 112, loss: 1.4899574518203735\n",
      "epoch: 8, batch: 113, loss: 1.2547814846038818\n",
      "epoch: 8, batch: 114, loss: 2.1760284900665283\n",
      "epoch: 8, batch: 115, loss: 1.5822795629501343\n",
      "epoch: 8, batch: 116, loss: 1.8464438915252686\n",
      "epoch: 8, batch: 117, loss: 2.3630573749542236\n",
      "epoch: 8, batch: 118, loss: 1.6580064296722412\n",
      "epoch: 8, batch: 119, loss: 1.4628241062164307\n",
      "epoch: 8, batch: 120, loss: 1.1968461275100708\n",
      "epoch: 8, batch: 121, loss: 1.4979368448257446\n",
      "epoch: 8, batch: 122, loss: 1.6793466806411743\n",
      "epoch: 8, batch: 123, loss: 2.0464119911193848\n",
      "epoch: 8, batch: 124, loss: 1.5343977212905884\n",
      "epoch: 8, batch: 125, loss: 2.0698516368865967\n",
      "epoch: 8, batch: 126, loss: 1.6772769689559937\n",
      "epoch: 8, batch: 127, loss: 1.8018081188201904\n",
      "epoch: 9, batch: 0, loss: 1.4232209920883179\n",
      "epoch: 9, batch: 1, loss: 2.0186448097229004\n",
      "epoch: 9, batch: 2, loss: 1.2848806381225586\n",
      "epoch: 9, batch: 3, loss: 1.794618010520935\n",
      "epoch: 9, batch: 4, loss: 1.4740508794784546\n",
      "epoch: 9, batch: 5, loss: 1.5337860584259033\n",
      "epoch: 9, batch: 6, loss: 1.5042740106582642\n",
      "epoch: 9, batch: 7, loss: 1.6685245037078857\n",
      "epoch: 9, batch: 8, loss: 1.3475072383880615\n",
      "epoch: 9, batch: 9, loss: 1.2166752815246582\n",
      "epoch: 9, batch: 10, loss: 1.6993160247802734\n",
      "epoch: 9, batch: 11, loss: 1.199739694595337\n",
      "epoch: 9, batch: 12, loss: 1.5694491863250732\n",
      "epoch: 9, batch: 13, loss: 1.40348219871521\n",
      "epoch: 9, batch: 14, loss: 1.6331878900527954\n",
      "epoch: 9, batch: 15, loss: 1.270552635192871\n",
      "epoch: 9, batch: 16, loss: 1.6624133586883545\n",
      "epoch: 9, batch: 17, loss: 1.6438010931015015\n",
      "epoch: 9, batch: 18, loss: 1.639859914779663\n",
      "epoch: 9, batch: 19, loss: 1.410523772239685\n",
      "epoch: 9, batch: 20, loss: 1.3710558414459229\n",
      "epoch: 9, batch: 21, loss: 2.1725456714630127\n",
      "epoch: 9, batch: 22, loss: 1.3905532360076904\n",
      "epoch: 9, batch: 23, loss: 2.1167991161346436\n",
      "epoch: 9, batch: 24, loss: 1.7162342071533203\n",
      "epoch: 9, batch: 25, loss: 1.7586967945098877\n",
      "epoch: 9, batch: 26, loss: 1.8213380575180054\n",
      "epoch: 9, batch: 27, loss: 1.9670085906982422\n",
      "epoch: 9, batch: 28, loss: 1.648899793624878\n",
      "epoch: 9, batch: 29, loss: 1.4758477210998535\n",
      "epoch: 9, batch: 30, loss: 1.5814406871795654\n",
      "epoch: 9, batch: 31, loss: 2.1103463172912598\n",
      "epoch: 9, batch: 32, loss: 1.7185394763946533\n",
      "epoch: 9, batch: 33, loss: 1.5714939832687378\n",
      "epoch: 9, batch: 34, loss: 1.9446537494659424\n",
      "epoch: 9, batch: 35, loss: 1.6491420269012451\n",
      "epoch: 9, batch: 36, loss: 1.4917882680892944\n",
      "epoch: 9, batch: 37, loss: 1.2758272886276245\n",
      "epoch: 9, batch: 38, loss: 1.6515874862670898\n",
      "epoch: 9, batch: 39, loss: 1.5746787786483765\n",
      "epoch: 9, batch: 40, loss: 1.6790368556976318\n",
      "epoch: 9, batch: 41, loss: 1.5136101245880127\n",
      "epoch: 9, batch: 42, loss: 1.8320382833480835\n",
      "epoch: 9, batch: 43, loss: 1.4434188604354858\n",
      "epoch: 9, batch: 44, loss: 1.692025899887085\n",
      "epoch: 9, batch: 45, loss: 1.4041305780410767\n",
      "epoch: 9, batch: 46, loss: 1.4595807790756226\n",
      "epoch: 9, batch: 47, loss: 1.589827299118042\n",
      "epoch: 9, batch: 48, loss: 1.7198410034179688\n",
      "epoch: 9, batch: 49, loss: 1.5806574821472168\n",
      "epoch: 9, batch: 50, loss: 1.5872695446014404\n",
      "epoch: 9, batch: 51, loss: 1.682665228843689\n",
      "epoch: 9, batch: 52, loss: 1.7057816982269287\n",
      "epoch: 9, batch: 53, loss: 1.7436561584472656\n",
      "epoch: 9, batch: 54, loss: 1.662803292274475\n",
      "epoch: 9, batch: 55, loss: 1.3811378479003906\n",
      "epoch: 9, batch: 56, loss: 1.76595938205719\n",
      "epoch: 9, batch: 57, loss: 1.5645530223846436\n",
      "epoch: 9, batch: 58, loss: 1.616754174232483\n",
      "epoch: 9, batch: 59, loss: 1.5154178142547607\n",
      "epoch: 9, batch: 60, loss: 1.449336290359497\n",
      "epoch: 9, batch: 61, loss: 1.603222131729126\n",
      "epoch: 9, batch: 62, loss: 1.714012861251831\n",
      "epoch: 9, batch: 63, loss: 1.3931043148040771\n",
      "epoch: 9, batch: 64, loss: 1.4493051767349243\n",
      "epoch: 9, batch: 65, loss: 1.6816132068634033\n",
      "epoch: 9, batch: 66, loss: 1.4253381490707397\n",
      "epoch: 9, batch: 67, loss: 1.6790844202041626\n",
      "epoch: 9, batch: 68, loss: 1.5992523431777954\n",
      "epoch: 9, batch: 69, loss: 1.6935241222381592\n",
      "epoch: 9, batch: 70, loss: 1.7115236520767212\n",
      "epoch: 9, batch: 71, loss: 1.6513690948486328\n",
      "epoch: 9, batch: 72, loss: 1.797645926475525\n",
      "epoch: 9, batch: 73, loss: 1.4259827136993408\n",
      "epoch: 9, batch: 74, loss: 1.6995391845703125\n",
      "epoch: 9, batch: 75, loss: 1.459194540977478\n",
      "epoch: 9, batch: 76, loss: 1.722759485244751\n",
      "epoch: 9, batch: 77, loss: 1.7040226459503174\n",
      "epoch: 9, batch: 78, loss: 1.5092689990997314\n",
      "epoch: 9, batch: 79, loss: 1.6546392440795898\n",
      "epoch: 9, batch: 80, loss: 1.6781425476074219\n",
      "epoch: 9, batch: 81, loss: 1.4234817028045654\n",
      "epoch: 9, batch: 82, loss: 1.7150996923446655\n",
      "epoch: 9, batch: 83, loss: 1.4297471046447754\n",
      "epoch: 9, batch: 84, loss: 1.4603596925735474\n",
      "epoch: 9, batch: 85, loss: 1.5968536138534546\n",
      "epoch: 9, batch: 86, loss: 1.4963839054107666\n",
      "epoch: 9, batch: 87, loss: 1.7062530517578125\n",
      "epoch: 9, batch: 88, loss: 1.560732126235962\n",
      "epoch: 9, batch: 89, loss: 1.475136399269104\n",
      "epoch: 9, batch: 90, loss: 1.6580222845077515\n",
      "epoch: 9, batch: 91, loss: 1.6055740118026733\n",
      "epoch: 9, batch: 92, loss: 1.7573339939117432\n",
      "epoch: 9, batch: 93, loss: 1.6328189373016357\n",
      "epoch: 9, batch: 94, loss: 1.6305698156356812\n",
      "epoch: 9, batch: 95, loss: 1.387974500656128\n",
      "epoch: 9, batch: 96, loss: 1.4927120208740234\n",
      "epoch: 9, batch: 97, loss: 1.5520777702331543\n",
      "epoch: 9, batch: 98, loss: 1.4067051410675049\n",
      "epoch: 9, batch: 99, loss: 1.3668018579483032\n",
      "epoch: 9, batch: 100, loss: 1.4668992757797241\n",
      "epoch: 9, batch: 101, loss: 1.530295968055725\n",
      "epoch: 9, batch: 102, loss: 1.4505159854888916\n",
      "epoch: 9, batch: 103, loss: 1.6539037227630615\n",
      "epoch: 9, batch: 104, loss: 1.5606567859649658\n",
      "epoch: 9, batch: 105, loss: 1.4820433855056763\n",
      "epoch: 9, batch: 106, loss: 1.4289000034332275\n",
      "epoch: 9, batch: 107, loss: 1.463057279586792\n",
      "epoch: 9, batch: 108, loss: 1.5926244258880615\n",
      "epoch: 9, batch: 109, loss: 1.4258614778518677\n",
      "epoch: 9, batch: 110, loss: 1.612410306930542\n",
      "epoch: 9, batch: 111, loss: 1.7761962413787842\n",
      "epoch: 9, batch: 112, loss: 1.9012210369110107\n",
      "epoch: 9, batch: 113, loss: 1.5415527820587158\n",
      "epoch: 9, batch: 114, loss: 1.4932457208633423\n",
      "epoch: 9, batch: 115, loss: 1.750610589981079\n",
      "epoch: 9, batch: 116, loss: 1.7333627939224243\n",
      "epoch: 9, batch: 117, loss: 1.3706157207489014\n",
      "epoch: 9, batch: 118, loss: 1.608102798461914\n",
      "epoch: 9, batch: 119, loss: 1.4264018535614014\n",
      "epoch: 9, batch: 120, loss: 1.4699559211730957\n",
      "epoch: 9, batch: 121, loss: 1.5881601572036743\n",
      "epoch: 9, batch: 122, loss: 1.2294856309890747\n",
      "epoch: 9, batch: 123, loss: 1.7069156169891357\n",
      "epoch: 9, batch: 124, loss: 1.405880093574524\n",
      "epoch: 9, batch: 125, loss: 1.4958744049072266\n",
      "epoch: 9, batch: 126, loss: 1.6444900035858154\n",
      "epoch: 9, batch: 127, loss: 1.456653118133545\n",
      "epoch: 10, batch: 0, loss: 1.6915404796600342\n",
      "epoch: 10, batch: 1, loss: 1.4826385974884033\n",
      "epoch: 10, batch: 2, loss: 1.5942766666412354\n",
      "epoch: 10, batch: 3, loss: 1.6121400594711304\n",
      "epoch: 10, batch: 4, loss: 1.515454888343811\n",
      "epoch: 10, batch: 5, loss: 1.4844143390655518\n",
      "epoch: 10, batch: 6, loss: 1.5709680318832397\n",
      "epoch: 10, batch: 7, loss: 1.8079032897949219\n",
      "epoch: 10, batch: 8, loss: 1.701084852218628\n",
      "epoch: 10, batch: 9, loss: 1.5037658214569092\n",
      "epoch: 10, batch: 10, loss: 1.4763578176498413\n",
      "epoch: 10, batch: 11, loss: 1.442607045173645\n",
      "epoch: 10, batch: 12, loss: 1.4701087474822998\n",
      "epoch: 10, batch: 13, loss: 1.7376985549926758\n",
      "epoch: 10, batch: 14, loss: 1.5714025497436523\n",
      "epoch: 10, batch: 15, loss: 1.5025781393051147\n",
      "epoch: 10, batch: 16, loss: 1.5408496856689453\n",
      "epoch: 10, batch: 17, loss: 1.6095765829086304\n",
      "epoch: 10, batch: 18, loss: 1.3659337759017944\n",
      "epoch: 10, batch: 19, loss: 1.7531875371932983\n",
      "epoch: 10, batch: 20, loss: 1.371002197265625\n",
      "epoch: 10, batch: 21, loss: 1.5847141742706299\n",
      "epoch: 10, batch: 22, loss: 1.4848382472991943\n",
      "epoch: 10, batch: 23, loss: 1.810973882675171\n",
      "epoch: 10, batch: 24, loss: 1.6168369054794312\n",
      "epoch: 10, batch: 25, loss: 1.663877248764038\n",
      "epoch: 10, batch: 26, loss: 1.8195950984954834\n",
      "epoch: 10, batch: 27, loss: 1.3831924200057983\n",
      "epoch: 10, batch: 28, loss: 1.4796192646026611\n",
      "epoch: 10, batch: 29, loss: 1.451989769935608\n",
      "epoch: 10, batch: 30, loss: 1.6615102291107178\n",
      "epoch: 10, batch: 31, loss: 1.5032713413238525\n",
      "epoch: 10, batch: 32, loss: 1.4011555910110474\n",
      "epoch: 10, batch: 33, loss: 1.3837192058563232\n",
      "epoch: 10, batch: 34, loss: 1.391027808189392\n",
      "epoch: 10, batch: 35, loss: 1.8165128231048584\n",
      "epoch: 10, batch: 36, loss: 1.3846608400344849\n",
      "epoch: 10, batch: 37, loss: 1.8429969549179077\n",
      "epoch: 10, batch: 38, loss: 1.622528076171875\n",
      "epoch: 10, batch: 39, loss: 1.5499194860458374\n",
      "epoch: 10, batch: 40, loss: 1.6943254470825195\n",
      "epoch: 10, batch: 41, loss: 1.7427324056625366\n",
      "epoch: 10, batch: 42, loss: 1.9113247394561768\n",
      "epoch: 10, batch: 43, loss: 1.4389288425445557\n",
      "epoch: 10, batch: 44, loss: 1.531406044960022\n",
      "epoch: 10, batch: 45, loss: 1.0968183279037476\n",
      "epoch: 10, batch: 46, loss: 1.2234764099121094\n",
      "epoch: 10, batch: 47, loss: 1.5440616607666016\n",
      "epoch: 10, batch: 48, loss: 1.396276831626892\n",
      "epoch: 10, batch: 49, loss: 1.4874248504638672\n",
      "epoch: 10, batch: 50, loss: 1.7155792713165283\n",
      "epoch: 10, batch: 51, loss: 1.2326730489730835\n",
      "epoch: 10, batch: 52, loss: 1.6665130853652954\n",
      "epoch: 10, batch: 53, loss: 1.521905541419983\n",
      "epoch: 10, batch: 54, loss: 1.654874563217163\n",
      "epoch: 10, batch: 55, loss: 1.1878881454467773\n",
      "epoch: 10, batch: 56, loss: 1.6510899066925049\n",
      "epoch: 10, batch: 57, loss: 1.417618989944458\n",
      "epoch: 10, batch: 58, loss: 1.8082926273345947\n",
      "epoch: 10, batch: 59, loss: 1.6265060901641846\n",
      "epoch: 10, batch: 60, loss: 1.3719327449798584\n",
      "epoch: 10, batch: 61, loss: 1.4444764852523804\n",
      "epoch: 10, batch: 62, loss: 1.6451466083526611\n",
      "epoch: 10, batch: 63, loss: 1.5100343227386475\n",
      "epoch: 10, batch: 64, loss: 1.9564096927642822\n",
      "epoch: 10, batch: 65, loss: 1.7085310220718384\n",
      "epoch: 10, batch: 66, loss: 1.8054349422454834\n",
      "epoch: 10, batch: 67, loss: 1.4616425037384033\n",
      "epoch: 10, batch: 68, loss: 1.2999292612075806\n",
      "epoch: 10, batch: 69, loss: 1.2870543003082275\n",
      "epoch: 10, batch: 70, loss: 1.5871238708496094\n",
      "epoch: 10, batch: 71, loss: 1.6775211095809937\n",
      "epoch: 10, batch: 72, loss: 1.133388876914978\n",
      "epoch: 10, batch: 73, loss: 1.6542524099349976\n",
      "epoch: 10, batch: 74, loss: 1.6799602508544922\n",
      "epoch: 10, batch: 75, loss: 1.5621063709259033\n",
      "epoch: 10, batch: 76, loss: 1.6372114419937134\n",
      "epoch: 10, batch: 77, loss: 1.395423173904419\n",
      "epoch: 10, batch: 78, loss: 1.489294409751892\n",
      "epoch: 10, batch: 79, loss: 1.4416805505752563\n",
      "epoch: 10, batch: 80, loss: 1.7555214166641235\n",
      "epoch: 10, batch: 81, loss: 1.8331905603408813\n",
      "epoch: 10, batch: 82, loss: 1.524293303489685\n",
      "epoch: 10, batch: 83, loss: 1.4716293811798096\n",
      "epoch: 10, batch: 84, loss: 1.3016074895858765\n",
      "epoch: 10, batch: 85, loss: 1.826401948928833\n",
      "epoch: 10, batch: 86, loss: 1.2239973545074463\n",
      "epoch: 10, batch: 87, loss: 1.4047034978866577\n",
      "epoch: 10, batch: 88, loss: 1.6953588724136353\n",
      "epoch: 10, batch: 89, loss: 1.7053115367889404\n",
      "epoch: 10, batch: 90, loss: 1.5582990646362305\n",
      "epoch: 10, batch: 91, loss: 1.318327784538269\n",
      "epoch: 10, batch: 92, loss: 1.655275583267212\n",
      "epoch: 10, batch: 93, loss: 1.7901618480682373\n",
      "epoch: 10, batch: 94, loss: 1.3874051570892334\n",
      "epoch: 10, batch: 95, loss: 1.3616392612457275\n",
      "epoch: 10, batch: 96, loss: 1.4500716924667358\n",
      "epoch: 10, batch: 97, loss: 1.0450899600982666\n",
      "epoch: 10, batch: 98, loss: 1.663802146911621\n",
      "epoch: 10, batch: 99, loss: 1.7613319158554077\n",
      "epoch: 10, batch: 100, loss: 1.1486485004425049\n",
      "epoch: 10, batch: 101, loss: 1.4249025583267212\n",
      "epoch: 10, batch: 102, loss: 1.3661201000213623\n",
      "epoch: 10, batch: 103, loss: 1.9524749517440796\n",
      "epoch: 10, batch: 104, loss: 1.511040210723877\n",
      "epoch: 10, batch: 105, loss: 1.945600152015686\n",
      "epoch: 10, batch: 106, loss: 1.7308906316757202\n",
      "epoch: 10, batch: 107, loss: 1.5378628969192505\n",
      "epoch: 10, batch: 108, loss: 1.447059988975525\n",
      "epoch: 10, batch: 109, loss: 1.1232600212097168\n",
      "epoch: 10, batch: 110, loss: 1.5191490650177002\n",
      "epoch: 10, batch: 111, loss: 1.4839835166931152\n",
      "epoch: 10, batch: 112, loss: 1.3981572389602661\n",
      "epoch: 10, batch: 113, loss: 1.8371425867080688\n",
      "epoch: 10, batch: 114, loss: 1.6461986303329468\n",
      "epoch: 10, batch: 115, loss: 1.2007423639297485\n",
      "epoch: 10, batch: 116, loss: 1.8883432149887085\n",
      "epoch: 10, batch: 117, loss: 1.3383256196975708\n",
      "epoch: 10, batch: 118, loss: 1.231459140777588\n",
      "epoch: 10, batch: 119, loss: 1.4124062061309814\n",
      "epoch: 10, batch: 120, loss: 1.229354977607727\n",
      "epoch: 10, batch: 121, loss: 1.5810197591781616\n",
      "epoch: 10, batch: 122, loss: 1.8676685094833374\n",
      "epoch: 10, batch: 123, loss: 1.64456045627594\n",
      "epoch: 10, batch: 124, loss: 1.3407922983169556\n",
      "epoch: 10, batch: 125, loss: 2.0738577842712402\n",
      "epoch: 10, batch: 126, loss: 1.422717809677124\n",
      "epoch: 10, batch: 127, loss: 1.0884836912155151\n",
      "epoch: 11, batch: 0, loss: 1.409147024154663\n",
      "epoch: 11, batch: 1, loss: 1.3628937005996704\n",
      "epoch: 11, batch: 2, loss: 1.6302766799926758\n",
      "epoch: 11, batch: 3, loss: 1.5067565441131592\n",
      "epoch: 11, batch: 4, loss: 1.692875862121582\n",
      "epoch: 11, batch: 5, loss: 1.368901014328003\n",
      "epoch: 11, batch: 6, loss: 1.3705542087554932\n",
      "epoch: 11, batch: 7, loss: 1.2492964267730713\n",
      "epoch: 11, batch: 8, loss: 1.125373125076294\n",
      "epoch: 11, batch: 9, loss: 1.782701849937439\n",
      "epoch: 11, batch: 10, loss: 1.860377550125122\n",
      "epoch: 11, batch: 11, loss: 2.0405960083007812\n",
      "epoch: 11, batch: 12, loss: 1.7132179737091064\n",
      "epoch: 11, batch: 13, loss: 1.5733503103256226\n",
      "epoch: 11, batch: 14, loss: 1.5721838474273682\n",
      "epoch: 11, batch: 15, loss: 1.7510915994644165\n",
      "epoch: 11, batch: 16, loss: 1.3547651767730713\n",
      "epoch: 11, batch: 17, loss: 1.460191011428833\n",
      "epoch: 11, batch: 18, loss: 1.1799752712249756\n",
      "epoch: 11, batch: 19, loss: 1.7762664556503296\n",
      "epoch: 11, batch: 20, loss: 1.6995233297348022\n",
      "epoch: 11, batch: 21, loss: 1.6290785074234009\n",
      "epoch: 11, batch: 22, loss: 1.5888763666152954\n",
      "epoch: 11, batch: 23, loss: 1.7671394348144531\n",
      "epoch: 11, batch: 24, loss: 1.4403960704803467\n",
      "epoch: 11, batch: 25, loss: 1.593082070350647\n",
      "epoch: 11, batch: 26, loss: 1.7661819458007812\n",
      "epoch: 11, batch: 27, loss: 1.7136318683624268\n",
      "epoch: 11, batch: 28, loss: 0.9716939926147461\n",
      "epoch: 11, batch: 29, loss: 1.0772120952606201\n",
      "epoch: 11, batch: 30, loss: 1.3225716352462769\n",
      "epoch: 11, batch: 31, loss: 1.5822073221206665\n",
      "epoch: 11, batch: 32, loss: 1.143958330154419\n",
      "epoch: 11, batch: 33, loss: 1.4005097150802612\n",
      "epoch: 11, batch: 34, loss: 1.3381555080413818\n",
      "epoch: 11, batch: 35, loss: 1.2005503177642822\n",
      "epoch: 11, batch: 36, loss: 1.5517919063568115\n",
      "epoch: 11, batch: 37, loss: 1.2634607553482056\n",
      "epoch: 11, batch: 38, loss: 1.3673226833343506\n",
      "epoch: 11, batch: 39, loss: 1.6807136535644531\n",
      "epoch: 11, batch: 40, loss: 1.5248948335647583\n",
      "epoch: 11, batch: 41, loss: 1.5223915576934814\n",
      "epoch: 11, batch: 42, loss: 1.7403701543807983\n",
      "epoch: 11, batch: 43, loss: 0.9906872510910034\n",
      "epoch: 11, batch: 44, loss: 1.722021460533142\n",
      "epoch: 11, batch: 45, loss: 1.359705924987793\n",
      "epoch: 11, batch: 46, loss: 1.2523415088653564\n",
      "epoch: 11, batch: 47, loss: 1.4527184963226318\n",
      "epoch: 11, batch: 48, loss: 1.637874960899353\n",
      "epoch: 11, batch: 49, loss: 1.1436681747436523\n",
      "epoch: 11, batch: 50, loss: 1.5456933975219727\n",
      "epoch: 11, batch: 51, loss: 1.6853256225585938\n",
      "epoch: 11, batch: 52, loss: 1.7496896982192993\n",
      "epoch: 11, batch: 53, loss: 1.560420274734497\n",
      "epoch: 11, batch: 54, loss: 1.9011743068695068\n",
      "epoch: 11, batch: 55, loss: 1.6644346714019775\n",
      "epoch: 11, batch: 56, loss: 1.6527483463287354\n",
      "epoch: 11, batch: 57, loss: 1.832903265953064\n",
      "epoch: 11, batch: 58, loss: 1.4862266778945923\n",
      "epoch: 11, batch: 59, loss: 1.313690423965454\n",
      "epoch: 11, batch: 60, loss: 1.643304467201233\n",
      "epoch: 11, batch: 61, loss: 1.2637158632278442\n",
      "epoch: 11, batch: 62, loss: 1.888795256614685\n",
      "epoch: 11, batch: 63, loss: 1.5132852792739868\n",
      "epoch: 11, batch: 64, loss: 1.6929575204849243\n",
      "epoch: 11, batch: 65, loss: 1.6128637790679932\n",
      "epoch: 11, batch: 66, loss: 1.8215709924697876\n",
      "epoch: 11, batch: 67, loss: 1.6793018579483032\n",
      "epoch: 11, batch: 68, loss: 1.8745830059051514\n",
      "epoch: 11, batch: 69, loss: 1.7766742706298828\n",
      "epoch: 11, batch: 70, loss: 1.55983567237854\n",
      "epoch: 11, batch: 71, loss: 1.5388360023498535\n",
      "epoch: 11, batch: 72, loss: 1.3669620752334595\n",
      "epoch: 11, batch: 73, loss: 1.5250016450881958\n",
      "epoch: 11, batch: 74, loss: 1.135461449623108\n",
      "epoch: 11, batch: 75, loss: 1.3351494073867798\n",
      "epoch: 11, batch: 76, loss: 1.5032203197479248\n",
      "epoch: 11, batch: 77, loss: 2.1242940425872803\n",
      "epoch: 11, batch: 78, loss: 1.7665960788726807\n",
      "epoch: 11, batch: 79, loss: 1.8897027969360352\n",
      "epoch: 11, batch: 80, loss: 1.506874680519104\n",
      "epoch: 11, batch: 81, loss: 1.8102811574935913\n",
      "epoch: 11, batch: 82, loss: 1.4427480697631836\n",
      "epoch: 11, batch: 83, loss: 1.67068350315094\n",
      "epoch: 11, batch: 84, loss: 1.3960319757461548\n",
      "epoch: 11, batch: 85, loss: 1.768598198890686\n",
      "epoch: 11, batch: 86, loss: 1.3628714084625244\n",
      "epoch: 11, batch: 87, loss: 1.4531748294830322\n",
      "epoch: 11, batch: 88, loss: 1.6298370361328125\n",
      "epoch: 11, batch: 89, loss: 1.522779107093811\n",
      "epoch: 11, batch: 90, loss: 1.7090473175048828\n",
      "epoch: 11, batch: 91, loss: 1.3147337436676025\n",
      "epoch: 11, batch: 92, loss: 1.5021613836288452\n",
      "epoch: 11, batch: 93, loss: 1.3194185495376587\n",
      "epoch: 11, batch: 94, loss: 1.5927554368972778\n",
      "epoch: 11, batch: 95, loss: 1.4331097602844238\n",
      "epoch: 11, batch: 96, loss: 1.6478526592254639\n",
      "epoch: 11, batch: 97, loss: 1.6292814016342163\n",
      "epoch: 11, batch: 98, loss: 1.4345972537994385\n",
      "epoch: 11, batch: 99, loss: 1.4335899353027344\n",
      "epoch: 11, batch: 100, loss: 1.694441795349121\n",
      "epoch: 11, batch: 101, loss: 1.6560083627700806\n",
      "epoch: 11, batch: 102, loss: 1.3167438507080078\n",
      "epoch: 11, batch: 103, loss: 1.7608003616333008\n",
      "epoch: 11, batch: 104, loss: 1.4208076000213623\n",
      "epoch: 11, batch: 105, loss: 1.3357130289077759\n",
      "epoch: 11, batch: 106, loss: 1.2541000843048096\n",
      "epoch: 11, batch: 107, loss: 1.8746435642242432\n",
      "epoch: 11, batch: 108, loss: 1.5526607036590576\n",
      "epoch: 11, batch: 109, loss: 1.562004804611206\n",
      "epoch: 11, batch: 110, loss: 1.5747921466827393\n",
      "epoch: 11, batch: 111, loss: 1.3023273944854736\n",
      "epoch: 11, batch: 112, loss: 1.820814847946167\n",
      "epoch: 11, batch: 113, loss: 1.7546043395996094\n",
      "epoch: 11, batch: 114, loss: 1.4354403018951416\n",
      "epoch: 11, batch: 115, loss: 1.74445378780365\n",
      "epoch: 11, batch: 116, loss: 1.522538185119629\n",
      "epoch: 11, batch: 117, loss: 1.4662376642227173\n",
      "epoch: 11, batch: 118, loss: 1.614423394203186\n",
      "epoch: 11, batch: 119, loss: 1.5656464099884033\n",
      "epoch: 11, batch: 120, loss: 1.5895084142684937\n",
      "epoch: 11, batch: 121, loss: 1.6676387786865234\n",
      "epoch: 11, batch: 122, loss: 1.4264723062515259\n",
      "epoch: 11, batch: 123, loss: 1.5423473119735718\n",
      "epoch: 11, batch: 124, loss: 1.5591917037963867\n",
      "epoch: 11, batch: 125, loss: 1.2277719974517822\n",
      "epoch: 11, batch: 126, loss: 1.6764917373657227\n",
      "epoch: 11, batch: 127, loss: 1.4018124341964722\n",
      "epoch: 12, batch: 0, loss: 1.3269422054290771\n",
      "epoch: 12, batch: 1, loss: 1.3144190311431885\n",
      "epoch: 12, batch: 2, loss: 1.290003776550293\n",
      "epoch: 12, batch: 3, loss: 1.3244001865386963\n",
      "epoch: 12, batch: 4, loss: 1.43959641456604\n",
      "epoch: 12, batch: 5, loss: 1.8326324224472046\n",
      "epoch: 12, batch: 6, loss: 1.3999069929122925\n",
      "epoch: 12, batch: 7, loss: 1.7015186548233032\n",
      "epoch: 12, batch: 8, loss: 1.5568712949752808\n",
      "epoch: 12, batch: 9, loss: 1.3510162830352783\n",
      "epoch: 12, batch: 10, loss: 1.9045798778533936\n",
      "epoch: 12, batch: 11, loss: 1.462868332862854\n",
      "epoch: 12, batch: 12, loss: 1.5433015823364258\n",
      "epoch: 12, batch: 13, loss: 1.1913093328475952\n",
      "epoch: 12, batch: 14, loss: 1.440478801727295\n",
      "epoch: 12, batch: 15, loss: 1.4027888774871826\n",
      "epoch: 12, batch: 16, loss: 1.7065465450286865\n",
      "epoch: 12, batch: 17, loss: 1.9815795421600342\n",
      "epoch: 12, batch: 18, loss: 1.4347760677337646\n",
      "epoch: 12, batch: 19, loss: 1.7765185832977295\n",
      "epoch: 12, batch: 20, loss: 1.5379884243011475\n",
      "epoch: 12, batch: 21, loss: 1.4871842861175537\n",
      "epoch: 12, batch: 22, loss: 1.3670099973678589\n",
      "epoch: 12, batch: 23, loss: 1.7559226751327515\n",
      "epoch: 12, batch: 24, loss: 1.4312000274658203\n",
      "epoch: 12, batch: 25, loss: 1.6002066135406494\n",
      "epoch: 12, batch: 26, loss: 1.7559810876846313\n",
      "epoch: 12, batch: 27, loss: 1.6623541116714478\n",
      "epoch: 12, batch: 28, loss: 1.3825902938842773\n",
      "epoch: 12, batch: 29, loss: 1.8792117834091187\n",
      "epoch: 12, batch: 30, loss: 1.5870354175567627\n",
      "epoch: 12, batch: 31, loss: 2.051121950149536\n",
      "epoch: 12, batch: 32, loss: 1.4149878025054932\n",
      "epoch: 12, batch: 33, loss: 1.094409704208374\n",
      "epoch: 12, batch: 34, loss: 1.5801820755004883\n",
      "epoch: 12, batch: 35, loss: 1.7104942798614502\n",
      "epoch: 12, batch: 36, loss: 1.5868221521377563\n",
      "epoch: 12, batch: 37, loss: 1.3381611108779907\n",
      "epoch: 12, batch: 38, loss: 1.745039701461792\n",
      "epoch: 12, batch: 39, loss: 1.586945652961731\n",
      "epoch: 12, batch: 40, loss: 1.3355495929718018\n",
      "epoch: 12, batch: 41, loss: 1.8421554565429688\n",
      "epoch: 12, batch: 42, loss: 1.6781142950057983\n",
      "epoch: 12, batch: 43, loss: 1.5940804481506348\n",
      "epoch: 12, batch: 44, loss: 1.4951608180999756\n",
      "epoch: 12, batch: 45, loss: 1.8444610834121704\n",
      "epoch: 12, batch: 46, loss: 1.5355350971221924\n",
      "epoch: 12, batch: 47, loss: 1.8552796840667725\n",
      "epoch: 12, batch: 48, loss: 1.6669557094573975\n",
      "epoch: 12, batch: 49, loss: 1.8801854848861694\n",
      "epoch: 12, batch: 50, loss: 1.8163611888885498\n",
      "epoch: 12, batch: 51, loss: 1.5061763525009155\n",
      "epoch: 12, batch: 52, loss: 1.6357231140136719\n",
      "epoch: 12, batch: 53, loss: 1.5634294748306274\n",
      "epoch: 12, batch: 54, loss: 1.6184051036834717\n",
      "epoch: 12, batch: 55, loss: 1.155895471572876\n",
      "epoch: 12, batch: 56, loss: 1.8488171100616455\n",
      "epoch: 12, batch: 57, loss: 1.800453782081604\n",
      "epoch: 12, batch: 58, loss: 1.3724641799926758\n",
      "epoch: 12, batch: 59, loss: 1.4945603609085083\n",
      "epoch: 12, batch: 60, loss: 1.5591773986816406\n",
      "epoch: 12, batch: 61, loss: 1.4812439680099487\n",
      "epoch: 12, batch: 62, loss: 1.3797237873077393\n",
      "epoch: 12, batch: 63, loss: 1.6338495016098022\n",
      "epoch: 12, batch: 64, loss: 1.7140095233917236\n",
      "epoch: 12, batch: 65, loss: 1.6563894748687744\n",
      "epoch: 12, batch: 66, loss: 1.8182098865509033\n",
      "epoch: 12, batch: 67, loss: 1.6329963207244873\n",
      "epoch: 12, batch: 68, loss: 1.7922130823135376\n",
      "epoch: 12, batch: 69, loss: 1.5374736785888672\n",
      "epoch: 12, batch: 70, loss: 1.5820766687393188\n",
      "epoch: 12, batch: 71, loss: 1.6256166696548462\n",
      "epoch: 12, batch: 72, loss: 1.6878688335418701\n",
      "epoch: 12, batch: 73, loss: 1.305995225906372\n",
      "epoch: 12, batch: 74, loss: 1.5930761098861694\n",
      "epoch: 12, batch: 75, loss: 1.6769832372665405\n",
      "epoch: 12, batch: 76, loss: 1.4170135259628296\n",
      "epoch: 12, batch: 77, loss: 1.5931942462921143\n",
      "epoch: 12, batch: 78, loss: 1.6017837524414062\n",
      "epoch: 12, batch: 79, loss: 1.6058162450790405\n",
      "epoch: 12, batch: 80, loss: 1.7173783779144287\n",
      "epoch: 12, batch: 81, loss: 1.4946143627166748\n",
      "epoch: 12, batch: 82, loss: 1.6569969654083252\n",
      "epoch: 12, batch: 83, loss: 1.508533239364624\n",
      "epoch: 12, batch: 84, loss: 1.4847007989883423\n",
      "epoch: 12, batch: 85, loss: 1.6574592590332031\n",
      "epoch: 12, batch: 86, loss: 1.429694652557373\n",
      "epoch: 12, batch: 87, loss: 1.435605764389038\n",
      "epoch: 12, batch: 88, loss: 1.4925410747528076\n",
      "epoch: 12, batch: 89, loss: 1.5344164371490479\n",
      "epoch: 12, batch: 90, loss: 1.5048750638961792\n",
      "epoch: 12, batch: 91, loss: 1.6106334924697876\n",
      "epoch: 12, batch: 92, loss: 1.6214675903320312\n",
      "epoch: 12, batch: 93, loss: 1.4099342823028564\n",
      "epoch: 12, batch: 94, loss: 1.2906721830368042\n",
      "epoch: 12, batch: 95, loss: 1.4291014671325684\n",
      "epoch: 12, batch: 96, loss: 1.4991798400878906\n",
      "epoch: 12, batch: 97, loss: 1.4579814672470093\n",
      "epoch: 12, batch: 98, loss: 1.5003736019134521\n",
      "epoch: 12, batch: 99, loss: 1.492746114730835\n",
      "epoch: 12, batch: 100, loss: 1.696030616760254\n",
      "epoch: 12, batch: 101, loss: 1.4740430116653442\n",
      "epoch: 12, batch: 102, loss: 1.4467899799346924\n",
      "epoch: 12, batch: 103, loss: 1.4802953004837036\n",
      "epoch: 12, batch: 104, loss: 1.3370356559753418\n",
      "epoch: 12, batch: 105, loss: 1.354992151260376\n",
      "epoch: 12, batch: 106, loss: 1.6037471294403076\n",
      "epoch: 12, batch: 107, loss: 1.5486997365951538\n",
      "epoch: 12, batch: 108, loss: 1.6329987049102783\n",
      "epoch: 12, batch: 109, loss: 1.5878651142120361\n",
      "epoch: 12, batch: 110, loss: 1.5767940282821655\n",
      "epoch: 12, batch: 111, loss: 1.5970425605773926\n",
      "epoch: 12, batch: 112, loss: 1.7247753143310547\n",
      "epoch: 12, batch: 113, loss: 1.5072345733642578\n",
      "epoch: 12, batch: 114, loss: 1.5967124700546265\n",
      "epoch: 12, batch: 115, loss: 1.6202306747436523\n",
      "epoch: 12, batch: 116, loss: 1.4884517192840576\n",
      "epoch: 12, batch: 117, loss: 1.4446802139282227\n",
      "epoch: 12, batch: 118, loss: 1.6199201345443726\n",
      "epoch: 12, batch: 119, loss: 1.632615327835083\n",
      "epoch: 12, batch: 120, loss: 1.3864095211029053\n",
      "epoch: 12, batch: 121, loss: 1.4774736166000366\n",
      "epoch: 12, batch: 122, loss: 1.554955244064331\n",
      "epoch: 12, batch: 123, loss: 1.4858938455581665\n",
      "epoch: 12, batch: 124, loss: 1.6505467891693115\n",
      "epoch: 12, batch: 125, loss: 1.5905823707580566\n",
      "epoch: 12, batch: 126, loss: 1.5844995975494385\n",
      "epoch: 12, batch: 127, loss: 1.5664798021316528\n",
      "epoch: 13, batch: 0, loss: 1.5105805397033691\n",
      "epoch: 13, batch: 1, loss: 1.5157514810562134\n",
      "epoch: 13, batch: 2, loss: 1.4895554780960083\n",
      "epoch: 13, batch: 3, loss: 1.582291841506958\n",
      "epoch: 13, batch: 4, loss: 1.4761155843734741\n",
      "epoch: 13, batch: 5, loss: 1.5463244915008545\n",
      "epoch: 13, batch: 6, loss: 1.489695429801941\n",
      "epoch: 13, batch: 7, loss: 1.4610239267349243\n",
      "epoch: 13, batch: 8, loss: 1.5361672639846802\n",
      "epoch: 13, batch: 9, loss: 1.5447447299957275\n",
      "epoch: 13, batch: 10, loss: 1.5822057723999023\n",
      "epoch: 13, batch: 11, loss: 1.354723572731018\n",
      "epoch: 13, batch: 12, loss: 1.417494297027588\n",
      "epoch: 13, batch: 13, loss: 1.5618287324905396\n",
      "epoch: 13, batch: 14, loss: 1.5563081502914429\n",
      "epoch: 13, batch: 15, loss: 1.6964824199676514\n",
      "epoch: 13, batch: 16, loss: 1.8317439556121826\n",
      "epoch: 13, batch: 17, loss: 1.4715516567230225\n",
      "epoch: 13, batch: 18, loss: 1.4691702127456665\n",
      "epoch: 13, batch: 19, loss: 1.524808645248413\n",
      "epoch: 13, batch: 20, loss: 1.651789903640747\n",
      "epoch: 13, batch: 21, loss: 1.4081566333770752\n",
      "epoch: 13, batch: 22, loss: 1.652600884437561\n",
      "epoch: 13, batch: 23, loss: 1.4525200128555298\n",
      "epoch: 13, batch: 24, loss: 1.4068020582199097\n",
      "epoch: 13, batch: 25, loss: 1.9321075677871704\n",
      "epoch: 13, batch: 26, loss: 1.7159922122955322\n",
      "epoch: 13, batch: 27, loss: 1.6999181509017944\n",
      "epoch: 13, batch: 28, loss: 1.256892442703247\n",
      "epoch: 13, batch: 29, loss: 1.7912566661834717\n",
      "epoch: 13, batch: 30, loss: 1.6330127716064453\n",
      "epoch: 13, batch: 31, loss: 1.7164809703826904\n",
      "epoch: 13, batch: 32, loss: 1.6437017917633057\n",
      "epoch: 13, batch: 33, loss: 1.5295547246932983\n",
      "epoch: 13, batch: 34, loss: 1.6004352569580078\n",
      "epoch: 13, batch: 35, loss: 1.6377754211425781\n",
      "epoch: 13, batch: 36, loss: 1.5926742553710938\n",
      "epoch: 13, batch: 37, loss: 1.3553612232208252\n",
      "epoch: 13, batch: 38, loss: 1.4360482692718506\n",
      "epoch: 13, batch: 39, loss: 1.618938684463501\n",
      "epoch: 13, batch: 40, loss: 1.4671640396118164\n",
      "epoch: 13, batch: 41, loss: 1.458268165588379\n",
      "epoch: 13, batch: 42, loss: 1.3543180227279663\n",
      "epoch: 13, batch: 43, loss: 1.225078821182251\n",
      "epoch: 13, batch: 44, loss: 1.522809624671936\n",
      "epoch: 13, batch: 45, loss: 1.6219583749771118\n",
      "epoch: 13, batch: 46, loss: 1.5825474262237549\n",
      "epoch: 13, batch: 47, loss: 1.502288818359375\n",
      "epoch: 13, batch: 48, loss: 1.5283514261245728\n",
      "epoch: 13, batch: 49, loss: 1.530534029006958\n",
      "epoch: 13, batch: 50, loss: 1.7374557256698608\n",
      "epoch: 13, batch: 51, loss: 1.4144675731658936\n",
      "epoch: 13, batch: 52, loss: 1.4476526975631714\n",
      "epoch: 13, batch: 53, loss: 1.7053238153457642\n",
      "epoch: 13, batch: 54, loss: 1.6863517761230469\n",
      "epoch: 13, batch: 55, loss: 1.7931005954742432\n",
      "epoch: 13, batch: 56, loss: 1.636584997177124\n",
      "epoch: 13, batch: 57, loss: 1.1627203226089478\n",
      "epoch: 13, batch: 58, loss: 1.3866790533065796\n",
      "epoch: 13, batch: 59, loss: 1.4940574169158936\n",
      "epoch: 13, batch: 60, loss: 1.6000064611434937\n",
      "epoch: 13, batch: 61, loss: 1.1732001304626465\n",
      "epoch: 13, batch: 62, loss: 1.571629285812378\n",
      "epoch: 13, batch: 63, loss: 1.4952987432479858\n",
      "epoch: 13, batch: 64, loss: 1.599448800086975\n",
      "epoch: 13, batch: 65, loss: 1.7629998922348022\n",
      "epoch: 13, batch: 66, loss: 1.6254806518554688\n",
      "epoch: 13, batch: 67, loss: 1.307430386543274\n",
      "epoch: 13, batch: 68, loss: 1.3234431743621826\n",
      "epoch: 13, batch: 69, loss: 1.353149175643921\n",
      "epoch: 13, batch: 70, loss: 1.6934159994125366\n",
      "epoch: 13, batch: 71, loss: 1.3974220752716064\n",
      "epoch: 13, batch: 72, loss: 1.5618088245391846\n",
      "epoch: 13, batch: 73, loss: 1.5967603921890259\n",
      "epoch: 13, batch: 74, loss: 1.7078678607940674\n",
      "epoch: 13, batch: 75, loss: 1.8350187540054321\n",
      "epoch: 13, batch: 76, loss: 1.472425103187561\n",
      "epoch: 13, batch: 77, loss: 1.737295389175415\n",
      "epoch: 13, batch: 78, loss: 1.6145786046981812\n",
      "epoch: 13, batch: 79, loss: 1.4450771808624268\n",
      "epoch: 13, batch: 80, loss: 1.3282768726348877\n",
      "epoch: 13, batch: 81, loss: 1.4770089387893677\n",
      "epoch: 13, batch: 82, loss: 1.3572988510131836\n",
      "epoch: 13, batch: 83, loss: 1.3185796737670898\n",
      "epoch: 13, batch: 84, loss: 1.8526086807250977\n",
      "epoch: 13, batch: 85, loss: 1.5607576370239258\n",
      "epoch: 13, batch: 86, loss: 1.3862507343292236\n",
      "epoch: 13, batch: 87, loss: 1.5376449823379517\n",
      "epoch: 13, batch: 88, loss: 1.7047767639160156\n",
      "epoch: 13, batch: 89, loss: 1.4734869003295898\n",
      "epoch: 13, batch: 90, loss: 1.2383034229278564\n",
      "epoch: 13, batch: 91, loss: 1.713927984237671\n",
      "epoch: 13, batch: 92, loss: 1.4390519857406616\n",
      "epoch: 13, batch: 93, loss: 1.5762780904769897\n",
      "epoch: 13, batch: 94, loss: 1.4543219804763794\n",
      "epoch: 13, batch: 95, loss: 1.5533164739608765\n",
      "epoch: 13, batch: 96, loss: 1.7660160064697266\n",
      "epoch: 13, batch: 97, loss: 1.3708534240722656\n",
      "epoch: 13, batch: 98, loss: 1.6216990947723389\n",
      "epoch: 13, batch: 99, loss: 1.398704171180725\n",
      "epoch: 13, batch: 100, loss: 1.4529907703399658\n",
      "epoch: 13, batch: 101, loss: 1.4097011089324951\n",
      "epoch: 13, batch: 102, loss: 1.7053301334381104\n",
      "epoch: 13, batch: 103, loss: 1.7491991519927979\n",
      "epoch: 13, batch: 104, loss: 1.3857498168945312\n",
      "epoch: 13, batch: 105, loss: 1.6977819204330444\n",
      "epoch: 13, batch: 106, loss: 1.3528965711593628\n",
      "epoch: 13, batch: 107, loss: 1.4784313440322876\n",
      "epoch: 13, batch: 108, loss: 1.531842589378357\n",
      "epoch: 13, batch: 109, loss: 1.3058748245239258\n",
      "epoch: 13, batch: 110, loss: 1.4112396240234375\n",
      "epoch: 13, batch: 111, loss: 1.5115982294082642\n",
      "epoch: 13, batch: 112, loss: 1.7297956943511963\n",
      "epoch: 13, batch: 113, loss: 1.7971134185791016\n",
      "epoch: 13, batch: 114, loss: 1.3482309579849243\n",
      "epoch: 13, batch: 115, loss: 1.487747311592102\n",
      "epoch: 13, batch: 116, loss: 1.4723937511444092\n",
      "epoch: 13, batch: 117, loss: 1.773680329322815\n",
      "epoch: 13, batch: 118, loss: 1.3458538055419922\n",
      "epoch: 13, batch: 119, loss: 1.736952543258667\n",
      "epoch: 13, batch: 120, loss: 1.429133415222168\n",
      "epoch: 13, batch: 121, loss: 1.3840832710266113\n",
      "epoch: 13, batch: 122, loss: 1.6457948684692383\n",
      "epoch: 13, batch: 123, loss: 1.519769549369812\n",
      "epoch: 13, batch: 124, loss: 1.4069788455963135\n",
      "epoch: 13, batch: 125, loss: 1.4358785152435303\n",
      "epoch: 13, batch: 126, loss: 1.5274202823638916\n",
      "epoch: 13, batch: 127, loss: 1.6987078189849854\n",
      "epoch: 14, batch: 0, loss: 1.6904436349868774\n",
      "epoch: 14, batch: 1, loss: 1.3057130575180054\n",
      "epoch: 14, batch: 2, loss: 1.350784182548523\n",
      "epoch: 14, batch: 3, loss: 1.6395771503448486\n",
      "epoch: 14, batch: 4, loss: 1.485263466835022\n",
      "epoch: 14, batch: 5, loss: 1.948540449142456\n",
      "epoch: 14, batch: 6, loss: 1.829785943031311\n",
      "epoch: 14, batch: 7, loss: 1.7219693660736084\n",
      "epoch: 14, batch: 8, loss: 1.8153400421142578\n",
      "epoch: 14, batch: 9, loss: 1.9387801885604858\n",
      "epoch: 14, batch: 10, loss: 1.5182464122772217\n",
      "epoch: 14, batch: 11, loss: 1.4204858541488647\n",
      "epoch: 14, batch: 12, loss: 1.4173376560211182\n",
      "epoch: 14, batch: 13, loss: 1.5026216506958008\n",
      "epoch: 14, batch: 14, loss: 1.4238216876983643\n",
      "epoch: 14, batch: 15, loss: 1.4840567111968994\n",
      "epoch: 14, batch: 16, loss: 1.5402395725250244\n",
      "epoch: 14, batch: 17, loss: 1.5735278129577637\n",
      "epoch: 14, batch: 18, loss: 1.4096572399139404\n",
      "epoch: 14, batch: 19, loss: 1.5792986154556274\n",
      "epoch: 14, batch: 20, loss: 1.7459468841552734\n",
      "epoch: 14, batch: 21, loss: 1.4432936906814575\n",
      "epoch: 14, batch: 22, loss: 1.6685043573379517\n",
      "epoch: 14, batch: 23, loss: 1.366931676864624\n",
      "epoch: 14, batch: 24, loss: 1.6449352502822876\n",
      "epoch: 14, batch: 25, loss: 1.1275646686553955\n",
      "epoch: 14, batch: 26, loss: 1.8585755825042725\n",
      "epoch: 14, batch: 27, loss: 2.0136711597442627\n",
      "epoch: 14, batch: 28, loss: 1.3748352527618408\n",
      "epoch: 14, batch: 29, loss: 1.549859881401062\n",
      "epoch: 14, batch: 30, loss: 2.0430045127868652\n",
      "epoch: 14, batch: 31, loss: 1.0984406471252441\n",
      "epoch: 14, batch: 32, loss: 1.5603187084197998\n",
      "epoch: 14, batch: 33, loss: 1.5743467807769775\n",
      "epoch: 14, batch: 34, loss: 1.5017485618591309\n",
      "epoch: 14, batch: 35, loss: 1.3285324573516846\n",
      "epoch: 14, batch: 36, loss: 1.4370832443237305\n",
      "epoch: 14, batch: 37, loss: 1.430591106414795\n",
      "epoch: 14, batch: 38, loss: 1.3503786325454712\n",
      "epoch: 14, batch: 39, loss: 1.503336787223816\n",
      "epoch: 14, batch: 40, loss: 2.1083500385284424\n",
      "epoch: 14, batch: 41, loss: 1.4260444641113281\n",
      "epoch: 14, batch: 42, loss: 1.7876758575439453\n",
      "epoch: 14, batch: 43, loss: 1.640074372291565\n",
      "epoch: 14, batch: 44, loss: 1.2394812107086182\n",
      "epoch: 14, batch: 45, loss: 1.2314999103546143\n",
      "epoch: 14, batch: 46, loss: 1.6440351009368896\n",
      "epoch: 14, batch: 47, loss: 1.602968454360962\n",
      "epoch: 14, batch: 48, loss: 1.3899794816970825\n",
      "epoch: 14, batch: 49, loss: 1.5548088550567627\n",
      "epoch: 14, batch: 50, loss: 1.2616112232208252\n",
      "epoch: 14, batch: 51, loss: 1.5788508653640747\n",
      "epoch: 14, batch: 52, loss: 1.952855110168457\n",
      "epoch: 14, batch: 53, loss: 1.9624879360198975\n",
      "epoch: 14, batch: 54, loss: 1.7010676860809326\n",
      "epoch: 14, batch: 55, loss: 1.7410789728164673\n",
      "epoch: 14, batch: 56, loss: 1.7570664882659912\n",
      "epoch: 14, batch: 57, loss: 1.9253860712051392\n",
      "epoch: 14, batch: 58, loss: 1.4919668436050415\n",
      "epoch: 14, batch: 59, loss: 1.7926712036132812\n",
      "epoch: 14, batch: 60, loss: 1.467245101928711\n",
      "epoch: 14, batch: 61, loss: 1.431161880493164\n",
      "epoch: 14, batch: 62, loss: 1.3445996046066284\n",
      "epoch: 14, batch: 63, loss: 1.8447325229644775\n",
      "epoch: 14, batch: 64, loss: 1.368213415145874\n",
      "epoch: 14, batch: 65, loss: 1.4840891361236572\n",
      "epoch: 14, batch: 66, loss: 1.3816403150558472\n",
      "epoch: 14, batch: 67, loss: 1.7501413822174072\n",
      "epoch: 14, batch: 68, loss: 1.4208053350448608\n",
      "epoch: 14, batch: 69, loss: 1.790935754776001\n",
      "epoch: 14, batch: 70, loss: 1.788069725036621\n",
      "epoch: 14, batch: 71, loss: 1.5204730033874512\n",
      "epoch: 14, batch: 72, loss: 1.2400877475738525\n",
      "epoch: 14, batch: 73, loss: 1.201082468032837\n",
      "epoch: 14, batch: 74, loss: 1.7062263488769531\n",
      "epoch: 14, batch: 75, loss: 1.7527843713760376\n",
      "epoch: 14, batch: 76, loss: 1.3057692050933838\n",
      "epoch: 14, batch: 77, loss: 1.3344643115997314\n",
      "epoch: 14, batch: 78, loss: 1.8037973642349243\n",
      "epoch: 14, batch: 79, loss: 1.2154414653778076\n",
      "epoch: 14, batch: 80, loss: 1.3735321760177612\n",
      "epoch: 14, batch: 81, loss: 1.5557600259780884\n",
      "epoch: 14, batch: 82, loss: 1.351722002029419\n",
      "epoch: 14, batch: 83, loss: 1.8789710998535156\n",
      "epoch: 14, batch: 84, loss: 1.0716688632965088\n",
      "epoch: 14, batch: 85, loss: 1.3867870569229126\n",
      "epoch: 14, batch: 86, loss: 1.8809703588485718\n",
      "epoch: 14, batch: 87, loss: 1.8195044994354248\n",
      "epoch: 14, batch: 88, loss: 1.5588643550872803\n",
      "epoch: 14, batch: 89, loss: 1.5325902700424194\n",
      "epoch: 14, batch: 90, loss: 1.3967632055282593\n",
      "epoch: 14, batch: 91, loss: 1.3825666904449463\n",
      "epoch: 14, batch: 92, loss: 1.352750301361084\n",
      "epoch: 14, batch: 93, loss: 1.6592304706573486\n",
      "epoch: 14, batch: 94, loss: 1.673906922340393\n",
      "epoch: 14, batch: 95, loss: 1.5560822486877441\n",
      "epoch: 14, batch: 96, loss: 1.3168246746063232\n",
      "epoch: 14, batch: 97, loss: 1.4986679553985596\n",
      "epoch: 14, batch: 98, loss: 1.516073226928711\n",
      "epoch: 14, batch: 99, loss: 1.8502260446548462\n",
      "epoch: 14, batch: 100, loss: 1.4737417697906494\n",
      "epoch: 14, batch: 101, loss: 1.3292487859725952\n",
      "epoch: 14, batch: 102, loss: 1.6401958465576172\n",
      "epoch: 14, batch: 103, loss: 1.3386112451553345\n",
      "epoch: 14, batch: 104, loss: 1.284008502960205\n",
      "epoch: 14, batch: 105, loss: 1.7441049814224243\n",
      "epoch: 14, batch: 106, loss: 1.7757256031036377\n",
      "epoch: 14, batch: 107, loss: 1.7264655828475952\n",
      "epoch: 14, batch: 108, loss: 1.547442078590393\n",
      "epoch: 14, batch: 109, loss: 1.318879246711731\n",
      "epoch: 14, batch: 110, loss: 1.6343361139297485\n",
      "epoch: 14, batch: 111, loss: 1.3938541412353516\n",
      "epoch: 14, batch: 112, loss: 1.4643042087554932\n",
      "epoch: 14, batch: 113, loss: 1.692539930343628\n",
      "epoch: 14, batch: 114, loss: 1.416261911392212\n",
      "epoch: 14, batch: 115, loss: 1.1833112239837646\n",
      "epoch: 14, batch: 116, loss: 1.4611666202545166\n",
      "epoch: 14, batch: 117, loss: 1.6274563074111938\n",
      "epoch: 14, batch: 118, loss: 1.5939708948135376\n",
      "epoch: 14, batch: 119, loss: 1.583231806755066\n",
      "epoch: 14, batch: 120, loss: 1.6954066753387451\n",
      "epoch: 14, batch: 121, loss: 1.5220106840133667\n",
      "epoch: 14, batch: 122, loss: 1.3763338327407837\n",
      "epoch: 14, batch: 123, loss: 1.3283286094665527\n",
      "epoch: 14, batch: 124, loss: 1.3481067419052124\n",
      "epoch: 14, batch: 125, loss: 1.5783908367156982\n",
      "epoch: 14, batch: 126, loss: 1.4341284036636353\n",
      "epoch: 14, batch: 127, loss: 1.3829017877578735\n",
      "epoch: 15, batch: 0, loss: 1.473339319229126\n",
      "epoch: 15, batch: 1, loss: 1.413881540298462\n",
      "epoch: 15, batch: 2, loss: 1.5281364917755127\n",
      "epoch: 15, batch: 3, loss: 1.569845199584961\n",
      "epoch: 15, batch: 4, loss: 1.5395004749298096\n",
      "epoch: 15, batch: 5, loss: 1.545042634010315\n",
      "epoch: 15, batch: 6, loss: 1.4646971225738525\n",
      "epoch: 15, batch: 7, loss: 1.4841861724853516\n",
      "epoch: 15, batch: 8, loss: 1.3929413557052612\n",
      "epoch: 15, batch: 9, loss: 1.9256401062011719\n",
      "epoch: 15, batch: 10, loss: 1.5394312143325806\n",
      "epoch: 15, batch: 11, loss: 1.635048508644104\n",
      "epoch: 15, batch: 12, loss: 1.337730884552002\n",
      "epoch: 15, batch: 13, loss: 1.6348416805267334\n",
      "epoch: 15, batch: 14, loss: 1.7895278930664062\n",
      "epoch: 15, batch: 15, loss: 1.4033527374267578\n",
      "epoch: 15, batch: 16, loss: 1.6712074279785156\n",
      "epoch: 15, batch: 17, loss: 1.4832713603973389\n",
      "epoch: 15, batch: 18, loss: 1.450930118560791\n",
      "epoch: 15, batch: 19, loss: 1.6465142965316772\n",
      "epoch: 15, batch: 20, loss: 1.5953011512756348\n",
      "epoch: 15, batch: 21, loss: 1.5216064453125\n",
      "epoch: 15, batch: 22, loss: 1.6944564580917358\n",
      "epoch: 15, batch: 23, loss: 1.5164875984191895\n",
      "epoch: 15, batch: 24, loss: 1.8185169696807861\n",
      "epoch: 15, batch: 25, loss: 1.5216974020004272\n",
      "epoch: 15, batch: 26, loss: 1.2752879858016968\n",
      "epoch: 15, batch: 27, loss: 1.54643714427948\n",
      "epoch: 15, batch: 28, loss: 1.5824960470199585\n",
      "epoch: 15, batch: 29, loss: 1.4254233837127686\n",
      "epoch: 15, batch: 30, loss: 1.4584401845932007\n",
      "epoch: 15, batch: 31, loss: 1.6003471612930298\n",
      "epoch: 15, batch: 32, loss: 1.4590554237365723\n",
      "epoch: 15, batch: 33, loss: 1.3939803838729858\n",
      "epoch: 15, batch: 34, loss: 1.4584102630615234\n",
      "epoch: 15, batch: 35, loss: 1.4637537002563477\n",
      "epoch: 15, batch: 36, loss: 1.3843443393707275\n",
      "epoch: 15, batch: 37, loss: 1.5484697818756104\n",
      "epoch: 15, batch: 38, loss: 1.707311987876892\n",
      "epoch: 15, batch: 39, loss: 1.3236937522888184\n",
      "epoch: 15, batch: 40, loss: 1.275503396987915\n",
      "epoch: 15, batch: 41, loss: 1.3979343175888062\n",
      "epoch: 15, batch: 42, loss: 1.5338599681854248\n",
      "epoch: 15, batch: 43, loss: 1.6209052801132202\n",
      "epoch: 15, batch: 44, loss: 1.5919444561004639\n",
      "epoch: 15, batch: 45, loss: 2.022150754928589\n",
      "epoch: 15, batch: 46, loss: 1.5800120830535889\n",
      "epoch: 15, batch: 47, loss: 1.5615003108978271\n",
      "epoch: 15, batch: 48, loss: 1.4984476566314697\n",
      "epoch: 15, batch: 49, loss: 1.4584256410598755\n",
      "epoch: 15, batch: 50, loss: 1.6010076999664307\n",
      "epoch: 15, batch: 51, loss: 1.5727264881134033\n",
      "epoch: 15, batch: 52, loss: 1.3521318435668945\n",
      "epoch: 15, batch: 53, loss: 1.5957577228546143\n",
      "epoch: 15, batch: 54, loss: 1.0057215690612793\n",
      "epoch: 15, batch: 55, loss: 1.4736032485961914\n",
      "epoch: 15, batch: 56, loss: 1.7689628601074219\n",
      "epoch: 15, batch: 57, loss: 1.312135934829712\n",
      "epoch: 15, batch: 58, loss: 1.4367130994796753\n",
      "epoch: 15, batch: 59, loss: 1.6474727392196655\n",
      "epoch: 15, batch: 60, loss: 1.6408958435058594\n",
      "epoch: 15, batch: 61, loss: 1.6517528295516968\n",
      "epoch: 15, batch: 62, loss: 1.7421224117279053\n",
      "epoch: 15, batch: 63, loss: 1.7251325845718384\n",
      "epoch: 15, batch: 64, loss: 1.3263717889785767\n",
      "epoch: 15, batch: 65, loss: 1.3457139730453491\n",
      "epoch: 15, batch: 66, loss: 1.6688134670257568\n",
      "epoch: 15, batch: 67, loss: 2.0152859687805176\n",
      "epoch: 15, batch: 68, loss: 1.525286316871643\n",
      "epoch: 15, batch: 69, loss: 1.4807392358779907\n",
      "epoch: 15, batch: 70, loss: 1.658713698387146\n",
      "epoch: 15, batch: 71, loss: 1.557297945022583\n",
      "epoch: 15, batch: 72, loss: 1.8370517492294312\n",
      "epoch: 15, batch: 73, loss: 1.1412814855575562\n",
      "epoch: 15, batch: 74, loss: 1.4124577045440674\n",
      "epoch: 15, batch: 75, loss: 1.3639541864395142\n",
      "epoch: 15, batch: 76, loss: 1.3135130405426025\n",
      "epoch: 15, batch: 77, loss: 1.317807674407959\n",
      "epoch: 15, batch: 78, loss: 1.840428352355957\n",
      "epoch: 15, batch: 79, loss: 1.7059234380722046\n",
      "epoch: 15, batch: 80, loss: 1.5505454540252686\n",
      "epoch: 15, batch: 81, loss: 1.2435054779052734\n",
      "epoch: 15, batch: 82, loss: 1.8319499492645264\n",
      "epoch: 15, batch: 83, loss: 1.2633335590362549\n",
      "epoch: 15, batch: 84, loss: 1.3790758848190308\n",
      "epoch: 15, batch: 85, loss: 1.720113754272461\n",
      "epoch: 15, batch: 86, loss: 1.5400216579437256\n",
      "epoch: 15, batch: 87, loss: 1.3509972095489502\n",
      "epoch: 15, batch: 88, loss: 1.6812406778335571\n",
      "epoch: 15, batch: 89, loss: 1.0912435054779053\n",
      "epoch: 15, batch: 90, loss: 1.8288993835449219\n",
      "epoch: 15, batch: 91, loss: 1.4477189779281616\n",
      "epoch: 15, batch: 92, loss: 1.777799367904663\n",
      "epoch: 15, batch: 93, loss: 1.9496856927871704\n",
      "epoch: 15, batch: 94, loss: 1.3184411525726318\n",
      "epoch: 15, batch: 95, loss: 1.4207909107208252\n",
      "epoch: 15, batch: 96, loss: 1.300716519355774\n",
      "epoch: 15, batch: 97, loss: 1.7558019161224365\n",
      "epoch: 15, batch: 98, loss: 1.3694289922714233\n",
      "epoch: 15, batch: 99, loss: 1.5438518524169922\n",
      "epoch: 15, batch: 100, loss: 1.5162073373794556\n",
      "epoch: 15, batch: 101, loss: 1.6162045001983643\n",
      "epoch: 15, batch: 102, loss: 1.154363989830017\n",
      "epoch: 15, batch: 103, loss: 1.4675830602645874\n",
      "epoch: 15, batch: 104, loss: 1.597212314605713\n",
      "epoch: 15, batch: 105, loss: 1.374677300453186\n",
      "epoch: 15, batch: 106, loss: 1.6796003580093384\n",
      "epoch: 15, batch: 107, loss: 1.6545034646987915\n",
      "epoch: 15, batch: 108, loss: 1.4820165634155273\n",
      "epoch: 15, batch: 109, loss: 1.3540165424346924\n",
      "epoch: 15, batch: 110, loss: 1.5317034721374512\n",
      "epoch: 15, batch: 111, loss: 1.670464277267456\n",
      "epoch: 15, batch: 112, loss: 1.4159003496170044\n",
      "epoch: 15, batch: 113, loss: 1.42799973487854\n",
      "epoch: 15, batch: 114, loss: 1.6839065551757812\n",
      "epoch: 15, batch: 115, loss: 1.747534155845642\n",
      "epoch: 15, batch: 116, loss: 1.5353891849517822\n",
      "epoch: 15, batch: 117, loss: 1.546579360961914\n",
      "epoch: 15, batch: 118, loss: 1.49955153465271\n",
      "epoch: 15, batch: 119, loss: 1.8477599620819092\n",
      "epoch: 15, batch: 120, loss: 1.2092078924179077\n",
      "epoch: 15, batch: 121, loss: 1.4170477390289307\n",
      "epoch: 15, batch: 122, loss: 1.344586968421936\n",
      "epoch: 15, batch: 123, loss: 1.5918382406234741\n",
      "epoch: 15, batch: 124, loss: 1.6896255016326904\n",
      "epoch: 15, batch: 125, loss: 1.5299245119094849\n",
      "epoch: 15, batch: 126, loss: 1.3852941989898682\n",
      "epoch: 15, batch: 127, loss: 1.8025530576705933\n",
      "epoch: 16, batch: 0, loss: 1.7317094802856445\n",
      "epoch: 16, batch: 1, loss: 1.5102393627166748\n",
      "epoch: 16, batch: 2, loss: 1.38619065284729\n",
      "epoch: 16, batch: 3, loss: 1.7184604406356812\n",
      "epoch: 16, batch: 4, loss: 1.4778461456298828\n",
      "epoch: 16, batch: 5, loss: 1.5550700426101685\n",
      "epoch: 16, batch: 6, loss: 1.3638017177581787\n",
      "epoch: 16, batch: 7, loss: 1.5510207414627075\n",
      "epoch: 16, batch: 8, loss: 1.98789381980896\n",
      "epoch: 16, batch: 9, loss: 1.6547151803970337\n",
      "epoch: 16, batch: 10, loss: 1.5935890674591064\n",
      "epoch: 16, batch: 11, loss: 1.5522366762161255\n",
      "epoch: 16, batch: 12, loss: 1.5498063564300537\n",
      "epoch: 16, batch: 13, loss: 1.4560768604278564\n",
      "epoch: 16, batch: 14, loss: 1.5751365423202515\n",
      "epoch: 16, batch: 15, loss: 1.5075058937072754\n",
      "epoch: 16, batch: 16, loss: 1.8474254608154297\n",
      "epoch: 16, batch: 17, loss: 1.4156638383865356\n",
      "epoch: 16, batch: 18, loss: 1.4299638271331787\n",
      "epoch: 16, batch: 19, loss: 1.4492571353912354\n",
      "epoch: 16, batch: 20, loss: 1.452487587928772\n",
      "epoch: 16, batch: 21, loss: 1.4132840633392334\n",
      "epoch: 16, batch: 22, loss: 1.6116873025894165\n",
      "epoch: 16, batch: 23, loss: 1.566684365272522\n",
      "epoch: 16, batch: 24, loss: 1.4889990091323853\n",
      "epoch: 16, batch: 25, loss: 1.4970539808273315\n",
      "epoch: 16, batch: 26, loss: 1.4592442512512207\n",
      "epoch: 16, batch: 27, loss: 1.5037826299667358\n",
      "epoch: 16, batch: 28, loss: 1.3614763021469116\n",
      "epoch: 16, batch: 29, loss: 1.364243984222412\n",
      "epoch: 16, batch: 30, loss: 1.3530402183532715\n",
      "epoch: 16, batch: 31, loss: 1.4349313974380493\n",
      "epoch: 16, batch: 32, loss: 1.5267258882522583\n",
      "epoch: 16, batch: 33, loss: 1.8018465042114258\n",
      "epoch: 16, batch: 34, loss: 1.4568407535552979\n",
      "epoch: 16, batch: 35, loss: 1.6178693771362305\n",
      "epoch: 16, batch: 36, loss: 1.549588918685913\n",
      "epoch: 16, batch: 37, loss: 1.2948534488677979\n",
      "epoch: 16, batch: 38, loss: 1.708428144454956\n",
      "epoch: 16, batch: 39, loss: 1.401682734489441\n",
      "epoch: 16, batch: 40, loss: 1.4510838985443115\n",
      "epoch: 16, batch: 41, loss: 1.4535863399505615\n",
      "epoch: 16, batch: 42, loss: 1.4627037048339844\n",
      "epoch: 16, batch: 43, loss: 1.4053140878677368\n",
      "epoch: 16, batch: 44, loss: 1.490287184715271\n",
      "epoch: 16, batch: 45, loss: 1.5108636617660522\n",
      "epoch: 16, batch: 46, loss: 1.4730554819107056\n",
      "epoch: 16, batch: 47, loss: 1.6207443475723267\n",
      "epoch: 16, batch: 48, loss: 1.5784006118774414\n",
      "epoch: 16, batch: 49, loss: 1.5541797876358032\n",
      "epoch: 16, batch: 50, loss: 1.5850400924682617\n",
      "epoch: 16, batch: 51, loss: 1.5732793807983398\n",
      "epoch: 16, batch: 52, loss: 1.5507771968841553\n",
      "epoch: 16, batch: 53, loss: 1.5215154886245728\n",
      "epoch: 16, batch: 54, loss: 1.8299487829208374\n",
      "epoch: 16, batch: 55, loss: 1.4716328382492065\n",
      "epoch: 16, batch: 56, loss: 1.5669479370117188\n",
      "epoch: 16, batch: 57, loss: 1.4618812799453735\n",
      "epoch: 16, batch: 58, loss: 1.5301648378372192\n",
      "epoch: 16, batch: 59, loss: 1.6038318872451782\n",
      "epoch: 16, batch: 60, loss: 1.5162770748138428\n",
      "epoch: 16, batch: 61, loss: 1.4671059846878052\n",
      "epoch: 16, batch: 62, loss: 1.6706514358520508\n",
      "epoch: 16, batch: 63, loss: 1.466579794883728\n",
      "epoch: 16, batch: 64, loss: 1.5682982206344604\n",
      "epoch: 16, batch: 65, loss: 1.5021634101867676\n",
      "epoch: 16, batch: 66, loss: 1.4158549308776855\n",
      "epoch: 16, batch: 67, loss: 1.4909170866012573\n",
      "epoch: 16, batch: 68, loss: 1.6191142797470093\n",
      "epoch: 16, batch: 69, loss: 1.376146674156189\n",
      "epoch: 16, batch: 70, loss: 1.4665967226028442\n",
      "epoch: 16, batch: 71, loss: 1.437513828277588\n",
      "epoch: 16, batch: 72, loss: 1.4455667734146118\n",
      "epoch: 16, batch: 73, loss: 1.5313599109649658\n",
      "epoch: 16, batch: 74, loss: 1.305456519126892\n",
      "epoch: 16, batch: 75, loss: 1.702925443649292\n",
      "epoch: 16, batch: 76, loss: 1.2916300296783447\n",
      "epoch: 16, batch: 77, loss: 1.6246654987335205\n",
      "epoch: 16, batch: 78, loss: 1.5035345554351807\n",
      "epoch: 16, batch: 79, loss: 1.4661178588867188\n",
      "epoch: 16, batch: 80, loss: 1.7665560245513916\n",
      "epoch: 16, batch: 81, loss: 1.1865732669830322\n",
      "epoch: 16, batch: 82, loss: 1.332358479499817\n",
      "epoch: 16, batch: 83, loss: 1.6358808279037476\n",
      "epoch: 16, batch: 84, loss: 1.629076600074768\n",
      "epoch: 16, batch: 85, loss: 1.2370986938476562\n",
      "epoch: 16, batch: 86, loss: 1.5840578079223633\n",
      "epoch: 16, batch: 87, loss: 1.4514299631118774\n",
      "epoch: 16, batch: 88, loss: 1.4913086891174316\n",
      "epoch: 16, batch: 89, loss: 1.3392126560211182\n",
      "epoch: 16, batch: 90, loss: 1.61044180393219\n",
      "epoch: 16, batch: 91, loss: 1.5592219829559326\n",
      "epoch: 16, batch: 92, loss: 1.6633424758911133\n",
      "epoch: 16, batch: 93, loss: 1.618191123008728\n",
      "epoch: 16, batch: 94, loss: 1.7867088317871094\n",
      "epoch: 16, batch: 95, loss: 1.3567291498184204\n",
      "epoch: 16, batch: 96, loss: 1.384856939315796\n",
      "epoch: 16, batch: 97, loss: 1.4000202417373657\n",
      "epoch: 16, batch: 98, loss: 1.7207248210906982\n",
      "epoch: 16, batch: 99, loss: 1.3526475429534912\n",
      "epoch: 16, batch: 100, loss: 1.788901925086975\n",
      "epoch: 16, batch: 101, loss: 1.573993444442749\n",
      "epoch: 16, batch: 102, loss: 1.6615760326385498\n",
      "epoch: 16, batch: 103, loss: 1.4706518650054932\n",
      "epoch: 16, batch: 104, loss: 1.505720853805542\n",
      "epoch: 16, batch: 105, loss: 1.8269987106323242\n",
      "epoch: 16, batch: 106, loss: 1.474261999130249\n",
      "epoch: 16, batch: 107, loss: 1.4055076837539673\n",
      "epoch: 16, batch: 108, loss: 1.6972682476043701\n",
      "epoch: 16, batch: 109, loss: 1.6885372400283813\n",
      "epoch: 16, batch: 110, loss: 1.5302413702011108\n",
      "epoch: 16, batch: 111, loss: 1.544180154800415\n",
      "epoch: 16, batch: 112, loss: 1.545411467552185\n",
      "epoch: 16, batch: 113, loss: 1.5191463232040405\n",
      "epoch: 16, batch: 114, loss: 1.4557278156280518\n",
      "epoch: 16, batch: 115, loss: 1.2874724864959717\n",
      "epoch: 16, batch: 116, loss: 1.7618436813354492\n",
      "epoch: 16, batch: 117, loss: 1.4740852117538452\n",
      "epoch: 16, batch: 118, loss: 1.6889760494232178\n",
      "epoch: 16, batch: 119, loss: 1.040088415145874\n",
      "epoch: 16, batch: 120, loss: 1.6225900650024414\n",
      "epoch: 16, batch: 121, loss: 1.5913057327270508\n",
      "epoch: 16, batch: 122, loss: 1.5752122402191162\n",
      "epoch: 16, batch: 123, loss: 1.6512258052825928\n",
      "epoch: 16, batch: 124, loss: 1.2343430519104004\n",
      "epoch: 16, batch: 125, loss: 1.4896697998046875\n",
      "epoch: 16, batch: 126, loss: 1.7672725915908813\n",
      "epoch: 16, batch: 127, loss: 1.7909598350524902\n",
      "epoch: 17, batch: 0, loss: 1.45730459690094\n",
      "epoch: 17, batch: 1, loss: 1.5896029472351074\n",
      "epoch: 17, batch: 2, loss: 1.4288763999938965\n",
      "epoch: 17, batch: 3, loss: 1.62588632106781\n",
      "epoch: 17, batch: 4, loss: 1.4008915424346924\n",
      "epoch: 17, batch: 5, loss: 1.7272714376449585\n",
      "epoch: 17, batch: 6, loss: 1.3696463108062744\n",
      "epoch: 17, batch: 7, loss: 1.5028624534606934\n",
      "epoch: 17, batch: 8, loss: 1.5862163305282593\n",
      "epoch: 17, batch: 9, loss: 1.3726259469985962\n",
      "epoch: 17, batch: 10, loss: 1.3995994329452515\n",
      "epoch: 17, batch: 11, loss: 1.589597463607788\n",
      "epoch: 17, batch: 12, loss: 1.6301336288452148\n",
      "epoch: 17, batch: 13, loss: 1.620540976524353\n",
      "epoch: 17, batch: 14, loss: 1.7178547382354736\n",
      "epoch: 17, batch: 15, loss: 1.9660718441009521\n",
      "epoch: 17, batch: 16, loss: 1.2710692882537842\n",
      "epoch: 17, batch: 17, loss: 1.4870952367782593\n",
      "epoch: 17, batch: 18, loss: 1.3567270040512085\n",
      "epoch: 17, batch: 19, loss: 1.4619678258895874\n",
      "epoch: 17, batch: 20, loss: 1.782106637954712\n",
      "epoch: 17, batch: 21, loss: 1.2485934495925903\n",
      "epoch: 17, batch: 22, loss: 1.5851809978485107\n",
      "epoch: 17, batch: 23, loss: 1.536953330039978\n",
      "epoch: 17, batch: 24, loss: 1.528308391571045\n",
      "epoch: 17, batch: 25, loss: 1.623500108718872\n",
      "epoch: 17, batch: 26, loss: 1.44573175907135\n",
      "epoch: 17, batch: 27, loss: 1.5732018947601318\n",
      "epoch: 17, batch: 28, loss: 1.6033375263214111\n",
      "epoch: 17, batch: 29, loss: 1.6923011541366577\n",
      "epoch: 17, batch: 30, loss: 1.434602975845337\n",
      "epoch: 17, batch: 31, loss: 1.4255173206329346\n",
      "epoch: 17, batch: 32, loss: 1.4817558526992798\n",
      "epoch: 17, batch: 33, loss: 1.6137263774871826\n",
      "epoch: 17, batch: 34, loss: 1.6646385192871094\n",
      "epoch: 17, batch: 35, loss: 1.4717565774917603\n",
      "epoch: 17, batch: 36, loss: 1.6477760076522827\n",
      "epoch: 17, batch: 37, loss: 1.725385308265686\n",
      "epoch: 17, batch: 38, loss: 1.5268608331680298\n",
      "epoch: 17, batch: 39, loss: 1.4957524538040161\n",
      "epoch: 17, batch: 40, loss: 1.6621487140655518\n",
      "epoch: 17, batch: 41, loss: 1.3895341157913208\n",
      "epoch: 17, batch: 42, loss: 1.5375465154647827\n",
      "epoch: 17, batch: 43, loss: 1.5144290924072266\n",
      "epoch: 17, batch: 44, loss: 1.6920121908187866\n",
      "epoch: 17, batch: 45, loss: 1.9529081583023071\n",
      "epoch: 17, batch: 46, loss: 1.3715927600860596\n",
      "epoch: 17, batch: 47, loss: 1.4503755569458008\n",
      "epoch: 17, batch: 48, loss: 1.4515666961669922\n",
      "epoch: 17, batch: 49, loss: 1.5567386150360107\n",
      "epoch: 17, batch: 50, loss: 1.5800433158874512\n",
      "epoch: 17, batch: 51, loss: 1.7074944972991943\n",
      "epoch: 17, batch: 52, loss: 1.2768458127975464\n",
      "epoch: 17, batch: 53, loss: 1.6476995944976807\n",
      "epoch: 17, batch: 54, loss: 1.3708055019378662\n",
      "epoch: 17, batch: 55, loss: 1.0813162326812744\n",
      "epoch: 17, batch: 56, loss: 1.4801380634307861\n",
      "epoch: 17, batch: 57, loss: 1.7909176349639893\n",
      "epoch: 17, batch: 58, loss: 1.4058789014816284\n",
      "epoch: 17, batch: 59, loss: 1.776839017868042\n",
      "epoch: 17, batch: 60, loss: 1.6380491256713867\n",
      "epoch: 17, batch: 61, loss: 1.8272110223770142\n",
      "epoch: 17, batch: 62, loss: 1.4037727117538452\n",
      "epoch: 17, batch: 63, loss: 1.796532392501831\n",
      "epoch: 17, batch: 64, loss: 1.3319557905197144\n",
      "epoch: 17, batch: 65, loss: 1.8442624807357788\n",
      "epoch: 17, batch: 66, loss: 1.5236563682556152\n",
      "epoch: 17, batch: 67, loss: 1.8123458623886108\n",
      "epoch: 17, batch: 68, loss: 1.513353943824768\n",
      "epoch: 17, batch: 69, loss: 1.2513309717178345\n",
      "epoch: 17, batch: 70, loss: 1.4989663362503052\n",
      "epoch: 17, batch: 71, loss: 1.693699836730957\n",
      "epoch: 17, batch: 72, loss: 1.2976148128509521\n",
      "epoch: 17, batch: 73, loss: 1.476711392402649\n",
      "epoch: 17, batch: 74, loss: 1.3907802104949951\n",
      "epoch: 17, batch: 75, loss: 1.2628198862075806\n",
      "epoch: 17, batch: 76, loss: 1.6292059421539307\n",
      "epoch: 17, batch: 77, loss: 1.5507289171218872\n",
      "epoch: 17, batch: 78, loss: 1.5918588638305664\n",
      "epoch: 17, batch: 79, loss: 1.3823356628417969\n",
      "epoch: 17, batch: 80, loss: 1.350250244140625\n",
      "epoch: 17, batch: 81, loss: 1.8046687841415405\n",
      "epoch: 17, batch: 82, loss: 1.328036904335022\n",
      "epoch: 17, batch: 83, loss: 1.4138633012771606\n",
      "epoch: 17, batch: 84, loss: 1.486879587173462\n",
      "epoch: 17, batch: 85, loss: 1.5783989429473877\n",
      "epoch: 17, batch: 86, loss: 1.4238985776901245\n",
      "epoch: 17, batch: 87, loss: 1.691481590270996\n",
      "epoch: 17, batch: 88, loss: 1.4490472078323364\n",
      "epoch: 17, batch: 89, loss: 1.4893205165863037\n",
      "epoch: 17, batch: 90, loss: 1.2428195476531982\n",
      "epoch: 17, batch: 91, loss: 1.8672937154769897\n",
      "epoch: 17, batch: 92, loss: 1.8705530166625977\n",
      "epoch: 17, batch: 93, loss: 1.269808053970337\n",
      "epoch: 17, batch: 94, loss: 1.397372841835022\n",
      "epoch: 17, batch: 95, loss: 1.3165479898452759\n",
      "epoch: 17, batch: 96, loss: 1.1792019605636597\n",
      "epoch: 17, batch: 97, loss: 1.6548326015472412\n",
      "epoch: 17, batch: 98, loss: 1.1745399236679077\n",
      "epoch: 17, batch: 99, loss: 1.2410708665847778\n",
      "epoch: 17, batch: 100, loss: 1.8017984628677368\n",
      "epoch: 17, batch: 101, loss: 1.3507411479949951\n",
      "epoch: 17, batch: 102, loss: 1.6129287481307983\n",
      "epoch: 17, batch: 103, loss: 1.6136219501495361\n",
      "epoch: 17, batch: 104, loss: 1.0886785984039307\n",
      "epoch: 17, batch: 105, loss: 1.9327472448349\n",
      "epoch: 17, batch: 106, loss: 1.546635627746582\n",
      "epoch: 17, batch: 107, loss: 2.12776517868042\n",
      "epoch: 17, batch: 108, loss: 1.5334763526916504\n",
      "epoch: 17, batch: 109, loss: 1.8348699808120728\n",
      "epoch: 17, batch: 110, loss: 1.367250680923462\n",
      "epoch: 17, batch: 111, loss: 1.457435965538025\n",
      "epoch: 17, batch: 112, loss: 1.6572465896606445\n",
      "epoch: 17, batch: 113, loss: 1.5199134349822998\n",
      "epoch: 17, batch: 114, loss: 1.4762601852416992\n",
      "epoch: 17, batch: 115, loss: 1.3569598197937012\n",
      "epoch: 17, batch: 116, loss: 1.6219621896743774\n",
      "epoch: 17, batch: 117, loss: 1.7367324829101562\n",
      "epoch: 17, batch: 118, loss: 1.4720304012298584\n",
      "epoch: 17, batch: 119, loss: 1.1205087900161743\n",
      "epoch: 17, batch: 120, loss: 1.421642780303955\n",
      "epoch: 17, batch: 121, loss: 1.4860634803771973\n",
      "epoch: 17, batch: 122, loss: 1.4287750720977783\n",
      "epoch: 17, batch: 123, loss: 1.4485275745391846\n",
      "epoch: 17, batch: 124, loss: 1.7455838918685913\n",
      "epoch: 17, batch: 125, loss: 1.267114281654358\n",
      "epoch: 17, batch: 126, loss: 1.736510992050171\n",
      "epoch: 17, batch: 127, loss: 1.1530699729919434\n",
      "epoch: 18, batch: 0, loss: 1.8086200952529907\n",
      "epoch: 18, batch: 1, loss: 1.3691685199737549\n",
      "epoch: 18, batch: 2, loss: 1.6461941003799438\n",
      "epoch: 18, batch: 3, loss: 1.73794686794281\n",
      "epoch: 18, batch: 4, loss: 1.937111496925354\n",
      "epoch: 18, batch: 5, loss: 1.5635966062545776\n",
      "epoch: 18, batch: 6, loss: 1.6993052959442139\n",
      "epoch: 18, batch: 7, loss: 1.8072302341461182\n",
      "epoch: 18, batch: 8, loss: 2.240736722946167\n",
      "epoch: 18, batch: 9, loss: 1.520824909210205\n",
      "epoch: 18, batch: 10, loss: 1.3049296140670776\n",
      "epoch: 18, batch: 11, loss: 1.5684618949890137\n",
      "epoch: 18, batch: 12, loss: 1.3776137828826904\n",
      "epoch: 18, batch: 13, loss: 1.4465659856796265\n",
      "epoch: 18, batch: 14, loss: 1.2266199588775635\n",
      "epoch: 18, batch: 15, loss: 1.4825451374053955\n",
      "epoch: 18, batch: 16, loss: 1.352036952972412\n",
      "epoch: 18, batch: 17, loss: 1.5472851991653442\n",
      "epoch: 18, batch: 18, loss: 1.735655426979065\n",
      "epoch: 18, batch: 19, loss: 1.4254993200302124\n",
      "epoch: 18, batch: 20, loss: 1.7226972579956055\n",
      "epoch: 18, batch: 21, loss: 1.6882622241973877\n",
      "epoch: 18, batch: 22, loss: 1.1852360963821411\n",
      "epoch: 18, batch: 23, loss: 1.454527497291565\n",
      "epoch: 18, batch: 24, loss: 1.6231553554534912\n",
      "epoch: 18, batch: 25, loss: 1.6489359140396118\n",
      "epoch: 18, batch: 26, loss: 1.480621099472046\n",
      "epoch: 18, batch: 27, loss: 1.6998838186264038\n",
      "epoch: 18, batch: 28, loss: 1.41222083568573\n",
      "epoch: 18, batch: 29, loss: 1.4418836832046509\n",
      "epoch: 18, batch: 30, loss: 1.650782823562622\n",
      "epoch: 18, batch: 31, loss: 1.5881320238113403\n",
      "epoch: 18, batch: 32, loss: 1.707759141921997\n",
      "epoch: 18, batch: 33, loss: 1.4471158981323242\n",
      "epoch: 18, batch: 34, loss: 1.5798602104187012\n",
      "epoch: 18, batch: 35, loss: 1.5972894430160522\n",
      "epoch: 18, batch: 36, loss: 1.474657416343689\n",
      "epoch: 18, batch: 37, loss: 1.6897283792495728\n",
      "epoch: 18, batch: 38, loss: 1.7451655864715576\n",
      "epoch: 18, batch: 39, loss: 1.2913737297058105\n",
      "epoch: 18, batch: 40, loss: 1.6079063415527344\n",
      "epoch: 18, batch: 41, loss: 1.585801124572754\n",
      "epoch: 18, batch: 42, loss: 1.5572067499160767\n",
      "epoch: 18, batch: 43, loss: 1.5892345905303955\n",
      "epoch: 18, batch: 44, loss: 1.1967689990997314\n",
      "epoch: 18, batch: 45, loss: 1.386258840560913\n",
      "epoch: 18, batch: 46, loss: 1.2708033323287964\n",
      "epoch: 18, batch: 47, loss: 1.563274621963501\n",
      "epoch: 18, batch: 48, loss: 1.3834896087646484\n",
      "epoch: 18, batch: 49, loss: 1.5972570180892944\n",
      "epoch: 18, batch: 50, loss: 1.5454533100128174\n",
      "epoch: 18, batch: 51, loss: 1.4867786169052124\n",
      "epoch: 18, batch: 52, loss: 1.6587963104248047\n",
      "epoch: 18, batch: 53, loss: 1.2443625926971436\n",
      "epoch: 18, batch: 54, loss: 1.5083558559417725\n",
      "epoch: 18, batch: 55, loss: 1.8618011474609375\n",
      "epoch: 18, batch: 56, loss: 1.4651455879211426\n",
      "epoch: 18, batch: 57, loss: 1.7349332571029663\n",
      "epoch: 18, batch: 58, loss: 1.6576855182647705\n",
      "epoch: 18, batch: 59, loss: 1.7098653316497803\n",
      "epoch: 18, batch: 60, loss: 1.5577389001846313\n",
      "epoch: 18, batch: 61, loss: 1.3971171379089355\n",
      "epoch: 18, batch: 62, loss: 1.5504748821258545\n",
      "epoch: 18, batch: 63, loss: 1.5901556015014648\n",
      "epoch: 18, batch: 64, loss: 1.6686064004898071\n",
      "epoch: 18, batch: 65, loss: 1.7160484790802002\n",
      "epoch: 18, batch: 66, loss: 1.6337429285049438\n",
      "epoch: 18, batch: 67, loss: 1.679351568222046\n",
      "epoch: 18, batch: 68, loss: 1.376604676246643\n",
      "epoch: 18, batch: 69, loss: 1.4205734729766846\n",
      "epoch: 18, batch: 70, loss: 1.5144593715667725\n",
      "epoch: 18, batch: 71, loss: 1.4595075845718384\n",
      "epoch: 18, batch: 72, loss: 1.5957000255584717\n",
      "epoch: 18, batch: 73, loss: 1.6135104894638062\n",
      "epoch: 18, batch: 74, loss: 1.4294984340667725\n",
      "epoch: 18, batch: 75, loss: 1.7941497564315796\n",
      "epoch: 18, batch: 76, loss: 1.5992028713226318\n",
      "epoch: 18, batch: 77, loss: 1.4724886417388916\n",
      "epoch: 18, batch: 78, loss: 1.6406786441802979\n",
      "epoch: 18, batch: 79, loss: 1.6098392009735107\n",
      "epoch: 18, batch: 80, loss: 1.6037343740463257\n",
      "epoch: 18, batch: 81, loss: 1.428335428237915\n",
      "epoch: 18, batch: 82, loss: 1.593568205833435\n",
      "epoch: 18, batch: 83, loss: 1.4481384754180908\n",
      "epoch: 18, batch: 84, loss: 1.6918436288833618\n",
      "epoch: 18, batch: 85, loss: 1.5229593515396118\n",
      "epoch: 18, batch: 86, loss: 1.4675037860870361\n",
      "epoch: 18, batch: 87, loss: 1.1981045007705688\n",
      "epoch: 18, batch: 88, loss: 1.430715799331665\n",
      "epoch: 18, batch: 89, loss: 1.5818660259246826\n",
      "epoch: 18, batch: 90, loss: 1.5007455348968506\n",
      "epoch: 18, batch: 91, loss: 1.7122058868408203\n",
      "epoch: 18, batch: 92, loss: 1.4502882957458496\n",
      "epoch: 18, batch: 93, loss: 1.5360667705535889\n",
      "epoch: 18, batch: 94, loss: 1.4058330059051514\n",
      "epoch: 18, batch: 95, loss: 1.6378247737884521\n",
      "epoch: 18, batch: 96, loss: 1.4456344842910767\n",
      "epoch: 18, batch: 97, loss: 1.5332170724868774\n",
      "epoch: 18, batch: 98, loss: 1.477532982826233\n",
      "epoch: 18, batch: 99, loss: 1.6071364879608154\n",
      "epoch: 18, batch: 100, loss: 1.338033676147461\n",
      "epoch: 18, batch: 101, loss: 1.594666838645935\n",
      "epoch: 18, batch: 102, loss: 1.5123627185821533\n",
      "epoch: 18, batch: 103, loss: 1.489539384841919\n",
      "epoch: 18, batch: 104, loss: 1.4403244256973267\n",
      "epoch: 18, batch: 105, loss: 1.6197398900985718\n",
      "epoch: 18, batch: 106, loss: 1.5937752723693848\n",
      "epoch: 18, batch: 107, loss: 1.4935362339019775\n",
      "epoch: 18, batch: 108, loss: 1.5423765182495117\n",
      "epoch: 18, batch: 109, loss: 1.4458110332489014\n",
      "epoch: 18, batch: 110, loss: 1.4108879566192627\n",
      "epoch: 18, batch: 111, loss: 1.5830705165863037\n",
      "epoch: 18, batch: 112, loss: 1.572762131690979\n",
      "epoch: 18, batch: 113, loss: 1.4980835914611816\n",
      "epoch: 18, batch: 114, loss: 1.360975742340088\n",
      "epoch: 18, batch: 115, loss: 1.678203821182251\n",
      "epoch: 18, batch: 116, loss: 1.4993135929107666\n",
      "epoch: 18, batch: 117, loss: 1.5626928806304932\n",
      "epoch: 18, batch: 118, loss: 1.5409748554229736\n",
      "epoch: 18, batch: 119, loss: 1.3360780477523804\n",
      "epoch: 18, batch: 120, loss: 1.6561285257339478\n",
      "epoch: 18, batch: 121, loss: 1.610356092453003\n",
      "epoch: 18, batch: 122, loss: 1.6041160821914673\n",
      "epoch: 18, batch: 123, loss: 1.4503023624420166\n",
      "epoch: 18, batch: 124, loss: 1.3348103761672974\n",
      "epoch: 18, batch: 125, loss: 1.4014536142349243\n",
      "epoch: 18, batch: 126, loss: 1.5693528652191162\n",
      "epoch: 18, batch: 127, loss: 1.5422804355621338\n",
      "epoch: 19, batch: 0, loss: 1.6177908182144165\n",
      "epoch: 19, batch: 1, loss: 1.755811095237732\n",
      "epoch: 19, batch: 2, loss: 1.6369707584381104\n",
      "epoch: 19, batch: 3, loss: 1.5108448266983032\n",
      "epoch: 19, batch: 4, loss: 1.5360898971557617\n",
      "epoch: 19, batch: 5, loss: 1.3430311679840088\n",
      "epoch: 19, batch: 6, loss: 1.6235065460205078\n",
      "epoch: 19, batch: 7, loss: 1.6516578197479248\n",
      "epoch: 19, batch: 8, loss: 1.3969066143035889\n",
      "epoch: 19, batch: 9, loss: 1.4757622480392456\n",
      "epoch: 19, batch: 10, loss: 1.5497370958328247\n",
      "epoch: 19, batch: 11, loss: 1.476361870765686\n",
      "epoch: 19, batch: 12, loss: 1.614680290222168\n",
      "epoch: 19, batch: 13, loss: 1.7270476818084717\n",
      "epoch: 19, batch: 14, loss: 1.3130826950073242\n",
      "epoch: 19, batch: 15, loss: 1.448543906211853\n",
      "epoch: 19, batch: 16, loss: 1.8137763738632202\n",
      "epoch: 19, batch: 17, loss: 1.7308677434921265\n",
      "epoch: 19, batch: 18, loss: 1.6553030014038086\n",
      "epoch: 19, batch: 19, loss: 1.3743921518325806\n",
      "epoch: 19, batch: 20, loss: 1.5332660675048828\n",
      "epoch: 19, batch: 21, loss: 1.365765929222107\n",
      "epoch: 19, batch: 22, loss: 1.4277528524398804\n",
      "epoch: 19, batch: 23, loss: 1.3948897123336792\n",
      "epoch: 19, batch: 24, loss: 1.6335039138793945\n",
      "epoch: 19, batch: 25, loss: 1.579395055770874\n",
      "epoch: 19, batch: 26, loss: 1.3986408710479736\n",
      "epoch: 19, batch: 27, loss: 1.5779305696487427\n",
      "epoch: 19, batch: 28, loss: 1.6199817657470703\n",
      "epoch: 19, batch: 29, loss: 1.4612452983856201\n",
      "epoch: 19, batch: 30, loss: 1.481401801109314\n",
      "epoch: 19, batch: 31, loss: 1.6510860919952393\n",
      "epoch: 19, batch: 32, loss: 1.5009772777557373\n",
      "epoch: 19, batch: 33, loss: 1.3618128299713135\n",
      "epoch: 19, batch: 34, loss: 1.3299387693405151\n",
      "epoch: 19, batch: 35, loss: 1.2225077152252197\n",
      "epoch: 19, batch: 36, loss: 1.2954251766204834\n",
      "epoch: 19, batch: 37, loss: 1.6726810932159424\n",
      "epoch: 19, batch: 38, loss: 1.5893279314041138\n",
      "epoch: 19, batch: 39, loss: 1.707217812538147\n",
      "epoch: 19, batch: 40, loss: 1.6496692895889282\n",
      "epoch: 19, batch: 41, loss: 1.5186588764190674\n",
      "epoch: 19, batch: 42, loss: 1.2876001596450806\n",
      "epoch: 19, batch: 43, loss: 1.442766785621643\n",
      "epoch: 19, batch: 44, loss: 1.5976247787475586\n",
      "epoch: 19, batch: 45, loss: 1.5765188932418823\n",
      "epoch: 19, batch: 46, loss: 1.8208824396133423\n",
      "epoch: 19, batch: 47, loss: 1.5183597803115845\n",
      "epoch: 19, batch: 48, loss: 1.4522294998168945\n",
      "epoch: 19, batch: 49, loss: 1.3216263055801392\n",
      "epoch: 19, batch: 50, loss: 1.5122148990631104\n",
      "epoch: 19, batch: 51, loss: 1.4865754842758179\n",
      "epoch: 19, batch: 52, loss: 1.587206482887268\n",
      "epoch: 19, batch: 53, loss: 1.8330974578857422\n",
      "epoch: 19, batch: 54, loss: 1.511553168296814\n",
      "epoch: 19, batch: 55, loss: 1.2825815677642822\n",
      "epoch: 19, batch: 56, loss: 1.4372332096099854\n",
      "epoch: 19, batch: 57, loss: 1.2837427854537964\n",
      "epoch: 19, batch: 58, loss: 1.726435899734497\n",
      "epoch: 19, batch: 59, loss: 1.7058225870132446\n",
      "epoch: 19, batch: 60, loss: 1.6299915313720703\n",
      "epoch: 19, batch: 61, loss: 1.183815360069275\n",
      "epoch: 19, batch: 62, loss: 1.5032734870910645\n",
      "epoch: 19, batch: 63, loss: 1.4246336221694946\n",
      "epoch: 19, batch: 64, loss: 1.6298977136611938\n",
      "epoch: 19, batch: 65, loss: 1.43701171875\n",
      "epoch: 19, batch: 66, loss: 1.5742332935333252\n",
      "epoch: 19, batch: 67, loss: 1.7927067279815674\n",
      "epoch: 19, batch: 68, loss: 1.3241902589797974\n",
      "epoch: 19, batch: 69, loss: 1.5989258289337158\n",
      "epoch: 19, batch: 70, loss: 1.507981538772583\n",
      "epoch: 19, batch: 71, loss: 1.5600364208221436\n",
      "epoch: 19, batch: 72, loss: 1.9099738597869873\n",
      "epoch: 19, batch: 73, loss: 1.2828218936920166\n",
      "epoch: 19, batch: 74, loss: 1.751839280128479\n",
      "epoch: 19, batch: 75, loss: 1.5677963495254517\n",
      "epoch: 19, batch: 76, loss: 1.6863183975219727\n",
      "epoch: 19, batch: 77, loss: 1.449663758277893\n",
      "epoch: 19, batch: 78, loss: 1.067946195602417\n",
      "epoch: 19, batch: 79, loss: 1.4787209033966064\n",
      "epoch: 19, batch: 80, loss: 1.2719886302947998\n",
      "epoch: 19, batch: 81, loss: 1.721146583557129\n",
      "epoch: 19, batch: 82, loss: 1.7971994876861572\n",
      "epoch: 19, batch: 83, loss: 1.622143030166626\n",
      "epoch: 19, batch: 84, loss: 1.2907249927520752\n",
      "epoch: 19, batch: 85, loss: 1.2029197216033936\n",
      "epoch: 19, batch: 86, loss: 1.7706935405731201\n",
      "epoch: 19, batch: 87, loss: 1.6155364513397217\n",
      "epoch: 19, batch: 88, loss: 1.5841412544250488\n",
      "epoch: 19, batch: 89, loss: 1.9249298572540283\n",
      "epoch: 19, batch: 90, loss: 1.9604835510253906\n",
      "epoch: 19, batch: 91, loss: 1.7247788906097412\n",
      "epoch: 19, batch: 92, loss: 1.8337494134902954\n",
      "epoch: 19, batch: 93, loss: 1.5733038187026978\n",
      "epoch: 19, batch: 94, loss: 1.4554088115692139\n",
      "epoch: 19, batch: 95, loss: 1.5881000757217407\n",
      "epoch: 19, batch: 96, loss: 1.0052225589752197\n",
      "epoch: 19, batch: 97, loss: 1.6611919403076172\n",
      "epoch: 19, batch: 98, loss: 1.6579835414886475\n",
      "epoch: 19, batch: 99, loss: 1.3192702531814575\n",
      "epoch: 19, batch: 100, loss: 1.7326240539550781\n",
      "epoch: 19, batch: 101, loss: 1.683439016342163\n",
      "epoch: 19, batch: 102, loss: 1.1588718891143799\n",
      "epoch: 19, batch: 103, loss: 1.4543522596359253\n",
      "epoch: 19, batch: 104, loss: 1.4184045791625977\n",
      "epoch: 19, batch: 105, loss: 1.7712314128875732\n",
      "epoch: 19, batch: 106, loss: 1.0150940418243408\n",
      "epoch: 19, batch: 107, loss: 1.5329267978668213\n",
      "epoch: 19, batch: 108, loss: 1.2077767848968506\n",
      "epoch: 19, batch: 109, loss: 1.6582101583480835\n",
      "epoch: 19, batch: 110, loss: 1.411107063293457\n",
      "epoch: 19, batch: 111, loss: 1.2805850505828857\n",
      "epoch: 19, batch: 112, loss: 2.00534725189209\n",
      "epoch: 19, batch: 113, loss: 1.5772489309310913\n",
      "epoch: 19, batch: 114, loss: 1.533415675163269\n",
      "epoch: 19, batch: 115, loss: 1.5061571598052979\n",
      "epoch: 19, batch: 116, loss: 1.2865383625030518\n",
      "epoch: 19, batch: 117, loss: 1.647446870803833\n",
      "epoch: 19, batch: 118, loss: 2.142127752304077\n",
      "epoch: 19, batch: 119, loss: 2.0163042545318604\n",
      "epoch: 19, batch: 120, loss: 1.721909761428833\n",
      "epoch: 19, batch: 121, loss: 1.3670259714126587\n",
      "epoch: 19, batch: 122, loss: 2.025028944015503\n",
      "epoch: 19, batch: 123, loss: 1.6338274478912354\n",
      "epoch: 19, batch: 124, loss: 1.010128378868103\n",
      "epoch: 19, batch: 125, loss: 1.641730546951294\n",
      "epoch: 19, batch: 126, loss: 1.6709988117218018\n",
      "epoch: 19, batch: 127, loss: 1.508790373802185\n",
      "epoch: 20, batch: 0, loss: 1.6329246759414673\n",
      "epoch: 20, batch: 1, loss: 1.8476600646972656\n",
      "epoch: 20, batch: 2, loss: 1.3858445882797241\n",
      "epoch: 20, batch: 3, loss: 1.7570085525512695\n",
      "epoch: 20, batch: 4, loss: 1.6596227884292603\n",
      "epoch: 20, batch: 5, loss: 1.593022108078003\n",
      "epoch: 20, batch: 6, loss: 1.7838327884674072\n",
      "epoch: 20, batch: 7, loss: 1.617510437965393\n",
      "epoch: 20, batch: 8, loss: 1.3204553127288818\n",
      "epoch: 20, batch: 9, loss: 0.913094699382782\n",
      "epoch: 20, batch: 10, loss: 1.439699411392212\n",
      "epoch: 20, batch: 11, loss: 1.9106051921844482\n",
      "epoch: 20, batch: 12, loss: 1.3086457252502441\n",
      "epoch: 20, batch: 13, loss: 1.6533339023590088\n",
      "epoch: 20, batch: 14, loss: 1.5733201503753662\n",
      "epoch: 20, batch: 15, loss: 1.8366343975067139\n",
      "epoch: 20, batch: 16, loss: 1.403913140296936\n",
      "epoch: 20, batch: 17, loss: 1.736532211303711\n",
      "epoch: 20, batch: 18, loss: 1.0687501430511475\n",
      "epoch: 20, batch: 19, loss: 1.389986515045166\n",
      "epoch: 20, batch: 20, loss: 0.9218036532402039\n",
      "epoch: 20, batch: 21, loss: 1.344529151916504\n",
      "epoch: 20, batch: 22, loss: 1.392935037612915\n",
      "epoch: 20, batch: 23, loss: 1.4357049465179443\n",
      "epoch: 20, batch: 24, loss: 1.6320689916610718\n",
      "epoch: 20, batch: 25, loss: 1.8496124744415283\n",
      "epoch: 20, batch: 26, loss: 1.859161615371704\n",
      "epoch: 20, batch: 27, loss: 2.1356256008148193\n",
      "epoch: 20, batch: 28, loss: 1.271450161933899\n",
      "epoch: 20, batch: 29, loss: 1.099848747253418\n",
      "epoch: 20, batch: 30, loss: 1.3657528162002563\n",
      "epoch: 20, batch: 31, loss: 1.5874961614608765\n",
      "epoch: 20, batch: 32, loss: 1.5667301416397095\n",
      "epoch: 20, batch: 33, loss: 1.7315948009490967\n",
      "epoch: 20, batch: 34, loss: 1.3672634363174438\n",
      "epoch: 20, batch: 35, loss: 1.4818862676620483\n",
      "epoch: 20, batch: 36, loss: 1.8591169118881226\n",
      "epoch: 20, batch: 37, loss: 1.4792449474334717\n",
      "epoch: 20, batch: 38, loss: 1.1909294128417969\n",
      "epoch: 20, batch: 39, loss: 1.3792804479599\n",
      "epoch: 20, batch: 40, loss: 1.2862880229949951\n",
      "epoch: 20, batch: 41, loss: 1.8343147039413452\n",
      "epoch: 20, batch: 42, loss: 1.1198750734329224\n",
      "epoch: 20, batch: 43, loss: 1.4937139749526978\n",
      "epoch: 20, batch: 44, loss: 1.8943039178848267\n",
      "epoch: 20, batch: 45, loss: 1.2748510837554932\n",
      "epoch: 20, batch: 46, loss: 1.2378623485565186\n",
      "epoch: 20, batch: 47, loss: 1.6674659252166748\n",
      "epoch: 20, batch: 48, loss: 1.7040351629257202\n",
      "epoch: 20, batch: 49, loss: 1.917778730392456\n",
      "epoch: 20, batch: 50, loss: 2.079974412918091\n",
      "epoch: 20, batch: 51, loss: 1.4995405673980713\n",
      "epoch: 20, batch: 52, loss: 1.582289457321167\n",
      "epoch: 20, batch: 53, loss: 1.855089545249939\n",
      "epoch: 20, batch: 54, loss: 1.3423144817352295\n",
      "epoch: 20, batch: 55, loss: 1.6897646188735962\n",
      "epoch: 20, batch: 56, loss: 1.2813631296157837\n",
      "epoch: 20, batch: 57, loss: 1.2544305324554443\n",
      "epoch: 20, batch: 58, loss: 0.9422537684440613\n",
      "epoch: 20, batch: 59, loss: 1.7184503078460693\n",
      "epoch: 20, batch: 60, loss: 1.556100845336914\n",
      "epoch: 20, batch: 61, loss: 1.8400890827178955\n",
      "epoch: 20, batch: 62, loss: 1.5145930051803589\n",
      "epoch: 20, batch: 63, loss: 1.6994788646697998\n",
      "epoch: 20, batch: 64, loss: 1.690784215927124\n",
      "epoch: 20, batch: 65, loss: 1.4564354419708252\n",
      "epoch: 20, batch: 66, loss: 1.4988130331039429\n",
      "epoch: 20, batch: 67, loss: 1.750301718711853\n",
      "epoch: 20, batch: 68, loss: 1.6791261434555054\n",
      "epoch: 20, batch: 69, loss: 1.2277226448059082\n",
      "epoch: 20, batch: 70, loss: 1.4076566696166992\n",
      "epoch: 20, batch: 71, loss: 1.6107534170150757\n",
      "epoch: 20, batch: 72, loss: 1.2820273637771606\n",
      "epoch: 20, batch: 73, loss: 1.668884038925171\n",
      "epoch: 20, batch: 74, loss: 1.3556005954742432\n",
      "epoch: 20, batch: 75, loss: 1.8163877725601196\n",
      "epoch: 20, batch: 76, loss: 1.519728422164917\n",
      "epoch: 20, batch: 77, loss: 1.8513052463531494\n",
      "epoch: 20, batch: 78, loss: 2.04082989692688\n",
      "epoch: 20, batch: 79, loss: 1.6561390161514282\n",
      "epoch: 20, batch: 80, loss: 1.3982012271881104\n",
      "epoch: 20, batch: 81, loss: 1.3716622591018677\n",
      "epoch: 20, batch: 82, loss: 1.6201807260513306\n",
      "epoch: 20, batch: 83, loss: 1.3215276002883911\n",
      "epoch: 20, batch: 84, loss: 1.8071928024291992\n",
      "epoch: 20, batch: 85, loss: 1.7061868906021118\n",
      "epoch: 20, batch: 86, loss: 1.554560899734497\n",
      "epoch: 20, batch: 87, loss: 1.4907070398330688\n",
      "epoch: 20, batch: 88, loss: 1.3335494995117188\n",
      "epoch: 20, batch: 89, loss: 1.3148446083068848\n",
      "epoch: 20, batch: 90, loss: 1.7436418533325195\n",
      "epoch: 20, batch: 91, loss: 1.5994139909744263\n",
      "epoch: 20, batch: 92, loss: 1.6156349182128906\n",
      "epoch: 20, batch: 93, loss: 1.829870581626892\n",
      "epoch: 20, batch: 94, loss: 1.6327035427093506\n",
      "epoch: 20, batch: 95, loss: 1.8216403722763062\n",
      "epoch: 20, batch: 96, loss: 1.5205214023590088\n",
      "epoch: 20, batch: 97, loss: 1.5180885791778564\n",
      "epoch: 20, batch: 98, loss: 1.4606478214263916\n",
      "epoch: 20, batch: 99, loss: 1.4921648502349854\n",
      "epoch: 20, batch: 100, loss: 1.5356025695800781\n",
      "epoch: 20, batch: 101, loss: 1.5971925258636475\n",
      "epoch: 20, batch: 102, loss: 1.7593101263046265\n",
      "epoch: 20, batch: 103, loss: 1.4503028392791748\n",
      "epoch: 20, batch: 104, loss: 1.7454748153686523\n",
      "epoch: 20, batch: 105, loss: 1.5735843181610107\n",
      "epoch: 20, batch: 106, loss: 1.260443925857544\n",
      "epoch: 20, batch: 107, loss: 1.563323974609375\n",
      "epoch: 20, batch: 108, loss: 1.274438738822937\n",
      "epoch: 20, batch: 109, loss: 1.5666900873184204\n",
      "epoch: 20, batch: 110, loss: 1.484529733657837\n",
      "epoch: 20, batch: 111, loss: 1.3686411380767822\n",
      "epoch: 20, batch: 112, loss: 1.5328125953674316\n",
      "epoch: 20, batch: 113, loss: 1.7057912349700928\n",
      "epoch: 20, batch: 114, loss: 1.679233193397522\n",
      "epoch: 20, batch: 115, loss: 1.650282621383667\n",
      "epoch: 20, batch: 116, loss: 1.4365153312683105\n",
      "epoch: 20, batch: 117, loss: 1.4033385515213013\n",
      "epoch: 20, batch: 118, loss: 1.6516330242156982\n",
      "epoch: 20, batch: 119, loss: 1.5323125123977661\n",
      "epoch: 20, batch: 120, loss: 1.4952313899993896\n",
      "epoch: 20, batch: 121, loss: 1.2321144342422485\n",
      "epoch: 20, batch: 122, loss: 1.5964043140411377\n",
      "epoch: 20, batch: 123, loss: 1.4785869121551514\n",
      "epoch: 20, batch: 124, loss: 1.4312889575958252\n",
      "epoch: 20, batch: 125, loss: 1.6677356958389282\n",
      "epoch: 20, batch: 126, loss: 1.1826575994491577\n",
      "epoch: 20, batch: 127, loss: 1.462775468826294\n",
      "epoch: 21, batch: 0, loss: 1.368561863899231\n",
      "epoch: 21, batch: 1, loss: 1.8357070684432983\n",
      "epoch: 21, batch: 2, loss: 1.403257966041565\n",
      "epoch: 21, batch: 3, loss: 1.6969209909439087\n",
      "epoch: 21, batch: 4, loss: 1.5350291728973389\n",
      "epoch: 21, batch: 5, loss: 1.2883861064910889\n",
      "epoch: 21, batch: 6, loss: 1.4518604278564453\n",
      "epoch: 21, batch: 7, loss: 1.5752477645874023\n",
      "epoch: 21, batch: 8, loss: 1.5953683853149414\n",
      "epoch: 21, batch: 9, loss: 1.4958438873291016\n",
      "epoch: 21, batch: 10, loss: 1.5440524816513062\n",
      "epoch: 21, batch: 11, loss: 1.5263638496398926\n",
      "epoch: 21, batch: 12, loss: 1.414759874343872\n",
      "epoch: 21, batch: 13, loss: 1.4576504230499268\n",
      "epoch: 21, batch: 14, loss: 1.4197618961334229\n",
      "epoch: 21, batch: 15, loss: 1.7250696420669556\n",
      "epoch: 21, batch: 16, loss: 1.4539835453033447\n",
      "epoch: 21, batch: 17, loss: 1.5330684185028076\n",
      "epoch: 21, batch: 18, loss: 1.480586051940918\n",
      "epoch: 21, batch: 19, loss: 1.5023521184921265\n",
      "epoch: 21, batch: 20, loss: 1.5407593250274658\n",
      "epoch: 21, batch: 21, loss: 1.4524955749511719\n",
      "epoch: 21, batch: 22, loss: 1.6361072063446045\n",
      "epoch: 21, batch: 23, loss: 1.7102473974227905\n",
      "epoch: 21, batch: 24, loss: 1.4970930814743042\n",
      "epoch: 21, batch: 25, loss: 1.5314228534698486\n",
      "epoch: 21, batch: 26, loss: 1.4997220039367676\n",
      "epoch: 21, batch: 27, loss: 1.5849401950836182\n",
      "epoch: 21, batch: 28, loss: 1.671322226524353\n",
      "epoch: 21, batch: 29, loss: 1.6082655191421509\n",
      "epoch: 21, batch: 30, loss: 1.5171172618865967\n",
      "epoch: 21, batch: 31, loss: 1.4356303215026855\n",
      "epoch: 21, batch: 32, loss: 1.7115834951400757\n",
      "epoch: 21, batch: 33, loss: 1.5065945386886597\n",
      "epoch: 21, batch: 34, loss: 1.578224778175354\n",
      "epoch: 21, batch: 35, loss: 1.3253650665283203\n",
      "epoch: 21, batch: 36, loss: 1.5786528587341309\n",
      "epoch: 21, batch: 37, loss: 1.708743691444397\n",
      "epoch: 21, batch: 38, loss: 1.5735505819320679\n",
      "epoch: 21, batch: 39, loss: 1.5301549434661865\n",
      "epoch: 21, batch: 40, loss: 1.6275074481964111\n",
      "epoch: 21, batch: 41, loss: 1.737778663635254\n",
      "epoch: 21, batch: 42, loss: 1.7389061450958252\n",
      "epoch: 21, batch: 43, loss: 1.5336990356445312\n",
      "epoch: 21, batch: 44, loss: 1.7378641366958618\n",
      "epoch: 21, batch: 45, loss: 1.6464684009552002\n",
      "epoch: 21, batch: 46, loss: 1.6061906814575195\n",
      "epoch: 21, batch: 47, loss: 1.4427645206451416\n",
      "epoch: 21, batch: 48, loss: 1.5433404445648193\n",
      "epoch: 21, batch: 49, loss: 1.679223656654358\n",
      "epoch: 21, batch: 50, loss: 1.5585354566574097\n",
      "epoch: 21, batch: 51, loss: 1.4797900915145874\n",
      "epoch: 21, batch: 52, loss: 1.5389782190322876\n",
      "epoch: 21, batch: 53, loss: 1.5229965448379517\n",
      "epoch: 21, batch: 54, loss: 1.4948253631591797\n",
      "epoch: 21, batch: 55, loss: 1.5295273065567017\n",
      "epoch: 21, batch: 56, loss: 1.4812159538269043\n",
      "epoch: 21, batch: 57, loss: 1.6562449932098389\n",
      "epoch: 21, batch: 58, loss: 1.7153456211090088\n",
      "epoch: 21, batch: 59, loss: 1.5795466899871826\n",
      "epoch: 21, batch: 60, loss: 1.6780834197998047\n",
      "epoch: 21, batch: 61, loss: 1.5742251873016357\n",
      "epoch: 21, batch: 62, loss: 1.4400917291641235\n",
      "epoch: 21, batch: 63, loss: 1.5426952838897705\n",
      "epoch: 21, batch: 64, loss: 1.435021162033081\n",
      "epoch: 21, batch: 65, loss: 1.4875580072402954\n",
      "epoch: 21, batch: 66, loss: 1.5606120824813843\n",
      "epoch: 21, batch: 67, loss: 1.3926329612731934\n",
      "epoch: 21, batch: 68, loss: 1.4665746688842773\n",
      "epoch: 21, batch: 69, loss: 1.5147688388824463\n",
      "epoch: 21, batch: 70, loss: 1.6647666692733765\n",
      "epoch: 21, batch: 71, loss: 1.3968982696533203\n",
      "epoch: 21, batch: 72, loss: 1.6192010641098022\n",
      "epoch: 21, batch: 73, loss: 1.6678705215454102\n",
      "epoch: 21, batch: 74, loss: 1.5617948770523071\n",
      "epoch: 21, batch: 75, loss: 1.5399597883224487\n",
      "epoch: 21, batch: 76, loss: 1.5369713306427002\n",
      "epoch: 21, batch: 77, loss: 1.629962682723999\n",
      "epoch: 21, batch: 78, loss: 1.4779382944107056\n",
      "epoch: 21, batch: 79, loss: 1.5589721202850342\n",
      "epoch: 21, batch: 80, loss: 1.5851653814315796\n",
      "epoch: 21, batch: 81, loss: 1.6033985614776611\n",
      "epoch: 21, batch: 82, loss: 1.5453588962554932\n",
      "epoch: 21, batch: 83, loss: 1.5213721990585327\n",
      "epoch: 21, batch: 84, loss: 1.4703342914581299\n",
      "epoch: 21, batch: 85, loss: 1.48329758644104\n",
      "epoch: 21, batch: 86, loss: 1.4920637607574463\n",
      "epoch: 21, batch: 87, loss: 1.6196527481079102\n",
      "epoch: 21, batch: 88, loss: 1.5929033756256104\n",
      "epoch: 21, batch: 89, loss: 1.6010850667953491\n",
      "epoch: 21, batch: 90, loss: 1.637239694595337\n",
      "epoch: 21, batch: 91, loss: 1.6937711238861084\n",
      "epoch: 21, batch: 92, loss: 1.6785427331924438\n",
      "epoch: 21, batch: 93, loss: 1.5142585039138794\n",
      "epoch: 21, batch: 94, loss: 1.4985650777816772\n",
      "epoch: 21, batch: 95, loss: 1.392946481704712\n",
      "epoch: 21, batch: 96, loss: 1.547879934310913\n",
      "epoch: 21, batch: 97, loss: 1.6001955270767212\n",
      "epoch: 21, batch: 98, loss: 1.421929121017456\n",
      "epoch: 21, batch: 99, loss: 1.6666122674942017\n",
      "epoch: 21, batch: 100, loss: 1.6798555850982666\n",
      "epoch: 21, batch: 101, loss: 1.479923129081726\n",
      "epoch: 21, batch: 102, loss: 1.4501383304595947\n",
      "epoch: 21, batch: 103, loss: 1.5525529384613037\n",
      "epoch: 21, batch: 104, loss: 1.4607166051864624\n",
      "epoch: 21, batch: 105, loss: 1.522826910018921\n",
      "epoch: 21, batch: 106, loss: 1.5388209819793701\n",
      "epoch: 21, batch: 107, loss: 1.4898560047149658\n",
      "epoch: 21, batch: 108, loss: 1.540725588798523\n",
      "epoch: 21, batch: 109, loss: 1.5146514177322388\n",
      "epoch: 21, batch: 110, loss: 1.558733344078064\n",
      "epoch: 21, batch: 111, loss: 1.4773643016815186\n",
      "epoch: 21, batch: 112, loss: 1.6097307205200195\n",
      "epoch: 21, batch: 113, loss: 1.5347726345062256\n",
      "epoch: 21, batch: 114, loss: 1.5344970226287842\n",
      "epoch: 21, batch: 115, loss: 1.4819798469543457\n",
      "epoch: 21, batch: 116, loss: 1.5311157703399658\n",
      "epoch: 21, batch: 117, loss: 1.5683479309082031\n",
      "epoch: 21, batch: 118, loss: 1.5478923320770264\n",
      "epoch: 21, batch: 119, loss: 1.5554898977279663\n",
      "epoch: 21, batch: 120, loss: 1.4929548501968384\n",
      "epoch: 21, batch: 121, loss: 1.5746045112609863\n",
      "epoch: 21, batch: 122, loss: 1.536851167678833\n",
      "epoch: 21, batch: 123, loss: 1.6877720355987549\n",
      "epoch: 21, batch: 124, loss: 1.6010593175888062\n",
      "epoch: 21, batch: 125, loss: 1.4884437322616577\n",
      "epoch: 21, batch: 126, loss: 1.5430127382278442\n",
      "epoch: 21, batch: 127, loss: 1.6112961769104004\n",
      "epoch: 22, batch: 0, loss: 1.6485583782196045\n",
      "epoch: 22, batch: 1, loss: 1.6372143030166626\n",
      "epoch: 22, batch: 2, loss: 1.5720441341400146\n",
      "epoch: 22, batch: 3, loss: 1.570540189743042\n",
      "epoch: 22, batch: 4, loss: 1.6840379238128662\n",
      "epoch: 22, batch: 5, loss: 1.6328099966049194\n",
      "epoch: 22, batch: 6, loss: 1.324055790901184\n",
      "epoch: 22, batch: 7, loss: 1.622179388999939\n",
      "epoch: 22, batch: 8, loss: 1.5313336849212646\n",
      "epoch: 22, batch: 9, loss: 1.6035629510879517\n",
      "epoch: 22, batch: 10, loss: 1.461207389831543\n",
      "epoch: 22, batch: 11, loss: 1.503945231437683\n",
      "epoch: 22, batch: 12, loss: 1.5309780836105347\n",
      "epoch: 22, batch: 13, loss: 1.4032528400421143\n",
      "epoch: 22, batch: 14, loss: 1.5537450313568115\n",
      "epoch: 22, batch: 15, loss: 1.5678027868270874\n",
      "epoch: 22, batch: 16, loss: 1.5541547536849976\n",
      "epoch: 22, batch: 17, loss: 1.461759090423584\n",
      "epoch: 22, batch: 18, loss: 1.5342410802841187\n",
      "epoch: 22, batch: 19, loss: 1.6283471584320068\n",
      "epoch: 22, batch: 20, loss: 1.6048475503921509\n",
      "epoch: 22, batch: 21, loss: 1.6425548791885376\n",
      "epoch: 22, batch: 22, loss: 1.6676952838897705\n",
      "epoch: 22, batch: 23, loss: 1.4426946640014648\n",
      "epoch: 22, batch: 24, loss: 1.6415249109268188\n",
      "epoch: 22, batch: 25, loss: 1.7290252447128296\n",
      "epoch: 22, batch: 26, loss: 1.4836907386779785\n",
      "epoch: 22, batch: 27, loss: 1.5766494274139404\n",
      "epoch: 22, batch: 28, loss: 1.703839898109436\n",
      "epoch: 22, batch: 29, loss: 1.5881602764129639\n",
      "epoch: 22, batch: 30, loss: 1.6421611309051514\n",
      "epoch: 22, batch: 31, loss: 1.364471197128296\n",
      "epoch: 22, batch: 32, loss: 1.6361640691757202\n",
      "epoch: 22, batch: 33, loss: 1.5898996591567993\n",
      "epoch: 22, batch: 34, loss: 1.488616943359375\n",
      "epoch: 22, batch: 35, loss: 1.6097183227539062\n",
      "epoch: 22, batch: 36, loss: 1.4417459964752197\n",
      "epoch: 22, batch: 37, loss: 1.3422261476516724\n",
      "epoch: 22, batch: 38, loss: 1.6867249011993408\n",
      "epoch: 22, batch: 39, loss: 1.5365279912948608\n",
      "epoch: 22, batch: 40, loss: 1.6038614511489868\n",
      "epoch: 22, batch: 41, loss: 1.6763614416122437\n",
      "epoch: 22, batch: 42, loss: 1.5795369148254395\n",
      "epoch: 22, batch: 43, loss: 1.4815683364868164\n",
      "epoch: 22, batch: 44, loss: 1.4368083477020264\n",
      "epoch: 22, batch: 45, loss: 1.4603630304336548\n",
      "epoch: 22, batch: 46, loss: 1.552894949913025\n",
      "epoch: 22, batch: 47, loss: 1.541653037071228\n",
      "epoch: 22, batch: 48, loss: 1.763498067855835\n",
      "epoch: 22, batch: 49, loss: 1.4436415433883667\n",
      "epoch: 22, batch: 50, loss: 1.5812017917633057\n",
      "epoch: 22, batch: 51, loss: 1.569577693939209\n",
      "epoch: 22, batch: 52, loss: 1.3691060543060303\n",
      "epoch: 22, batch: 53, loss: 1.7333755493164062\n",
      "epoch: 22, batch: 54, loss: 1.567837119102478\n",
      "epoch: 22, batch: 55, loss: 1.4939510822296143\n",
      "epoch: 22, batch: 56, loss: 1.5562107563018799\n",
      "epoch: 22, batch: 57, loss: 1.3783336877822876\n",
      "epoch: 22, batch: 58, loss: 1.4191235303878784\n",
      "epoch: 22, batch: 59, loss: 1.7035763263702393\n",
      "epoch: 22, batch: 60, loss: 1.6005500555038452\n",
      "epoch: 22, batch: 61, loss: 1.7032684087753296\n",
      "epoch: 22, batch: 62, loss: 1.5311487913131714\n",
      "epoch: 22, batch: 63, loss: 1.6067924499511719\n",
      "epoch: 22, batch: 64, loss: 1.4926426410675049\n",
      "epoch: 22, batch: 65, loss: 1.5048125982284546\n",
      "epoch: 22, batch: 66, loss: 1.7443832159042358\n",
      "epoch: 22, batch: 67, loss: 1.7728627920150757\n",
      "epoch: 22, batch: 68, loss: 1.7483705282211304\n",
      "epoch: 22, batch: 69, loss: 1.4975405931472778\n",
      "epoch: 22, batch: 70, loss: 1.716906189918518\n",
      "epoch: 22, batch: 71, loss: 1.3354113101959229\n",
      "epoch: 22, batch: 72, loss: 1.5364727973937988\n",
      "epoch: 22, batch: 73, loss: 1.6877515316009521\n",
      "epoch: 22, batch: 74, loss: 1.477733850479126\n",
      "epoch: 22, batch: 75, loss: 1.4496982097625732\n",
      "epoch: 22, batch: 76, loss: 1.5556169748306274\n",
      "epoch: 22, batch: 77, loss: 1.4159709215164185\n",
      "epoch: 22, batch: 78, loss: 1.6341050863265991\n",
      "epoch: 22, batch: 79, loss: 1.6721655130386353\n",
      "epoch: 22, batch: 80, loss: 1.4097378253936768\n",
      "epoch: 22, batch: 81, loss: 1.6034570932388306\n",
      "epoch: 22, batch: 82, loss: 1.4465248584747314\n",
      "epoch: 22, batch: 83, loss: 1.5365779399871826\n",
      "epoch: 22, batch: 84, loss: 1.4692158699035645\n",
      "epoch: 22, batch: 85, loss: 1.2927677631378174\n",
      "epoch: 22, batch: 86, loss: 1.4753128290176392\n",
      "epoch: 22, batch: 87, loss: 1.489458441734314\n",
      "epoch: 22, batch: 88, loss: 1.5835716724395752\n",
      "epoch: 22, batch: 89, loss: 1.7142826318740845\n",
      "epoch: 22, batch: 90, loss: 1.2874025106430054\n",
      "epoch: 22, batch: 91, loss: 1.7092173099517822\n",
      "epoch: 22, batch: 92, loss: 1.6683578491210938\n",
      "epoch: 22, batch: 93, loss: 1.4839355945587158\n",
      "epoch: 22, batch: 94, loss: 1.3542791604995728\n",
      "epoch: 22, batch: 95, loss: 1.4584039449691772\n",
      "epoch: 22, batch: 96, loss: 1.438342571258545\n",
      "epoch: 22, batch: 97, loss: 1.4862675666809082\n",
      "epoch: 22, batch: 98, loss: 1.5193898677825928\n",
      "epoch: 22, batch: 99, loss: 1.38345468044281\n",
      "epoch: 22, batch: 100, loss: 1.5222152471542358\n",
      "epoch: 22, batch: 101, loss: 1.4635120630264282\n",
      "epoch: 22, batch: 102, loss: 1.422324299812317\n",
      "epoch: 22, batch: 103, loss: 1.3342241048812866\n",
      "epoch: 22, batch: 104, loss: 1.4002416133880615\n",
      "epoch: 22, batch: 105, loss: 1.6031866073608398\n",
      "epoch: 22, batch: 106, loss: 1.6966289281845093\n",
      "epoch: 22, batch: 107, loss: 1.4341981410980225\n",
      "epoch: 22, batch: 108, loss: 1.5524972677230835\n",
      "epoch: 22, batch: 109, loss: 1.5518052577972412\n",
      "epoch: 22, batch: 110, loss: 1.4312646389007568\n",
      "epoch: 22, batch: 111, loss: 1.6197818517684937\n",
      "epoch: 22, batch: 112, loss: 1.5702502727508545\n",
      "epoch: 22, batch: 113, loss: 1.3719642162322998\n",
      "epoch: 22, batch: 114, loss: 1.583507776260376\n",
      "epoch: 22, batch: 115, loss: 1.4969549179077148\n",
      "epoch: 22, batch: 116, loss: 1.5756572484970093\n",
      "epoch: 22, batch: 117, loss: 1.4921983480453491\n",
      "epoch: 22, batch: 118, loss: 1.5357521772384644\n",
      "epoch: 22, batch: 119, loss: 1.452189326286316\n",
      "epoch: 22, batch: 120, loss: 1.4990907907485962\n",
      "epoch: 22, batch: 121, loss: 1.5616153478622437\n",
      "epoch: 22, batch: 122, loss: 1.6692588329315186\n",
      "epoch: 22, batch: 123, loss: 1.498674750328064\n",
      "epoch: 22, batch: 124, loss: 1.4473867416381836\n",
      "epoch: 22, batch: 125, loss: 1.563421607017517\n",
      "epoch: 22, batch: 126, loss: 1.5825634002685547\n",
      "epoch: 22, batch: 127, loss: 1.551348328590393\n",
      "epoch: 23, batch: 0, loss: 1.5777430534362793\n",
      "epoch: 23, batch: 1, loss: 1.7528941631317139\n",
      "epoch: 23, batch: 2, loss: 1.5927631855010986\n",
      "epoch: 23, batch: 3, loss: 1.7296688556671143\n",
      "epoch: 23, batch: 4, loss: 1.4863431453704834\n",
      "epoch: 23, batch: 5, loss: 1.3285826444625854\n",
      "epoch: 23, batch: 6, loss: 1.443668007850647\n",
      "epoch: 23, batch: 7, loss: 1.671250581741333\n",
      "epoch: 23, batch: 8, loss: 1.3508331775665283\n",
      "epoch: 23, batch: 9, loss: 1.5275214910507202\n",
      "epoch: 23, batch: 10, loss: 1.6514564752578735\n",
      "epoch: 23, batch: 11, loss: 1.3731749057769775\n",
      "epoch: 23, batch: 12, loss: 1.6639516353607178\n",
      "epoch: 23, batch: 13, loss: 1.5969961881637573\n",
      "epoch: 23, batch: 14, loss: 1.4643774032592773\n",
      "epoch: 23, batch: 15, loss: 1.3936383724212646\n",
      "epoch: 23, batch: 16, loss: 1.269502878189087\n",
      "epoch: 23, batch: 17, loss: 1.8229639530181885\n",
      "epoch: 23, batch: 18, loss: 1.2857611179351807\n",
      "epoch: 23, batch: 19, loss: 1.3549808263778687\n",
      "epoch: 23, batch: 20, loss: 1.2914386987686157\n",
      "epoch: 23, batch: 21, loss: 1.4677613973617554\n",
      "epoch: 23, batch: 22, loss: 1.3939275741577148\n",
      "epoch: 23, batch: 23, loss: 1.5543183088302612\n",
      "epoch: 23, batch: 24, loss: 1.3690077066421509\n",
      "epoch: 23, batch: 25, loss: 1.3922523260116577\n",
      "epoch: 23, batch: 26, loss: 1.5300489664077759\n",
      "epoch: 23, batch: 27, loss: 1.4945026636123657\n",
      "epoch: 23, batch: 28, loss: 1.1849191188812256\n",
      "epoch: 23, batch: 29, loss: 1.6825182437896729\n",
      "epoch: 23, batch: 30, loss: 1.5440047979354858\n",
      "epoch: 23, batch: 31, loss: 1.5948492288589478\n",
      "epoch: 23, batch: 32, loss: 1.434171199798584\n",
      "epoch: 23, batch: 33, loss: 1.7470076084136963\n",
      "epoch: 23, batch: 34, loss: 1.600959062576294\n",
      "epoch: 23, batch: 35, loss: 1.4992042779922485\n",
      "epoch: 23, batch: 36, loss: 1.9716997146606445\n",
      "epoch: 23, batch: 37, loss: 1.1436479091644287\n",
      "epoch: 23, batch: 38, loss: 1.286839485168457\n",
      "epoch: 23, batch: 39, loss: 1.5261355638504028\n",
      "epoch: 23, batch: 40, loss: 1.5584584474563599\n",
      "epoch: 23, batch: 41, loss: 1.5314887762069702\n",
      "epoch: 23, batch: 42, loss: 1.781935453414917\n",
      "epoch: 23, batch: 43, loss: 1.3177273273468018\n",
      "epoch: 23, batch: 44, loss: 1.5940500497817993\n",
      "epoch: 23, batch: 45, loss: 1.1861900091171265\n",
      "epoch: 23, batch: 46, loss: 1.3842692375183105\n",
      "epoch: 23, batch: 47, loss: 1.431532621383667\n",
      "epoch: 23, batch: 48, loss: 1.5163915157318115\n",
      "epoch: 23, batch: 49, loss: 1.5263874530792236\n",
      "epoch: 23, batch: 50, loss: 1.4777017831802368\n",
      "epoch: 23, batch: 51, loss: 1.424973726272583\n",
      "epoch: 23, batch: 52, loss: 1.4889508485794067\n",
      "epoch: 23, batch: 53, loss: 1.3526362180709839\n",
      "epoch: 23, batch: 54, loss: 1.7137502431869507\n",
      "epoch: 23, batch: 55, loss: 1.637224555015564\n",
      "epoch: 23, batch: 56, loss: 1.5006862878799438\n",
      "epoch: 23, batch: 57, loss: 1.3626943826675415\n",
      "epoch: 23, batch: 58, loss: 1.162644624710083\n",
      "epoch: 23, batch: 59, loss: 1.582279086112976\n",
      "epoch: 23, batch: 60, loss: 1.5733239650726318\n",
      "epoch: 23, batch: 61, loss: 1.5572469234466553\n",
      "epoch: 23, batch: 62, loss: 1.6063811779022217\n",
      "epoch: 23, batch: 63, loss: 1.8129394054412842\n",
      "epoch: 23, batch: 64, loss: 1.50307035446167\n",
      "epoch: 23, batch: 65, loss: 1.4968160390853882\n",
      "epoch: 23, batch: 66, loss: 1.674246072769165\n",
      "epoch: 23, batch: 67, loss: 1.3274991512298584\n",
      "epoch: 23, batch: 68, loss: 1.6181678771972656\n",
      "epoch: 23, batch: 69, loss: 1.7663812637329102\n",
      "epoch: 23, batch: 70, loss: 1.6199274063110352\n",
      "epoch: 23, batch: 71, loss: 1.4825513362884521\n",
      "epoch: 23, batch: 72, loss: 1.607749581336975\n",
      "epoch: 23, batch: 73, loss: 1.3153148889541626\n",
      "epoch: 23, batch: 74, loss: 1.7463566064834595\n",
      "epoch: 23, batch: 75, loss: 1.4463492631912231\n",
      "epoch: 23, batch: 76, loss: 1.4339540004730225\n",
      "epoch: 23, batch: 77, loss: 1.7181907892227173\n",
      "epoch: 23, batch: 78, loss: 1.9279636144638062\n",
      "epoch: 23, batch: 79, loss: 1.5310715436935425\n",
      "epoch: 23, batch: 80, loss: 1.4011528491973877\n",
      "epoch: 23, batch: 81, loss: 1.136047601699829\n",
      "epoch: 23, batch: 82, loss: 1.3258776664733887\n",
      "epoch: 23, batch: 83, loss: 1.4962809085845947\n",
      "epoch: 23, batch: 84, loss: 1.5786714553833008\n",
      "epoch: 23, batch: 85, loss: 1.577547311782837\n",
      "epoch: 23, batch: 86, loss: 1.5893237590789795\n",
      "epoch: 23, batch: 87, loss: 1.439668893814087\n",
      "epoch: 23, batch: 88, loss: 1.5098178386688232\n",
      "epoch: 23, batch: 89, loss: 1.4992865324020386\n",
      "epoch: 23, batch: 90, loss: 1.8316738605499268\n",
      "epoch: 23, batch: 91, loss: 1.4065996408462524\n",
      "epoch: 23, batch: 92, loss: 1.59626305103302\n",
      "epoch: 23, batch: 93, loss: 1.6955547332763672\n",
      "epoch: 23, batch: 94, loss: 1.6421177387237549\n",
      "epoch: 23, batch: 95, loss: 1.5777822732925415\n",
      "epoch: 23, batch: 96, loss: 1.5007812976837158\n",
      "epoch: 23, batch: 97, loss: 1.445121169090271\n",
      "epoch: 23, batch: 98, loss: 1.4948463439941406\n",
      "epoch: 23, batch: 99, loss: 1.5442922115325928\n",
      "epoch: 23, batch: 100, loss: 1.8017936944961548\n",
      "epoch: 23, batch: 101, loss: 1.6938775777816772\n",
      "epoch: 23, batch: 102, loss: 1.467499017715454\n",
      "epoch: 23, batch: 103, loss: 1.9750187397003174\n",
      "epoch: 23, batch: 104, loss: 1.728074312210083\n",
      "epoch: 23, batch: 105, loss: 1.4310195446014404\n",
      "epoch: 23, batch: 106, loss: 1.3750994205474854\n",
      "epoch: 23, batch: 107, loss: 1.648453950881958\n",
      "epoch: 23, batch: 108, loss: 1.7715175151824951\n",
      "epoch: 23, batch: 109, loss: 2.0438852310180664\n",
      "epoch: 23, batch: 110, loss: 1.5791430473327637\n",
      "epoch: 23, batch: 111, loss: 1.995832085609436\n",
      "epoch: 23, batch: 112, loss: 1.3709571361541748\n",
      "epoch: 23, batch: 113, loss: 1.622396469116211\n",
      "epoch: 23, batch: 114, loss: 2.0327682495117188\n",
      "epoch: 23, batch: 115, loss: 2.0591697692871094\n",
      "epoch: 23, batch: 116, loss: 1.807591438293457\n",
      "epoch: 23, batch: 117, loss: 1.3134934902191162\n",
      "epoch: 23, batch: 118, loss: 1.2647541761398315\n",
      "epoch: 23, batch: 119, loss: 1.3870278596878052\n",
      "epoch: 23, batch: 120, loss: 1.4313191175460815\n",
      "epoch: 23, batch: 121, loss: 1.5431187152862549\n",
      "epoch: 23, batch: 122, loss: 1.5699074268341064\n",
      "epoch: 23, batch: 123, loss: 1.660740852355957\n",
      "epoch: 23, batch: 124, loss: 1.4564229249954224\n",
      "epoch: 23, batch: 125, loss: 1.3296706676483154\n",
      "epoch: 23, batch: 126, loss: 1.3238716125488281\n",
      "epoch: 23, batch: 127, loss: 1.3220449686050415\n",
      "epoch: 24, batch: 0, loss: 1.6387386322021484\n",
      "epoch: 24, batch: 1, loss: 1.5667226314544678\n",
      "epoch: 24, batch: 2, loss: 1.9805405139923096\n",
      "epoch: 24, batch: 3, loss: 1.7903600931167603\n",
      "epoch: 24, batch: 4, loss: 1.765389084815979\n",
      "epoch: 24, batch: 5, loss: 1.499866247177124\n",
      "epoch: 24, batch: 6, loss: 1.5597987174987793\n",
      "epoch: 24, batch: 7, loss: 1.489440679550171\n",
      "epoch: 24, batch: 8, loss: 1.3958404064178467\n",
      "epoch: 24, batch: 9, loss: 1.497300148010254\n",
      "epoch: 24, batch: 10, loss: 1.3267744779586792\n",
      "epoch: 24, batch: 11, loss: 1.2909907102584839\n",
      "epoch: 24, batch: 12, loss: 1.556638479232788\n",
      "epoch: 24, batch: 13, loss: 1.2625923156738281\n",
      "epoch: 24, batch: 14, loss: 1.1857004165649414\n",
      "epoch: 24, batch: 15, loss: 1.5265007019042969\n",
      "epoch: 24, batch: 16, loss: 1.725516676902771\n",
      "epoch: 24, batch: 17, loss: 2.0304057598114014\n",
      "epoch: 24, batch: 18, loss: 1.519403338432312\n",
      "epoch: 24, batch: 19, loss: 1.6730659008026123\n",
      "epoch: 24, batch: 20, loss: 1.3318175077438354\n",
      "epoch: 24, batch: 21, loss: 1.4940115213394165\n",
      "epoch: 24, batch: 22, loss: 1.8096565008163452\n",
      "epoch: 24, batch: 23, loss: 1.5967974662780762\n",
      "epoch: 24, batch: 24, loss: 1.9436569213867188\n",
      "epoch: 24, batch: 25, loss: 1.448017954826355\n",
      "epoch: 24, batch: 26, loss: 1.3253806829452515\n",
      "epoch: 24, batch: 27, loss: 1.7169783115386963\n",
      "epoch: 24, batch: 28, loss: 1.3473269939422607\n",
      "epoch: 24, batch: 29, loss: 1.508975625038147\n",
      "epoch: 24, batch: 30, loss: 1.8458197116851807\n",
      "epoch: 24, batch: 31, loss: 1.4312293529510498\n",
      "epoch: 24, batch: 32, loss: 1.217298984527588\n",
      "epoch: 24, batch: 33, loss: 1.5141098499298096\n",
      "epoch: 24, batch: 34, loss: 1.2843663692474365\n",
      "epoch: 24, batch: 35, loss: 1.2920631170272827\n",
      "epoch: 24, batch: 36, loss: 1.4782469272613525\n",
      "epoch: 24, batch: 37, loss: 1.2780826091766357\n",
      "epoch: 24, batch: 38, loss: 1.3782285451889038\n",
      "epoch: 24, batch: 39, loss: 1.1286617517471313\n",
      "epoch: 24, batch: 40, loss: 1.5519193410873413\n",
      "epoch: 24, batch: 41, loss: 1.5792771577835083\n",
      "epoch: 24, batch: 42, loss: 1.2163301706314087\n",
      "epoch: 24, batch: 43, loss: 1.637847661972046\n",
      "epoch: 24, batch: 44, loss: 2.025345802307129\n",
      "epoch: 24, batch: 45, loss: 1.769484519958496\n",
      "epoch: 24, batch: 46, loss: 1.8142486810684204\n",
      "epoch: 24, batch: 47, loss: 1.4558645486831665\n",
      "epoch: 24, batch: 48, loss: 1.8638263940811157\n",
      "epoch: 24, batch: 49, loss: 1.2247997522354126\n",
      "epoch: 24, batch: 50, loss: 1.7748115062713623\n",
      "epoch: 24, batch: 51, loss: 1.4502767324447632\n",
      "epoch: 24, batch: 52, loss: 1.8057796955108643\n",
      "epoch: 24, batch: 53, loss: 2.097738027572632\n",
      "epoch: 24, batch: 54, loss: 1.602770209312439\n",
      "epoch: 24, batch: 55, loss: 1.6283063888549805\n",
      "epoch: 24, batch: 56, loss: 1.40800940990448\n",
      "epoch: 24, batch: 57, loss: 1.2631824016571045\n",
      "epoch: 24, batch: 58, loss: 1.6834157705307007\n",
      "epoch: 24, batch: 59, loss: 1.7170101404190063\n",
      "epoch: 24, batch: 60, loss: 1.7783464193344116\n",
      "epoch: 24, batch: 61, loss: 1.6948802471160889\n",
      "epoch: 24, batch: 62, loss: 1.4167890548706055\n",
      "epoch: 24, batch: 63, loss: 1.6487138271331787\n",
      "epoch: 24, batch: 64, loss: 1.6074111461639404\n",
      "epoch: 24, batch: 65, loss: 1.5437933206558228\n",
      "epoch: 24, batch: 66, loss: 1.415684461593628\n",
      "epoch: 24, batch: 67, loss: 1.5345691442489624\n",
      "epoch: 24, batch: 68, loss: 1.4091174602508545\n",
      "epoch: 24, batch: 69, loss: 1.5567030906677246\n",
      "epoch: 24, batch: 70, loss: 1.4811124801635742\n",
      "epoch: 24, batch: 71, loss: 1.6115894317626953\n",
      "epoch: 24, batch: 72, loss: 1.4254709482192993\n",
      "epoch: 24, batch: 73, loss: 1.1384226083755493\n",
      "epoch: 24, batch: 74, loss: 1.7042999267578125\n",
      "epoch: 24, batch: 75, loss: 1.6159782409667969\n",
      "epoch: 24, batch: 76, loss: 1.146705985069275\n",
      "epoch: 24, batch: 77, loss: 1.664101243019104\n",
      "epoch: 24, batch: 78, loss: 1.544363021850586\n",
      "epoch: 24, batch: 79, loss: 1.5537889003753662\n",
      "epoch: 24, batch: 80, loss: 1.5474942922592163\n",
      "epoch: 24, batch: 81, loss: 1.6665070056915283\n",
      "epoch: 24, batch: 82, loss: 1.464615821838379\n",
      "epoch: 24, batch: 83, loss: 1.2541335821151733\n",
      "epoch: 24, batch: 84, loss: 1.3067477941513062\n",
      "epoch: 24, batch: 85, loss: 1.54070246219635\n",
      "epoch: 24, batch: 86, loss: 1.3741343021392822\n",
      "epoch: 24, batch: 87, loss: 2.3905935287475586\n",
      "epoch: 24, batch: 88, loss: 1.4748557806015015\n",
      "epoch: 24, batch: 89, loss: 1.6001513004302979\n",
      "epoch: 24, batch: 90, loss: 1.4650733470916748\n",
      "epoch: 24, batch: 91, loss: 1.3165210485458374\n",
      "epoch: 24, batch: 92, loss: 1.4077472686767578\n",
      "epoch: 24, batch: 93, loss: 1.616288423538208\n",
      "epoch: 24, batch: 94, loss: 1.5177109241485596\n",
      "epoch: 24, batch: 95, loss: 1.6055301427841187\n",
      "epoch: 24, batch: 96, loss: 1.3246617317199707\n",
      "epoch: 24, batch: 97, loss: 1.7833153009414673\n",
      "epoch: 24, batch: 98, loss: 1.3602136373519897\n",
      "epoch: 24, batch: 99, loss: 1.5216586589813232\n",
      "epoch: 24, batch: 100, loss: 1.9204009771347046\n",
      "epoch: 24, batch: 101, loss: 1.6016972064971924\n",
      "epoch: 24, batch: 102, loss: 1.2403905391693115\n",
      "epoch: 24, batch: 103, loss: 1.1684080362319946\n",
      "epoch: 24, batch: 104, loss: 1.586530327796936\n",
      "epoch: 24, batch: 105, loss: 1.5257329940795898\n",
      "epoch: 24, batch: 106, loss: 1.4581495523452759\n",
      "epoch: 24, batch: 107, loss: 1.483252763748169\n",
      "epoch: 24, batch: 108, loss: 1.3594961166381836\n",
      "epoch: 24, batch: 109, loss: 1.7734787464141846\n",
      "epoch: 24, batch: 110, loss: 1.4720661640167236\n",
      "epoch: 24, batch: 111, loss: 1.5433257818222046\n",
      "epoch: 24, batch: 112, loss: 1.3015588521957397\n",
      "epoch: 24, batch: 113, loss: 1.741223931312561\n",
      "epoch: 24, batch: 114, loss: 1.4908758401870728\n",
      "epoch: 24, batch: 115, loss: 1.2430709600448608\n",
      "epoch: 24, batch: 116, loss: 1.5668071508407593\n",
      "epoch: 24, batch: 117, loss: 1.9613653421401978\n",
      "epoch: 24, batch: 118, loss: 1.461396336555481\n",
      "epoch: 24, batch: 119, loss: 1.2480462789535522\n",
      "epoch: 24, batch: 120, loss: 1.4638715982437134\n",
      "epoch: 24, batch: 121, loss: 1.5398002862930298\n",
      "epoch: 24, batch: 122, loss: 1.1170873641967773\n",
      "epoch: 24, batch: 123, loss: 1.583601474761963\n",
      "epoch: 24, batch: 124, loss: 1.8025524616241455\n",
      "epoch: 24, batch: 125, loss: 1.1465167999267578\n",
      "epoch: 24, batch: 126, loss: 1.9883521795272827\n",
      "epoch: 24, batch: 127, loss: 1.6984096765518188\n",
      "epoch: 25, batch: 0, loss: 1.742142677307129\n",
      "epoch: 25, batch: 1, loss: 1.4631410837173462\n",
      "epoch: 25, batch: 2, loss: 1.661577582359314\n",
      "epoch: 25, batch: 3, loss: 1.4386320114135742\n",
      "epoch: 25, batch: 4, loss: 1.374420404434204\n",
      "epoch: 25, batch: 5, loss: 1.8979305028915405\n",
      "epoch: 25, batch: 6, loss: 1.274881362915039\n",
      "epoch: 25, batch: 7, loss: 1.523146390914917\n",
      "epoch: 25, batch: 8, loss: 1.5271060466766357\n",
      "epoch: 25, batch: 9, loss: 1.3195568323135376\n",
      "epoch: 25, batch: 10, loss: 1.9902416467666626\n",
      "epoch: 25, batch: 11, loss: 1.5478016138076782\n",
      "epoch: 25, batch: 12, loss: 1.655539870262146\n",
      "epoch: 25, batch: 13, loss: 1.5814478397369385\n",
      "epoch: 25, batch: 14, loss: 1.875936508178711\n",
      "epoch: 25, batch: 15, loss: 1.3765065670013428\n",
      "epoch: 25, batch: 16, loss: 1.5729902982711792\n",
      "epoch: 25, batch: 17, loss: 1.432340383529663\n",
      "epoch: 25, batch: 18, loss: 1.2729034423828125\n",
      "epoch: 25, batch: 19, loss: 1.2982847690582275\n",
      "epoch: 25, batch: 20, loss: 1.5167311429977417\n",
      "epoch: 25, batch: 21, loss: 1.3309980630874634\n",
      "epoch: 25, batch: 22, loss: 1.367966890335083\n",
      "epoch: 25, batch: 23, loss: 1.53810453414917\n",
      "epoch: 25, batch: 24, loss: 1.4804201126098633\n",
      "epoch: 25, batch: 25, loss: 1.5039784908294678\n",
      "epoch: 25, batch: 26, loss: 1.5233819484710693\n",
      "epoch: 25, batch: 27, loss: 1.2248222827911377\n",
      "epoch: 25, batch: 28, loss: 1.5865347385406494\n",
      "epoch: 25, batch: 29, loss: 1.651445746421814\n",
      "epoch: 25, batch: 30, loss: 1.525375247001648\n",
      "epoch: 25, batch: 31, loss: 1.4192354679107666\n",
      "epoch: 25, batch: 32, loss: 1.4718129634857178\n",
      "epoch: 25, batch: 33, loss: 1.142552137374878\n",
      "epoch: 25, batch: 34, loss: 2.17116117477417\n",
      "epoch: 25, batch: 35, loss: 1.3811376094818115\n",
      "epoch: 25, batch: 36, loss: 1.2827595472335815\n",
      "epoch: 25, batch: 37, loss: 1.7258033752441406\n",
      "epoch: 25, batch: 38, loss: 1.4932136535644531\n",
      "epoch: 25, batch: 39, loss: 1.322047233581543\n",
      "epoch: 25, batch: 40, loss: 1.3035285472869873\n",
      "epoch: 25, batch: 41, loss: 1.6933488845825195\n",
      "epoch: 25, batch: 42, loss: 1.2851731777191162\n",
      "epoch: 25, batch: 43, loss: 1.7119624614715576\n",
      "epoch: 25, batch: 44, loss: 1.7368147373199463\n",
      "epoch: 25, batch: 45, loss: 1.5837451219558716\n",
      "epoch: 25, batch: 46, loss: 1.4614551067352295\n",
      "epoch: 25, batch: 47, loss: 1.8072115182876587\n",
      "epoch: 25, batch: 48, loss: 1.4580891132354736\n",
      "epoch: 25, batch: 49, loss: 1.6111831665039062\n",
      "epoch: 25, batch: 50, loss: 1.4140090942382812\n",
      "epoch: 25, batch: 51, loss: 1.5847963094711304\n",
      "epoch: 25, batch: 52, loss: 1.4879300594329834\n",
      "epoch: 25, batch: 53, loss: 1.8302924633026123\n",
      "epoch: 25, batch: 54, loss: 1.512597680091858\n",
      "epoch: 25, batch: 55, loss: 1.60201096534729\n",
      "epoch: 25, batch: 56, loss: 1.4124183654785156\n",
      "epoch: 25, batch: 57, loss: 1.5292869806289673\n",
      "epoch: 25, batch: 58, loss: 1.655272126197815\n",
      "epoch: 25, batch: 59, loss: 1.4070165157318115\n",
      "epoch: 25, batch: 60, loss: 1.5096932649612427\n",
      "epoch: 25, batch: 61, loss: 1.519866704940796\n",
      "epoch: 25, batch: 62, loss: 1.5007076263427734\n",
      "epoch: 25, batch: 63, loss: 1.38362455368042\n",
      "epoch: 25, batch: 64, loss: 1.648655652999878\n",
      "epoch: 25, batch: 65, loss: 1.3622791767120361\n",
      "epoch: 25, batch: 66, loss: 1.6012346744537354\n",
      "epoch: 25, batch: 67, loss: 1.6452480554580688\n",
      "epoch: 25, batch: 68, loss: 1.437086820602417\n",
      "epoch: 25, batch: 69, loss: 1.2352187633514404\n",
      "epoch: 25, batch: 70, loss: 1.9346590042114258\n",
      "epoch: 25, batch: 71, loss: 1.2235853672027588\n",
      "epoch: 25, batch: 72, loss: 1.420813798904419\n",
      "epoch: 25, batch: 73, loss: 1.473067283630371\n",
      "epoch: 25, batch: 74, loss: 1.6552066802978516\n",
      "epoch: 25, batch: 75, loss: 1.3690729141235352\n",
      "epoch: 25, batch: 76, loss: 1.8220977783203125\n",
      "epoch: 25, batch: 77, loss: 1.5216858386993408\n",
      "epoch: 25, batch: 78, loss: 1.7304702997207642\n",
      "epoch: 25, batch: 79, loss: 1.6174781322479248\n",
      "epoch: 25, batch: 80, loss: 1.2450225353240967\n",
      "epoch: 25, batch: 81, loss: 1.4331395626068115\n",
      "epoch: 25, batch: 82, loss: 2.020695209503174\n",
      "epoch: 25, batch: 83, loss: 1.6458152532577515\n",
      "epoch: 25, batch: 84, loss: 1.555005669593811\n",
      "epoch: 25, batch: 85, loss: 1.6002013683319092\n",
      "epoch: 25, batch: 86, loss: 1.3983819484710693\n",
      "epoch: 25, batch: 87, loss: 1.5887372493743896\n",
      "epoch: 25, batch: 88, loss: 1.562805414199829\n",
      "epoch: 25, batch: 89, loss: 1.3350476026535034\n",
      "epoch: 25, batch: 90, loss: 1.7773535251617432\n",
      "epoch: 25, batch: 91, loss: 1.9832589626312256\n",
      "epoch: 25, batch: 92, loss: 1.4378106594085693\n",
      "epoch: 25, batch: 93, loss: 1.5859020948410034\n",
      "epoch: 25, batch: 94, loss: 1.411437749862671\n",
      "epoch: 25, batch: 95, loss: 1.3255178928375244\n",
      "epoch: 25, batch: 96, loss: 1.447242021560669\n",
      "epoch: 25, batch: 97, loss: 1.3071544170379639\n",
      "epoch: 25, batch: 98, loss: 1.9989553689956665\n",
      "epoch: 25, batch: 99, loss: 1.8180164098739624\n",
      "epoch: 25, batch: 100, loss: 1.4030981063842773\n",
      "epoch: 25, batch: 101, loss: 1.4186441898345947\n",
      "epoch: 25, batch: 102, loss: 1.357901930809021\n",
      "epoch: 25, batch: 103, loss: 1.5476255416870117\n",
      "epoch: 25, batch: 104, loss: 1.3841955661773682\n",
      "epoch: 25, batch: 105, loss: 1.4491117000579834\n",
      "epoch: 25, batch: 106, loss: 1.5767407417297363\n",
      "epoch: 25, batch: 107, loss: 1.8272173404693604\n",
      "epoch: 25, batch: 108, loss: 1.4509339332580566\n",
      "epoch: 25, batch: 109, loss: 1.4479206800460815\n",
      "epoch: 25, batch: 110, loss: 1.5414493083953857\n",
      "epoch: 25, batch: 111, loss: 1.4073044061660767\n",
      "epoch: 25, batch: 112, loss: 1.3336814641952515\n",
      "epoch: 25, batch: 113, loss: 1.7591472864151\n",
      "epoch: 25, batch: 114, loss: 1.2951984405517578\n",
      "epoch: 25, batch: 115, loss: 1.8082870244979858\n",
      "epoch: 25, batch: 116, loss: 1.3116347789764404\n",
      "epoch: 25, batch: 117, loss: 1.6522201299667358\n",
      "epoch: 25, batch: 118, loss: 1.429977536201477\n",
      "epoch: 25, batch: 119, loss: 1.5860493183135986\n",
      "epoch: 25, batch: 120, loss: 1.55103600025177\n",
      "epoch: 25, batch: 121, loss: 1.4513877630233765\n",
      "epoch: 25, batch: 122, loss: 1.41336989402771\n",
      "epoch: 25, batch: 123, loss: 1.621246099472046\n",
      "epoch: 25, batch: 124, loss: 1.6863946914672852\n",
      "epoch: 25, batch: 125, loss: 1.6678931713104248\n",
      "epoch: 25, batch: 126, loss: 1.6613285541534424\n",
      "epoch: 25, batch: 127, loss: 1.727844476699829\n",
      "epoch: 26, batch: 0, loss: 1.4263025522232056\n",
      "epoch: 26, batch: 1, loss: 1.8792308568954468\n",
      "epoch: 26, batch: 2, loss: 1.3747955560684204\n",
      "epoch: 26, batch: 3, loss: 1.5484737157821655\n",
      "epoch: 26, batch: 4, loss: 1.4166442155838013\n",
      "epoch: 26, batch: 5, loss: 1.458787441253662\n",
      "epoch: 26, batch: 6, loss: 1.520262360572815\n",
      "epoch: 26, batch: 7, loss: 1.526153326034546\n",
      "epoch: 26, batch: 8, loss: 1.571818232536316\n",
      "epoch: 26, batch: 9, loss: 1.6008126735687256\n",
      "epoch: 26, batch: 10, loss: 1.4553543329238892\n",
      "epoch: 26, batch: 11, loss: 1.490631341934204\n",
      "epoch: 26, batch: 12, loss: 1.4762259721755981\n",
      "epoch: 26, batch: 13, loss: 1.5615723133087158\n",
      "epoch: 26, batch: 14, loss: 1.6254526376724243\n",
      "epoch: 26, batch: 15, loss: 1.6778757572174072\n",
      "epoch: 26, batch: 16, loss: 1.7926113605499268\n",
      "epoch: 26, batch: 17, loss: 1.7467944622039795\n",
      "epoch: 26, batch: 18, loss: 1.2167327404022217\n",
      "epoch: 26, batch: 19, loss: 1.4579122066497803\n",
      "epoch: 26, batch: 20, loss: 1.4983856678009033\n",
      "epoch: 26, batch: 21, loss: 1.858668565750122\n",
      "epoch: 26, batch: 22, loss: 1.4262113571166992\n",
      "epoch: 26, batch: 23, loss: 1.2859300374984741\n",
      "epoch: 26, batch: 24, loss: 1.6914106607437134\n",
      "epoch: 26, batch: 25, loss: 1.2301690578460693\n",
      "epoch: 26, batch: 26, loss: 1.4390723705291748\n",
      "epoch: 26, batch: 27, loss: 1.8326612710952759\n",
      "epoch: 26, batch: 28, loss: 1.3912761211395264\n",
      "epoch: 26, batch: 29, loss: 1.5378491878509521\n",
      "epoch: 26, batch: 30, loss: 1.8573039770126343\n",
      "epoch: 26, batch: 31, loss: 1.7019093036651611\n",
      "epoch: 26, batch: 32, loss: 1.5927518606185913\n",
      "epoch: 26, batch: 33, loss: 1.6882435083389282\n",
      "epoch: 26, batch: 34, loss: 1.3464930057525635\n",
      "epoch: 26, batch: 35, loss: 1.2701075077056885\n",
      "epoch: 26, batch: 36, loss: 1.4279673099517822\n",
      "epoch: 26, batch: 37, loss: 1.3448846340179443\n",
      "epoch: 26, batch: 38, loss: 1.5181124210357666\n",
      "epoch: 26, batch: 39, loss: 1.3593981266021729\n",
      "epoch: 26, batch: 40, loss: 1.4852657318115234\n",
      "epoch: 26, batch: 41, loss: 1.8125152587890625\n",
      "epoch: 26, batch: 42, loss: 1.3769421577453613\n",
      "epoch: 26, batch: 43, loss: 1.6047064065933228\n",
      "epoch: 26, batch: 44, loss: 1.6038671731948853\n",
      "epoch: 26, batch: 45, loss: 1.4002631902694702\n",
      "epoch: 26, batch: 46, loss: 1.5523303747177124\n",
      "epoch: 26, batch: 47, loss: 1.3368263244628906\n",
      "epoch: 26, batch: 48, loss: 1.4954395294189453\n",
      "epoch: 26, batch: 49, loss: 1.3258908987045288\n",
      "epoch: 26, batch: 50, loss: 1.82240891456604\n",
      "epoch: 26, batch: 51, loss: 1.5388095378875732\n",
      "epoch: 26, batch: 52, loss: 1.6806892156600952\n",
      "epoch: 26, batch: 53, loss: 1.6592826843261719\n",
      "epoch: 26, batch: 54, loss: 1.3652645349502563\n",
      "epoch: 26, batch: 55, loss: 1.5693471431732178\n",
      "epoch: 26, batch: 56, loss: 1.448043704032898\n",
      "epoch: 26, batch: 57, loss: 1.4708521366119385\n",
      "epoch: 26, batch: 58, loss: 1.352026343345642\n",
      "epoch: 26, batch: 59, loss: 1.4589340686798096\n",
      "epoch: 26, batch: 60, loss: 1.5617291927337646\n",
      "epoch: 26, batch: 61, loss: 1.9695103168487549\n",
      "epoch: 26, batch: 62, loss: 1.1038296222686768\n",
      "epoch: 26, batch: 63, loss: 1.5614429712295532\n",
      "epoch: 26, batch: 64, loss: 1.3605419397354126\n",
      "epoch: 26, batch: 65, loss: 1.6518402099609375\n",
      "epoch: 26, batch: 66, loss: 1.264521598815918\n",
      "epoch: 26, batch: 67, loss: 1.22543203830719\n",
      "epoch: 26, batch: 68, loss: 1.4541857242584229\n",
      "epoch: 26, batch: 69, loss: 1.435089111328125\n",
      "epoch: 26, batch: 70, loss: 1.7170257568359375\n",
      "epoch: 26, batch: 71, loss: 1.4561100006103516\n",
      "epoch: 26, batch: 72, loss: 1.706507682800293\n",
      "epoch: 26, batch: 73, loss: 1.4680887460708618\n",
      "epoch: 26, batch: 74, loss: 1.4597532749176025\n",
      "epoch: 26, batch: 75, loss: 1.5190120935440063\n",
      "epoch: 26, batch: 76, loss: 1.5250600576400757\n",
      "epoch: 26, batch: 77, loss: 1.5247100591659546\n",
      "epoch: 26, batch: 78, loss: 1.5415327548980713\n",
      "epoch: 26, batch: 79, loss: 1.5253546237945557\n",
      "epoch: 26, batch: 80, loss: 1.4733681678771973\n",
      "epoch: 26, batch: 81, loss: 1.4359956979751587\n",
      "epoch: 26, batch: 82, loss: 1.8687013387680054\n",
      "epoch: 26, batch: 83, loss: 1.6157894134521484\n",
      "epoch: 26, batch: 84, loss: 1.347553014755249\n",
      "epoch: 26, batch: 85, loss: 1.4977504014968872\n",
      "epoch: 26, batch: 86, loss: 1.4996280670166016\n",
      "epoch: 26, batch: 87, loss: 1.567969799041748\n",
      "epoch: 26, batch: 88, loss: 1.268216848373413\n",
      "epoch: 26, batch: 89, loss: 1.7482125759124756\n",
      "epoch: 26, batch: 90, loss: 1.6309874057769775\n",
      "epoch: 26, batch: 91, loss: 1.3911480903625488\n",
      "epoch: 26, batch: 92, loss: 1.6765921115875244\n",
      "epoch: 26, batch: 93, loss: 1.5071417093276978\n",
      "epoch: 26, batch: 94, loss: 1.5836690664291382\n",
      "epoch: 26, batch: 95, loss: 1.6763824224472046\n",
      "epoch: 26, batch: 96, loss: 1.573498010635376\n",
      "epoch: 26, batch: 97, loss: 1.3551080226898193\n",
      "epoch: 26, batch: 98, loss: 1.651867151260376\n",
      "epoch: 26, batch: 99, loss: 1.6822229623794556\n",
      "epoch: 26, batch: 100, loss: 1.4771169424057007\n",
      "epoch: 26, batch: 101, loss: 1.4125053882598877\n",
      "epoch: 26, batch: 102, loss: 1.3544743061065674\n",
      "epoch: 26, batch: 103, loss: 1.71713387966156\n",
      "epoch: 26, batch: 104, loss: 1.857824683189392\n",
      "epoch: 26, batch: 105, loss: 1.632878065109253\n",
      "epoch: 26, batch: 106, loss: 1.5292651653289795\n",
      "epoch: 26, batch: 107, loss: 1.4723700284957886\n",
      "epoch: 26, batch: 108, loss: 1.6569868326187134\n",
      "epoch: 26, batch: 109, loss: 1.5160109996795654\n",
      "epoch: 26, batch: 110, loss: 1.6338955163955688\n",
      "epoch: 26, batch: 111, loss: 1.5495128631591797\n",
      "epoch: 26, batch: 112, loss: 1.7751705646514893\n",
      "epoch: 26, batch: 113, loss: 1.455701231956482\n",
      "epoch: 26, batch: 114, loss: 1.605263352394104\n",
      "epoch: 26, batch: 115, loss: 1.8316688537597656\n",
      "epoch: 26, batch: 116, loss: 1.4538726806640625\n",
      "epoch: 26, batch: 117, loss: 1.4801557064056396\n",
      "epoch: 26, batch: 118, loss: 1.409263253211975\n",
      "epoch: 26, batch: 119, loss: 1.810593843460083\n",
      "epoch: 26, batch: 120, loss: 1.4477565288543701\n",
      "epoch: 26, batch: 121, loss: 1.7605597972869873\n",
      "epoch: 26, batch: 122, loss: 1.5914649963378906\n",
      "epoch: 26, batch: 123, loss: 1.7375919818878174\n",
      "epoch: 26, batch: 124, loss: 1.4465763568878174\n",
      "epoch: 26, batch: 125, loss: 1.4728038311004639\n",
      "epoch: 26, batch: 126, loss: 1.4145843982696533\n",
      "epoch: 26, batch: 127, loss: 1.4194557666778564\n",
      "epoch: 27, batch: 0, loss: 1.7836164236068726\n",
      "epoch: 27, batch: 1, loss: 1.5854237079620361\n",
      "epoch: 27, batch: 2, loss: 1.7321830987930298\n",
      "epoch: 27, batch: 3, loss: 1.3858489990234375\n",
      "epoch: 27, batch: 4, loss: 1.5588945150375366\n",
      "epoch: 27, batch: 5, loss: 1.3095835447311401\n",
      "epoch: 27, batch: 6, loss: 1.4579336643218994\n",
      "epoch: 27, batch: 7, loss: 1.7176929712295532\n",
      "epoch: 27, batch: 8, loss: 1.614092230796814\n",
      "epoch: 27, batch: 9, loss: 1.3619673252105713\n",
      "epoch: 27, batch: 10, loss: 1.6787707805633545\n",
      "epoch: 27, batch: 11, loss: 1.5162941217422485\n",
      "epoch: 27, batch: 12, loss: 1.4163705110549927\n",
      "epoch: 27, batch: 13, loss: 1.3793452978134155\n",
      "epoch: 27, batch: 14, loss: 1.5246731042861938\n",
      "epoch: 27, batch: 15, loss: 1.4580250978469849\n",
      "epoch: 27, batch: 16, loss: 1.7239545583724976\n",
      "epoch: 27, batch: 17, loss: 1.4138052463531494\n",
      "epoch: 27, batch: 18, loss: 1.735412359237671\n",
      "epoch: 27, batch: 19, loss: 1.456181287765503\n",
      "epoch: 27, batch: 20, loss: 1.4755340814590454\n",
      "epoch: 27, batch: 21, loss: 2.0260443687438965\n",
      "epoch: 27, batch: 22, loss: 1.7014328241348267\n",
      "epoch: 27, batch: 23, loss: 1.7930610179901123\n",
      "epoch: 27, batch: 24, loss: 1.558929443359375\n",
      "epoch: 27, batch: 25, loss: 1.310957431793213\n",
      "epoch: 27, batch: 26, loss: 1.7388769388198853\n",
      "epoch: 27, batch: 27, loss: 1.601112961769104\n",
      "epoch: 27, batch: 28, loss: 1.4528838396072388\n",
      "epoch: 27, batch: 29, loss: 1.6058248281478882\n",
      "epoch: 27, batch: 30, loss: 1.7252117395401\n",
      "epoch: 27, batch: 31, loss: 1.5435478687286377\n",
      "epoch: 27, batch: 32, loss: 1.4631309509277344\n",
      "epoch: 27, batch: 33, loss: 1.6876237392425537\n",
      "epoch: 27, batch: 34, loss: 1.5733873844146729\n",
      "epoch: 27, batch: 35, loss: 1.8726379871368408\n",
      "epoch: 27, batch: 36, loss: 1.4191381931304932\n",
      "epoch: 27, batch: 37, loss: 1.5279443264007568\n",
      "epoch: 27, batch: 38, loss: 1.6181399822235107\n",
      "epoch: 27, batch: 39, loss: 1.4553000926971436\n",
      "epoch: 27, batch: 40, loss: 1.3578662872314453\n",
      "epoch: 27, batch: 41, loss: 1.531923770904541\n",
      "epoch: 27, batch: 42, loss: 1.5319006443023682\n",
      "epoch: 27, batch: 43, loss: 1.3621985912322998\n",
      "epoch: 27, batch: 44, loss: 1.5717098712921143\n",
      "epoch: 27, batch: 45, loss: 1.4283279180526733\n",
      "epoch: 27, batch: 46, loss: 1.4968676567077637\n",
      "epoch: 27, batch: 47, loss: 1.4729130268096924\n",
      "epoch: 27, batch: 48, loss: 1.7377121448516846\n",
      "epoch: 27, batch: 49, loss: 1.2818201780319214\n",
      "epoch: 27, batch: 50, loss: 1.4652047157287598\n",
      "epoch: 27, batch: 51, loss: 1.6274707317352295\n",
      "epoch: 27, batch: 52, loss: 1.392998456954956\n",
      "epoch: 27, batch: 53, loss: 1.44635808467865\n",
      "epoch: 27, batch: 54, loss: 1.3905197381973267\n",
      "epoch: 27, batch: 55, loss: 1.388383388519287\n",
      "epoch: 27, batch: 56, loss: 1.420627236366272\n",
      "epoch: 27, batch: 57, loss: 1.6676067113876343\n",
      "epoch: 27, batch: 58, loss: 1.8328564167022705\n",
      "epoch: 27, batch: 59, loss: 1.5319576263427734\n",
      "epoch: 27, batch: 60, loss: 1.247959852218628\n",
      "epoch: 27, batch: 61, loss: 1.539856195449829\n",
      "epoch: 27, batch: 62, loss: 1.7268707752227783\n",
      "epoch: 27, batch: 63, loss: 1.52623450756073\n",
      "epoch: 27, batch: 64, loss: 1.1383655071258545\n",
      "epoch: 27, batch: 65, loss: 1.4065430164337158\n",
      "epoch: 27, batch: 66, loss: 1.7894970178604126\n",
      "epoch: 27, batch: 67, loss: 1.401397943496704\n",
      "epoch: 27, batch: 68, loss: 1.4499719142913818\n",
      "epoch: 27, batch: 69, loss: 1.6492931842803955\n",
      "epoch: 27, batch: 70, loss: 1.3473554849624634\n",
      "epoch: 27, batch: 71, loss: 1.493345022201538\n",
      "epoch: 27, batch: 72, loss: 1.3500537872314453\n",
      "epoch: 27, batch: 73, loss: 1.2669150829315186\n",
      "epoch: 27, batch: 74, loss: 1.3897541761398315\n",
      "epoch: 27, batch: 75, loss: 1.7445571422576904\n",
      "epoch: 27, batch: 76, loss: 1.8014808893203735\n",
      "epoch: 27, batch: 77, loss: 1.321221947669983\n",
      "epoch: 27, batch: 78, loss: 1.5848314762115479\n",
      "epoch: 27, batch: 79, loss: 1.330333948135376\n",
      "epoch: 27, batch: 80, loss: 1.5424456596374512\n",
      "epoch: 27, batch: 81, loss: 1.5916072130203247\n",
      "epoch: 27, batch: 82, loss: 1.5411109924316406\n",
      "epoch: 27, batch: 83, loss: 1.3491430282592773\n",
      "epoch: 27, batch: 84, loss: 1.5311050415039062\n",
      "epoch: 27, batch: 85, loss: 1.366647481918335\n",
      "epoch: 27, batch: 86, loss: 1.9100151062011719\n",
      "epoch: 27, batch: 87, loss: 1.4964447021484375\n",
      "epoch: 27, batch: 88, loss: 1.4609452486038208\n",
      "epoch: 27, batch: 89, loss: 1.5876671075820923\n",
      "epoch: 27, batch: 90, loss: 1.5338542461395264\n",
      "epoch: 27, batch: 91, loss: 1.776095986366272\n",
      "epoch: 27, batch: 92, loss: 1.3967556953430176\n",
      "epoch: 27, batch: 93, loss: 1.4478529691696167\n",
      "epoch: 27, batch: 94, loss: 1.6325976848602295\n",
      "epoch: 27, batch: 95, loss: 1.8559211492538452\n",
      "epoch: 27, batch: 96, loss: 1.4908114671707153\n",
      "epoch: 27, batch: 97, loss: 1.692795991897583\n",
      "epoch: 27, batch: 98, loss: 1.6136223077774048\n",
      "epoch: 27, batch: 99, loss: 1.4525792598724365\n",
      "epoch: 27, batch: 100, loss: 1.3582531213760376\n",
      "epoch: 27, batch: 101, loss: 1.579469084739685\n",
      "epoch: 27, batch: 102, loss: 1.7385215759277344\n",
      "epoch: 27, batch: 103, loss: 1.4763262271881104\n",
      "epoch: 27, batch: 104, loss: 1.386808156967163\n",
      "epoch: 27, batch: 105, loss: 1.5608017444610596\n",
      "epoch: 27, batch: 106, loss: 1.3293483257293701\n",
      "epoch: 27, batch: 107, loss: 1.4788678884506226\n",
      "epoch: 27, batch: 108, loss: 1.5711514949798584\n",
      "epoch: 27, batch: 109, loss: 1.3750941753387451\n",
      "epoch: 27, batch: 110, loss: 1.664506196975708\n",
      "epoch: 27, batch: 111, loss: 1.5165423154830933\n",
      "epoch: 27, batch: 112, loss: 1.4599486589431763\n",
      "epoch: 27, batch: 113, loss: 1.6050589084625244\n",
      "epoch: 27, batch: 114, loss: 1.5754637718200684\n",
      "epoch: 27, batch: 115, loss: 1.2863351106643677\n",
      "epoch: 27, batch: 116, loss: 1.5105679035186768\n",
      "epoch: 27, batch: 117, loss: 1.6689345836639404\n",
      "epoch: 27, batch: 118, loss: 1.5505698919296265\n",
      "epoch: 27, batch: 119, loss: 1.5557420253753662\n",
      "epoch: 27, batch: 120, loss: 1.6107451915740967\n",
      "epoch: 27, batch: 121, loss: 1.8078320026397705\n",
      "epoch: 27, batch: 122, loss: 1.4932739734649658\n",
      "epoch: 27, batch: 123, loss: 1.5185662508010864\n",
      "epoch: 27, batch: 124, loss: 1.624963402748108\n",
      "epoch: 27, batch: 125, loss: 1.5753564834594727\n",
      "epoch: 27, batch: 126, loss: 1.6016992330551147\n",
      "epoch: 27, batch: 127, loss: 1.4588396549224854\n",
      "epoch: 28, batch: 0, loss: 1.4116508960723877\n",
      "epoch: 28, batch: 1, loss: 1.6389329433441162\n",
      "epoch: 28, batch: 2, loss: 1.5310373306274414\n",
      "epoch: 28, batch: 3, loss: 1.2718709707260132\n",
      "epoch: 28, batch: 4, loss: 1.3668321371078491\n",
      "epoch: 28, batch: 5, loss: 1.5319024324417114\n",
      "epoch: 28, batch: 6, loss: 1.4939250946044922\n",
      "epoch: 28, batch: 7, loss: 1.3259642124176025\n",
      "epoch: 28, batch: 8, loss: 1.5327823162078857\n",
      "epoch: 28, batch: 9, loss: 1.8080791234970093\n",
      "epoch: 28, batch: 10, loss: 1.7631486654281616\n",
      "epoch: 28, batch: 11, loss: 1.8107738494873047\n",
      "epoch: 28, batch: 12, loss: 1.7509534358978271\n",
      "epoch: 28, batch: 13, loss: 1.5272070169448853\n",
      "epoch: 28, batch: 14, loss: 1.6689189672470093\n",
      "epoch: 28, batch: 15, loss: 1.4718443155288696\n",
      "epoch: 28, batch: 16, loss: 1.529255747795105\n",
      "epoch: 28, batch: 17, loss: 1.4638350009918213\n",
      "epoch: 28, batch: 18, loss: 1.5164635181427002\n",
      "epoch: 28, batch: 19, loss: 1.5604394674301147\n",
      "epoch: 28, batch: 20, loss: 1.512467384338379\n",
      "epoch: 28, batch: 21, loss: 1.5861831903457642\n",
      "epoch: 28, batch: 22, loss: 1.6060651540756226\n",
      "epoch: 28, batch: 23, loss: 1.4760358333587646\n",
      "epoch: 28, batch: 24, loss: 1.4724624156951904\n",
      "epoch: 28, batch: 25, loss: 1.5634372234344482\n",
      "epoch: 28, batch: 26, loss: 1.5500441789627075\n",
      "epoch: 28, batch: 27, loss: 1.3842231035232544\n",
      "epoch: 28, batch: 28, loss: 1.6365944147109985\n",
      "epoch: 28, batch: 29, loss: 1.6943477392196655\n",
      "epoch: 28, batch: 30, loss: 1.2534794807434082\n",
      "epoch: 28, batch: 31, loss: 1.5743539333343506\n",
      "epoch: 28, batch: 32, loss: 1.7938823699951172\n",
      "epoch: 28, batch: 33, loss: 1.5202192068099976\n",
      "epoch: 28, batch: 34, loss: 1.6561648845672607\n",
      "epoch: 28, batch: 35, loss: 1.4766743183135986\n",
      "epoch: 28, batch: 36, loss: 1.457430124282837\n",
      "epoch: 28, batch: 37, loss: 1.3417127132415771\n",
      "epoch: 28, batch: 38, loss: 1.3460941314697266\n",
      "epoch: 28, batch: 39, loss: 1.5120279788970947\n",
      "epoch: 28, batch: 40, loss: 1.7800476551055908\n",
      "epoch: 28, batch: 41, loss: 1.527427077293396\n",
      "epoch: 28, batch: 42, loss: 1.6957298517227173\n",
      "epoch: 28, batch: 43, loss: 1.5229607820510864\n",
      "epoch: 28, batch: 44, loss: 1.6168121099472046\n",
      "epoch: 28, batch: 45, loss: 1.36708664894104\n",
      "epoch: 28, batch: 46, loss: 1.677424430847168\n",
      "epoch: 28, batch: 47, loss: 1.7399238348007202\n",
      "epoch: 28, batch: 48, loss: 1.2149097919464111\n",
      "epoch: 28, batch: 49, loss: 1.230390191078186\n",
      "epoch: 28, batch: 50, loss: 1.403759241104126\n",
      "epoch: 28, batch: 51, loss: 1.5703275203704834\n",
      "epoch: 28, batch: 52, loss: 1.5680830478668213\n",
      "epoch: 28, batch: 53, loss: 1.3272887468338013\n",
      "epoch: 28, batch: 54, loss: 1.2822480201721191\n",
      "epoch: 28, batch: 55, loss: 1.5929633378982544\n",
      "epoch: 28, batch: 56, loss: 1.2036203145980835\n",
      "epoch: 28, batch: 57, loss: 1.5386310815811157\n",
      "epoch: 28, batch: 58, loss: 1.7038934230804443\n",
      "epoch: 28, batch: 59, loss: 1.5642130374908447\n",
      "epoch: 28, batch: 60, loss: 1.6913379430770874\n",
      "epoch: 28, batch: 61, loss: 1.7452285289764404\n",
      "epoch: 28, batch: 62, loss: 1.4782437086105347\n",
      "epoch: 28, batch: 63, loss: 1.6208664178848267\n",
      "epoch: 28, batch: 64, loss: 1.5577419996261597\n",
      "epoch: 28, batch: 65, loss: 1.5121347904205322\n",
      "epoch: 28, batch: 66, loss: 1.4697290658950806\n",
      "epoch: 28, batch: 67, loss: 1.6866347789764404\n",
      "epoch: 28, batch: 68, loss: 1.670334815979004\n",
      "epoch: 28, batch: 69, loss: 1.2799288034439087\n",
      "epoch: 28, batch: 70, loss: 1.609328269958496\n",
      "epoch: 28, batch: 71, loss: 1.5477782487869263\n",
      "epoch: 28, batch: 72, loss: 1.3482575416564941\n",
      "epoch: 28, batch: 73, loss: 1.3040481805801392\n",
      "epoch: 28, batch: 74, loss: 1.556606650352478\n",
      "epoch: 28, batch: 75, loss: 1.5067037343978882\n",
      "epoch: 28, batch: 76, loss: 1.636916160583496\n",
      "epoch: 28, batch: 77, loss: 1.2056018114089966\n",
      "epoch: 28, batch: 78, loss: 1.5749787092208862\n",
      "epoch: 28, batch: 79, loss: 1.647486686706543\n",
      "epoch: 28, batch: 80, loss: 1.5470882654190063\n",
      "epoch: 28, batch: 81, loss: 1.650640845298767\n",
      "epoch: 28, batch: 82, loss: 1.6466810703277588\n",
      "epoch: 28, batch: 83, loss: 1.6357171535491943\n",
      "epoch: 28, batch: 84, loss: 1.6123855113983154\n",
      "epoch: 28, batch: 85, loss: 1.7710866928100586\n",
      "epoch: 28, batch: 86, loss: 1.6296226978302002\n",
      "epoch: 28, batch: 87, loss: 1.6159021854400635\n",
      "epoch: 28, batch: 88, loss: 1.3277984857559204\n",
      "epoch: 28, batch: 89, loss: 1.479934573173523\n",
      "epoch: 28, batch: 90, loss: 1.6066814661026\n",
      "epoch: 28, batch: 91, loss: 1.67226243019104\n",
      "epoch: 28, batch: 92, loss: 1.3910009860992432\n",
      "epoch: 28, batch: 93, loss: 1.4546998739242554\n",
      "epoch: 28, batch: 94, loss: 1.6036983728408813\n",
      "epoch: 28, batch: 95, loss: 1.4931144714355469\n",
      "epoch: 28, batch: 96, loss: 1.2242929935455322\n",
      "epoch: 28, batch: 97, loss: 1.4920485019683838\n",
      "epoch: 28, batch: 98, loss: 1.4901862144470215\n",
      "epoch: 28, batch: 99, loss: 1.6250298023223877\n",
      "epoch: 28, batch: 100, loss: 1.5056010484695435\n",
      "epoch: 28, batch: 101, loss: 1.8235204219818115\n",
      "epoch: 28, batch: 102, loss: 1.4670264720916748\n",
      "epoch: 28, batch: 103, loss: 1.2262041568756104\n",
      "epoch: 28, batch: 104, loss: 1.6499836444854736\n",
      "epoch: 28, batch: 105, loss: 1.5531127452850342\n",
      "epoch: 28, batch: 106, loss: 1.5113979578018188\n",
      "epoch: 28, batch: 107, loss: 1.635597586631775\n",
      "epoch: 28, batch: 108, loss: 1.6775054931640625\n",
      "epoch: 28, batch: 109, loss: 1.5221502780914307\n",
      "epoch: 28, batch: 110, loss: 1.6463935375213623\n",
      "epoch: 28, batch: 111, loss: 1.5993010997772217\n",
      "epoch: 28, batch: 112, loss: 1.20932936668396\n",
      "epoch: 28, batch: 113, loss: 1.556943655014038\n",
      "epoch: 28, batch: 114, loss: 1.652227759361267\n",
      "epoch: 28, batch: 115, loss: 1.4799394607543945\n",
      "epoch: 28, batch: 116, loss: 1.6624481678009033\n",
      "epoch: 28, batch: 117, loss: 1.257568120956421\n",
      "epoch: 28, batch: 118, loss: 1.6125891208648682\n",
      "epoch: 28, batch: 119, loss: 1.7285598516464233\n",
      "epoch: 28, batch: 120, loss: 1.113641619682312\n",
      "epoch: 28, batch: 121, loss: 1.2957292795181274\n",
      "epoch: 28, batch: 122, loss: 1.6922184228897095\n",
      "epoch: 28, batch: 123, loss: 1.6490246057510376\n",
      "epoch: 28, batch: 124, loss: 1.4978885650634766\n",
      "epoch: 28, batch: 125, loss: 1.6007421016693115\n",
      "epoch: 28, batch: 126, loss: 1.5581945180892944\n",
      "epoch: 28, batch: 127, loss: 1.5938243865966797\n",
      "epoch: 29, batch: 0, loss: 1.304471731185913\n",
      "epoch: 29, batch: 1, loss: 1.4662578105926514\n",
      "epoch: 29, batch: 2, loss: 1.4702527523040771\n",
      "epoch: 29, batch: 3, loss: 1.8599560260772705\n",
      "epoch: 29, batch: 4, loss: 1.667986273765564\n",
      "epoch: 29, batch: 5, loss: 1.1680150032043457\n",
      "epoch: 29, batch: 6, loss: 1.3409292697906494\n",
      "epoch: 29, batch: 7, loss: 1.3117226362228394\n",
      "epoch: 29, batch: 8, loss: 1.6135826110839844\n",
      "epoch: 29, batch: 9, loss: 1.5177457332611084\n",
      "epoch: 29, batch: 10, loss: 1.4861979484558105\n",
      "epoch: 29, batch: 11, loss: 1.4056355953216553\n",
      "epoch: 29, batch: 12, loss: 1.6684131622314453\n",
      "epoch: 29, batch: 13, loss: 1.334151029586792\n",
      "epoch: 29, batch: 14, loss: 1.4723279476165771\n",
      "epoch: 29, batch: 15, loss: 1.6543794870376587\n",
      "epoch: 29, batch: 16, loss: 1.3759185075759888\n",
      "epoch: 29, batch: 17, loss: 1.694598913192749\n",
      "epoch: 29, batch: 18, loss: 1.596252679824829\n",
      "epoch: 29, batch: 19, loss: 1.5341274738311768\n",
      "epoch: 29, batch: 20, loss: 1.6024589538574219\n",
      "epoch: 29, batch: 21, loss: 1.5001018047332764\n",
      "epoch: 29, batch: 22, loss: 1.576657772064209\n",
      "epoch: 29, batch: 23, loss: 1.6908161640167236\n",
      "epoch: 29, batch: 24, loss: 1.5329216718673706\n",
      "epoch: 29, batch: 25, loss: 1.5453100204467773\n",
      "epoch: 29, batch: 26, loss: 0.9945600628852844\n",
      "epoch: 29, batch: 27, loss: 1.8069957494735718\n",
      "epoch: 29, batch: 28, loss: 1.3603683710098267\n",
      "epoch: 29, batch: 29, loss: 1.534655213356018\n",
      "epoch: 29, batch: 30, loss: 1.6189086437225342\n",
      "epoch: 29, batch: 31, loss: 1.5523335933685303\n",
      "epoch: 29, batch: 32, loss: 1.6038424968719482\n",
      "epoch: 29, batch: 33, loss: 1.6575632095336914\n",
      "epoch: 29, batch: 34, loss: 1.4800794124603271\n",
      "epoch: 29, batch: 35, loss: 1.4805927276611328\n",
      "epoch: 29, batch: 36, loss: 1.2944486141204834\n",
      "epoch: 29, batch: 37, loss: 1.6427690982818604\n",
      "epoch: 29, batch: 38, loss: 1.7373462915420532\n",
      "epoch: 29, batch: 39, loss: 1.2478190660476685\n",
      "epoch: 29, batch: 40, loss: 1.3738762140274048\n",
      "epoch: 29, batch: 41, loss: 1.6847118139266968\n",
      "epoch: 29, batch: 42, loss: 1.6384493112564087\n",
      "epoch: 29, batch: 43, loss: 1.5717061758041382\n",
      "epoch: 29, batch: 44, loss: 1.3076448440551758\n",
      "epoch: 29, batch: 45, loss: 1.4578557014465332\n",
      "epoch: 29, batch: 46, loss: 1.6136720180511475\n",
      "epoch: 29, batch: 47, loss: 1.8455498218536377\n",
      "epoch: 29, batch: 48, loss: 1.6435505151748657\n",
      "epoch: 29, batch: 49, loss: 1.6531174182891846\n",
      "epoch: 29, batch: 50, loss: 1.522075891494751\n",
      "epoch: 29, batch: 51, loss: 1.3104426860809326\n",
      "epoch: 29, batch: 52, loss: 1.6656936407089233\n",
      "epoch: 29, batch: 53, loss: 1.5561476945877075\n",
      "epoch: 29, batch: 54, loss: 1.5097843408584595\n",
      "epoch: 29, batch: 55, loss: 1.673386812210083\n",
      "epoch: 29, batch: 56, loss: 1.6942275762557983\n",
      "epoch: 29, batch: 57, loss: 1.4613568782806396\n",
      "epoch: 29, batch: 58, loss: 1.2995951175689697\n",
      "epoch: 29, batch: 59, loss: 1.6275438070297241\n",
      "epoch: 29, batch: 60, loss: 1.7063274383544922\n",
      "epoch: 29, batch: 61, loss: 1.484865427017212\n",
      "epoch: 29, batch: 62, loss: 1.4847071170806885\n",
      "epoch: 29, batch: 63, loss: 1.5174808502197266\n",
      "epoch: 29, batch: 64, loss: 1.6185325384140015\n",
      "epoch: 29, batch: 65, loss: 1.5301865339279175\n",
      "epoch: 29, batch: 66, loss: 1.5130420923233032\n",
      "epoch: 29, batch: 67, loss: 1.653193712234497\n",
      "epoch: 29, batch: 68, loss: 1.5462688207626343\n",
      "epoch: 29, batch: 69, loss: 1.560545563697815\n",
      "epoch: 29, batch: 70, loss: 1.5783790349960327\n",
      "epoch: 29, batch: 71, loss: 1.856382131576538\n",
      "epoch: 29, batch: 72, loss: 1.6935560703277588\n",
      "epoch: 29, batch: 73, loss: 1.2567131519317627\n",
      "epoch: 29, batch: 74, loss: 1.1349955797195435\n",
      "epoch: 29, batch: 75, loss: 1.3053828477859497\n",
      "epoch: 29, batch: 76, loss: 1.7000160217285156\n",
      "epoch: 29, batch: 77, loss: 1.7870947122573853\n",
      "epoch: 29, batch: 78, loss: 1.633920669555664\n",
      "epoch: 29, batch: 79, loss: 1.6901962757110596\n",
      "epoch: 29, batch: 80, loss: 1.508732795715332\n",
      "epoch: 29, batch: 81, loss: 1.4527682065963745\n",
      "epoch: 29, batch: 82, loss: 1.5128390789031982\n",
      "epoch: 29, batch: 83, loss: 1.4760669469833374\n",
      "epoch: 29, batch: 84, loss: 1.187575340270996\n",
      "epoch: 29, batch: 85, loss: 1.3467844724655151\n",
      "epoch: 29, batch: 86, loss: 1.6875776052474976\n",
      "epoch: 29, batch: 87, loss: 1.68852961063385\n",
      "epoch: 29, batch: 88, loss: 1.4996049404144287\n",
      "epoch: 29, batch: 89, loss: 1.312519907951355\n",
      "epoch: 29, batch: 90, loss: 1.8497329950332642\n",
      "epoch: 29, batch: 91, loss: 1.47480046749115\n",
      "epoch: 29, batch: 92, loss: 1.644266128540039\n",
      "epoch: 29, batch: 93, loss: 1.2993577718734741\n",
      "epoch: 29, batch: 94, loss: 1.6976051330566406\n",
      "epoch: 29, batch: 95, loss: 1.6848052740097046\n",
      "epoch: 29, batch: 96, loss: 1.4662790298461914\n",
      "epoch: 29, batch: 97, loss: 1.4585278034210205\n",
      "epoch: 29, batch: 98, loss: 1.4484198093414307\n",
      "epoch: 29, batch: 99, loss: 1.169284462928772\n",
      "epoch: 29, batch: 100, loss: 1.6304317712783813\n",
      "epoch: 29, batch: 101, loss: 1.6046030521392822\n",
      "epoch: 29, batch: 102, loss: 1.1214094161987305\n",
      "epoch: 29, batch: 103, loss: 1.5288803577423096\n",
      "epoch: 29, batch: 104, loss: 1.8111293315887451\n",
      "epoch: 29, batch: 105, loss: 1.3050225973129272\n",
      "epoch: 29, batch: 106, loss: 1.6528594493865967\n",
      "epoch: 29, batch: 107, loss: 1.6371749639511108\n",
      "epoch: 29, batch: 108, loss: 1.5186140537261963\n",
      "epoch: 29, batch: 109, loss: 1.303518533706665\n",
      "epoch: 29, batch: 110, loss: 1.7088534832000732\n",
      "epoch: 29, batch: 111, loss: 1.3932158946990967\n",
      "epoch: 29, batch: 112, loss: 1.4055287837982178\n",
      "epoch: 29, batch: 113, loss: 1.7187827825546265\n",
      "epoch: 29, batch: 114, loss: 1.8470375537872314\n",
      "epoch: 29, batch: 115, loss: 1.2848727703094482\n",
      "epoch: 29, batch: 116, loss: 1.5235263109207153\n",
      "epoch: 29, batch: 117, loss: 1.6805928945541382\n",
      "epoch: 29, batch: 118, loss: 1.6346139907836914\n",
      "epoch: 29, batch: 119, loss: 1.5307481288909912\n",
      "epoch: 29, batch: 120, loss: 1.6757783889770508\n",
      "epoch: 29, batch: 121, loss: 1.1978106498718262\n",
      "epoch: 29, batch: 122, loss: 1.7535158395767212\n",
      "epoch: 29, batch: 123, loss: 1.6167749166488647\n",
      "epoch: 29, batch: 124, loss: 1.698973298072815\n",
      "epoch: 29, batch: 125, loss: 1.7335847616195679\n",
      "epoch: 29, batch: 126, loss: 1.6201515197753906\n",
      "epoch: 29, batch: 127, loss: 1.676666259765625\n",
      "epoch: 30, batch: 0, loss: 1.858469009399414\n",
      "epoch: 30, batch: 1, loss: 1.3336398601531982\n",
      "epoch: 30, batch: 2, loss: 1.6562912464141846\n",
      "epoch: 30, batch: 3, loss: 1.3922210931777954\n",
      "epoch: 30, batch: 4, loss: 1.7741448879241943\n",
      "epoch: 30, batch: 5, loss: 1.180620551109314\n",
      "epoch: 30, batch: 6, loss: 1.3485829830169678\n",
      "epoch: 30, batch: 7, loss: 1.7631759643554688\n",
      "epoch: 30, batch: 8, loss: 1.6489216089248657\n",
      "epoch: 30, batch: 9, loss: 1.1780699491500854\n",
      "epoch: 30, batch: 10, loss: 1.335214614868164\n",
      "epoch: 30, batch: 11, loss: 1.5192168951034546\n",
      "epoch: 30, batch: 12, loss: 1.3506355285644531\n",
      "epoch: 30, batch: 13, loss: 1.7601664066314697\n",
      "epoch: 30, batch: 14, loss: 1.462554693222046\n",
      "epoch: 30, batch: 15, loss: 1.7908477783203125\n",
      "epoch: 30, batch: 16, loss: 1.1099287271499634\n",
      "epoch: 30, batch: 17, loss: 1.5481430292129517\n",
      "epoch: 30, batch: 18, loss: 1.3191099166870117\n",
      "epoch: 30, batch: 19, loss: 1.577438235282898\n",
      "epoch: 30, batch: 20, loss: 1.3008393049240112\n",
      "epoch: 30, batch: 21, loss: 1.6244055032730103\n",
      "epoch: 30, batch: 22, loss: 1.576674461364746\n",
      "epoch: 30, batch: 23, loss: 1.5586915016174316\n",
      "epoch: 30, batch: 24, loss: 1.5366461277008057\n",
      "epoch: 30, batch: 25, loss: 1.3286845684051514\n",
      "epoch: 30, batch: 26, loss: 1.60761296749115\n",
      "epoch: 30, batch: 27, loss: 1.5806375741958618\n",
      "epoch: 30, batch: 28, loss: 1.8003008365631104\n",
      "epoch: 30, batch: 29, loss: 1.518690824508667\n",
      "epoch: 30, batch: 30, loss: 1.4777066707611084\n",
      "epoch: 30, batch: 31, loss: 1.5785462856292725\n",
      "epoch: 30, batch: 32, loss: 1.5411416292190552\n",
      "epoch: 30, batch: 33, loss: 1.2781693935394287\n",
      "epoch: 30, batch: 34, loss: 1.554099678993225\n",
      "epoch: 30, batch: 35, loss: 1.4446662664413452\n",
      "epoch: 30, batch: 36, loss: 1.6538667678833008\n",
      "epoch: 30, batch: 37, loss: 1.5002186298370361\n",
      "epoch: 30, batch: 38, loss: 1.3804813623428345\n",
      "epoch: 30, batch: 39, loss: 1.261595606803894\n",
      "epoch: 30, batch: 40, loss: 1.7084720134735107\n",
      "epoch: 30, batch: 41, loss: 1.750958800315857\n",
      "epoch: 30, batch: 42, loss: 1.5766077041625977\n",
      "epoch: 30, batch: 43, loss: 1.6406946182250977\n",
      "epoch: 30, batch: 44, loss: 1.6414306163787842\n",
      "epoch: 30, batch: 45, loss: 1.324681043624878\n",
      "epoch: 30, batch: 46, loss: 1.3635201454162598\n",
      "epoch: 30, batch: 47, loss: 1.8588008880615234\n",
      "epoch: 30, batch: 48, loss: 1.6087687015533447\n",
      "epoch: 30, batch: 49, loss: 1.357587218284607\n",
      "epoch: 30, batch: 50, loss: 1.3622877597808838\n",
      "epoch: 30, batch: 51, loss: 1.6496670246124268\n",
      "epoch: 30, batch: 52, loss: 1.587907075881958\n",
      "epoch: 30, batch: 53, loss: 1.6344362497329712\n",
      "epoch: 30, batch: 54, loss: 1.3609504699707031\n",
      "epoch: 30, batch: 55, loss: 1.429690957069397\n",
      "epoch: 30, batch: 56, loss: 1.8420522212982178\n",
      "epoch: 30, batch: 57, loss: 1.7302278280258179\n",
      "epoch: 30, batch: 58, loss: 1.2024258375167847\n",
      "epoch: 30, batch: 59, loss: 1.5118366479873657\n",
      "epoch: 30, batch: 60, loss: 1.4773184061050415\n",
      "epoch: 30, batch: 61, loss: 1.7900307178497314\n",
      "epoch: 30, batch: 62, loss: 1.4964056015014648\n",
      "epoch: 30, batch: 63, loss: 1.808373212814331\n",
      "epoch: 30, batch: 64, loss: 1.3372395038604736\n",
      "epoch: 30, batch: 65, loss: 1.7231992483139038\n",
      "epoch: 30, batch: 66, loss: 1.8520587682724\n",
      "epoch: 30, batch: 67, loss: 1.5952297449111938\n",
      "epoch: 30, batch: 68, loss: 1.692160964012146\n",
      "epoch: 30, batch: 69, loss: 1.8370082378387451\n",
      "epoch: 30, batch: 70, loss: 1.611499547958374\n",
      "epoch: 30, batch: 71, loss: 1.6706123352050781\n",
      "epoch: 30, batch: 72, loss: 1.667104721069336\n",
      "epoch: 30, batch: 73, loss: 1.5419437885284424\n",
      "epoch: 30, batch: 74, loss: 1.4310939311981201\n",
      "epoch: 30, batch: 75, loss: 1.7080062627792358\n",
      "epoch: 30, batch: 76, loss: 1.2649613618850708\n",
      "epoch: 30, batch: 77, loss: 1.219918131828308\n",
      "epoch: 30, batch: 78, loss: 1.403611183166504\n",
      "epoch: 30, batch: 79, loss: 1.5914053916931152\n",
      "epoch: 30, batch: 80, loss: 1.4731032848358154\n",
      "epoch: 30, batch: 81, loss: 1.4665555953979492\n",
      "epoch: 30, batch: 82, loss: 1.3792681694030762\n",
      "epoch: 30, batch: 83, loss: 1.702254056930542\n",
      "epoch: 30, batch: 84, loss: 1.2073705196380615\n",
      "epoch: 30, batch: 85, loss: 1.3524843454360962\n",
      "epoch: 30, batch: 86, loss: 1.688166618347168\n",
      "epoch: 30, batch: 87, loss: 1.4109004735946655\n",
      "epoch: 30, batch: 88, loss: 1.6141124963760376\n",
      "epoch: 30, batch: 89, loss: 1.678945541381836\n",
      "epoch: 30, batch: 90, loss: 1.1854974031448364\n",
      "epoch: 30, batch: 91, loss: 1.654496192932129\n",
      "epoch: 30, batch: 92, loss: 1.8570127487182617\n",
      "epoch: 30, batch: 93, loss: 1.7472862005233765\n",
      "epoch: 30, batch: 94, loss: 1.638005018234253\n",
      "epoch: 30, batch: 95, loss: 1.7583059072494507\n",
      "epoch: 30, batch: 96, loss: 1.4858372211456299\n",
      "epoch: 30, batch: 97, loss: 1.3324998617172241\n",
      "epoch: 30, batch: 98, loss: 1.4531333446502686\n",
      "epoch: 30, batch: 99, loss: 1.7341153621673584\n",
      "epoch: 30, batch: 100, loss: 1.3795615434646606\n",
      "epoch: 30, batch: 101, loss: 1.6748613119125366\n",
      "epoch: 30, batch: 102, loss: 1.3910515308380127\n",
      "epoch: 30, batch: 103, loss: 1.3226922750473022\n",
      "epoch: 30, batch: 104, loss: 1.7115741968154907\n",
      "epoch: 30, batch: 105, loss: 1.5915271043777466\n",
      "epoch: 30, batch: 106, loss: 1.4454824924468994\n",
      "epoch: 30, batch: 107, loss: 1.3478984832763672\n",
      "epoch: 30, batch: 108, loss: 1.7974302768707275\n",
      "epoch: 30, batch: 109, loss: 1.7880929708480835\n",
      "epoch: 30, batch: 110, loss: 1.5934127569198608\n",
      "epoch: 30, batch: 111, loss: 1.654131293296814\n",
      "epoch: 30, batch: 112, loss: 1.6330721378326416\n",
      "epoch: 30, batch: 113, loss: 1.6436859369277954\n",
      "epoch: 30, batch: 114, loss: 1.8679660558700562\n",
      "epoch: 30, batch: 115, loss: 1.4310842752456665\n",
      "epoch: 30, batch: 116, loss: 1.7222025394439697\n",
      "epoch: 30, batch: 117, loss: 1.4878343343734741\n",
      "epoch: 30, batch: 118, loss: 1.5306370258331299\n",
      "epoch: 30, batch: 119, loss: 1.6787757873535156\n",
      "epoch: 30, batch: 120, loss: 1.6711689233779907\n",
      "epoch: 30, batch: 121, loss: 1.6425445079803467\n",
      "epoch: 30, batch: 122, loss: 1.249174952507019\n",
      "epoch: 30, batch: 123, loss: 1.4727152585983276\n",
      "epoch: 30, batch: 124, loss: 1.5951780080795288\n",
      "epoch: 30, batch: 125, loss: 1.4700117111206055\n",
      "epoch: 30, batch: 126, loss: 1.3987627029418945\n",
      "epoch: 30, batch: 127, loss: 1.7468116283416748\n",
      "epoch: 31, batch: 0, loss: 1.5702412128448486\n",
      "epoch: 31, batch: 1, loss: 1.4326984882354736\n",
      "epoch: 31, batch: 2, loss: 1.6625829935073853\n",
      "epoch: 31, batch: 3, loss: 1.4195992946624756\n",
      "epoch: 31, batch: 4, loss: 1.467734932899475\n",
      "epoch: 31, batch: 5, loss: 1.4554277658462524\n",
      "epoch: 31, batch: 6, loss: 1.325761079788208\n",
      "epoch: 31, batch: 7, loss: 1.7497106790542603\n",
      "epoch: 31, batch: 8, loss: 1.3773138523101807\n",
      "epoch: 31, batch: 9, loss: 1.7608468532562256\n",
      "epoch: 31, batch: 10, loss: 1.6020605564117432\n",
      "epoch: 31, batch: 11, loss: 1.2167972326278687\n",
      "epoch: 31, batch: 12, loss: 1.2678656578063965\n",
      "epoch: 31, batch: 13, loss: 1.4905927181243896\n",
      "epoch: 31, batch: 14, loss: 1.3557559251785278\n",
      "epoch: 31, batch: 15, loss: 1.6817941665649414\n",
      "epoch: 31, batch: 16, loss: 1.6219215393066406\n",
      "epoch: 31, batch: 17, loss: 1.6796300411224365\n",
      "epoch: 31, batch: 18, loss: 1.529309868812561\n",
      "epoch: 31, batch: 19, loss: 1.4886865615844727\n",
      "epoch: 31, batch: 20, loss: 1.721854567527771\n",
      "epoch: 31, batch: 21, loss: 1.6251628398895264\n",
      "epoch: 31, batch: 22, loss: 1.6081708669662476\n",
      "epoch: 31, batch: 23, loss: 1.5718990564346313\n",
      "epoch: 31, batch: 24, loss: 1.8205430507659912\n",
      "epoch: 31, batch: 25, loss: 1.2326886653900146\n",
      "epoch: 31, batch: 26, loss: 1.1515676975250244\n",
      "epoch: 31, batch: 27, loss: 1.8265947103500366\n",
      "epoch: 31, batch: 28, loss: 1.4975183010101318\n",
      "epoch: 31, batch: 29, loss: 1.2428596019744873\n",
      "epoch: 31, batch: 30, loss: 1.1393076181411743\n",
      "epoch: 31, batch: 31, loss: 1.4532191753387451\n",
      "epoch: 31, batch: 32, loss: 1.5535147190093994\n",
      "epoch: 31, batch: 33, loss: 1.5597822666168213\n",
      "epoch: 31, batch: 34, loss: 1.6491559743881226\n",
      "epoch: 31, batch: 35, loss: 1.674788236618042\n",
      "epoch: 31, batch: 36, loss: 1.73151433467865\n",
      "epoch: 31, batch: 37, loss: 1.5784177780151367\n",
      "epoch: 31, batch: 38, loss: 1.547055721282959\n",
      "epoch: 31, batch: 39, loss: 1.6848704814910889\n",
      "epoch: 31, batch: 40, loss: 1.471400260925293\n",
      "epoch: 31, batch: 41, loss: 1.7069814205169678\n",
      "epoch: 31, batch: 42, loss: 1.2645015716552734\n",
      "epoch: 31, batch: 43, loss: 1.5423461198806763\n",
      "epoch: 31, batch: 44, loss: 1.6340233087539673\n",
      "epoch: 31, batch: 45, loss: 1.3659836053848267\n",
      "epoch: 31, batch: 46, loss: 1.6027568578720093\n",
      "epoch: 31, batch: 47, loss: 1.5347801446914673\n",
      "epoch: 31, batch: 48, loss: 1.3703607320785522\n",
      "epoch: 31, batch: 49, loss: 1.289208173751831\n",
      "epoch: 31, batch: 50, loss: 1.367514729499817\n",
      "epoch: 31, batch: 51, loss: 1.188461184501648\n",
      "epoch: 31, batch: 52, loss: 1.5726722478866577\n",
      "epoch: 31, batch: 53, loss: 1.6259591579437256\n",
      "epoch: 31, batch: 54, loss: 1.4709199666976929\n",
      "epoch: 31, batch: 55, loss: 1.3202743530273438\n",
      "epoch: 31, batch: 56, loss: 1.1997807025909424\n",
      "epoch: 31, batch: 57, loss: 1.43517005443573\n",
      "epoch: 31, batch: 58, loss: 1.654298186302185\n",
      "epoch: 31, batch: 59, loss: 1.5932930707931519\n",
      "epoch: 31, batch: 60, loss: 1.336545467376709\n",
      "epoch: 31, batch: 61, loss: 1.6213098764419556\n",
      "epoch: 31, batch: 62, loss: 1.562058687210083\n",
      "epoch: 31, batch: 63, loss: 1.5374466180801392\n",
      "epoch: 31, batch: 64, loss: 1.245811939239502\n",
      "epoch: 31, batch: 65, loss: 1.639970064163208\n",
      "epoch: 31, batch: 66, loss: 1.3366241455078125\n",
      "epoch: 31, batch: 67, loss: 1.5426665544509888\n",
      "epoch: 31, batch: 68, loss: 1.653609275817871\n",
      "epoch: 31, batch: 69, loss: 1.6208184957504272\n",
      "epoch: 31, batch: 70, loss: 1.5658961534500122\n",
      "epoch: 31, batch: 71, loss: 1.6858656406402588\n",
      "epoch: 31, batch: 72, loss: 1.793952226638794\n",
      "epoch: 31, batch: 73, loss: 1.4699515104293823\n",
      "epoch: 31, batch: 74, loss: 1.8124616146087646\n",
      "epoch: 31, batch: 75, loss: 1.961517095565796\n",
      "epoch: 31, batch: 76, loss: 1.6342366933822632\n",
      "epoch: 31, batch: 77, loss: 1.5622951984405518\n",
      "epoch: 31, batch: 78, loss: 1.458452820777893\n",
      "epoch: 31, batch: 79, loss: 1.587965965270996\n",
      "epoch: 31, batch: 80, loss: 1.4490172863006592\n",
      "epoch: 31, batch: 81, loss: 1.5819331407546997\n",
      "epoch: 31, batch: 82, loss: 1.5097472667694092\n",
      "epoch: 31, batch: 83, loss: 1.4928518533706665\n",
      "epoch: 31, batch: 84, loss: 1.7475820779800415\n",
      "epoch: 31, batch: 85, loss: 1.0270601511001587\n",
      "epoch: 31, batch: 86, loss: 1.4378342628479004\n",
      "epoch: 31, batch: 87, loss: 1.5327364206314087\n",
      "epoch: 31, batch: 88, loss: 1.6241191625595093\n",
      "epoch: 31, batch: 89, loss: 1.3879919052124023\n",
      "epoch: 31, batch: 90, loss: 1.4435741901397705\n",
      "epoch: 31, batch: 91, loss: 1.4482765197753906\n",
      "epoch: 31, batch: 92, loss: 1.594260811805725\n",
      "epoch: 31, batch: 93, loss: 1.7237083911895752\n",
      "epoch: 31, batch: 94, loss: 1.333034634590149\n",
      "epoch: 31, batch: 95, loss: 1.6059916019439697\n",
      "epoch: 31, batch: 96, loss: 1.5628820657730103\n",
      "epoch: 31, batch: 97, loss: 1.7552753686904907\n",
      "epoch: 31, batch: 98, loss: 1.5619449615478516\n",
      "epoch: 31, batch: 99, loss: 1.3432360887527466\n",
      "epoch: 31, batch: 100, loss: 1.6004655361175537\n",
      "epoch: 31, batch: 101, loss: 1.2567397356033325\n",
      "epoch: 31, batch: 102, loss: 1.695001244544983\n",
      "epoch: 31, batch: 103, loss: 1.490984559059143\n",
      "epoch: 31, batch: 104, loss: 1.518584966659546\n",
      "epoch: 31, batch: 105, loss: 1.5825785398483276\n",
      "epoch: 31, batch: 106, loss: 1.6367671489715576\n",
      "epoch: 31, batch: 107, loss: 1.3972711563110352\n",
      "epoch: 31, batch: 108, loss: 1.4956799745559692\n",
      "epoch: 31, batch: 109, loss: 1.6474931240081787\n",
      "epoch: 31, batch: 110, loss: 1.5630080699920654\n",
      "epoch: 31, batch: 111, loss: 1.5624033212661743\n",
      "epoch: 31, batch: 112, loss: 1.597554087638855\n",
      "epoch: 31, batch: 113, loss: 1.5094530582427979\n",
      "epoch: 31, batch: 114, loss: 1.6385844945907593\n",
      "epoch: 31, batch: 115, loss: 1.4081981182098389\n",
      "epoch: 31, batch: 116, loss: 1.7049305438995361\n",
      "epoch: 31, batch: 117, loss: 1.3581308126449585\n",
      "epoch: 31, batch: 118, loss: 1.440294861793518\n",
      "epoch: 31, batch: 119, loss: 1.4354339838027954\n",
      "epoch: 31, batch: 120, loss: 1.5055568218231201\n",
      "epoch: 31, batch: 121, loss: 1.8953564167022705\n",
      "epoch: 31, batch: 122, loss: 1.6523326635360718\n",
      "epoch: 31, batch: 123, loss: 1.7672841548919678\n",
      "epoch: 31, batch: 124, loss: 1.3195586204528809\n",
      "epoch: 31, batch: 125, loss: 1.401017189025879\n",
      "epoch: 31, batch: 126, loss: 1.320923089981079\n",
      "epoch: 31, batch: 127, loss: 1.4787319898605347\n",
      "epoch: 32, batch: 0, loss: 1.6208226680755615\n",
      "epoch: 32, batch: 1, loss: 1.4120051860809326\n",
      "epoch: 32, batch: 2, loss: 1.4250056743621826\n",
      "epoch: 32, batch: 3, loss: 1.5506024360656738\n",
      "epoch: 32, batch: 4, loss: 1.6218410730361938\n",
      "epoch: 32, batch: 5, loss: 1.7659763097763062\n",
      "epoch: 32, batch: 6, loss: 1.464522361755371\n",
      "epoch: 32, batch: 7, loss: 1.2880386114120483\n",
      "epoch: 32, batch: 8, loss: 1.688857078552246\n",
      "epoch: 32, batch: 9, loss: 1.5339518785476685\n",
      "epoch: 32, batch: 10, loss: 1.3873207569122314\n",
      "epoch: 32, batch: 11, loss: 1.4725375175476074\n",
      "epoch: 32, batch: 12, loss: 1.452009916305542\n",
      "epoch: 32, batch: 13, loss: 1.6149370670318604\n",
      "epoch: 32, batch: 14, loss: 1.2639926671981812\n",
      "epoch: 32, batch: 15, loss: 1.5654939413070679\n",
      "epoch: 32, batch: 16, loss: 1.4568023681640625\n",
      "epoch: 32, batch: 17, loss: 1.6428197622299194\n",
      "epoch: 32, batch: 18, loss: 1.6685218811035156\n",
      "epoch: 32, batch: 19, loss: 1.5202399492263794\n",
      "epoch: 32, batch: 20, loss: 1.4780771732330322\n",
      "epoch: 32, batch: 21, loss: 1.3027417659759521\n",
      "epoch: 32, batch: 22, loss: 1.2353006601333618\n",
      "epoch: 32, batch: 23, loss: 1.4514869451522827\n",
      "epoch: 32, batch: 24, loss: 1.580851435661316\n",
      "epoch: 32, batch: 25, loss: 1.393225908279419\n",
      "epoch: 32, batch: 26, loss: 1.6079658269882202\n",
      "epoch: 32, batch: 27, loss: 1.2018568515777588\n",
      "epoch: 32, batch: 28, loss: 1.6923774480819702\n",
      "epoch: 32, batch: 29, loss: 1.5476770401000977\n",
      "epoch: 32, batch: 30, loss: 1.447522759437561\n",
      "epoch: 32, batch: 31, loss: 1.8217966556549072\n",
      "epoch: 32, batch: 32, loss: 1.506293535232544\n",
      "epoch: 32, batch: 33, loss: 1.5989927053451538\n",
      "epoch: 32, batch: 34, loss: 1.6293748617172241\n",
      "epoch: 32, batch: 35, loss: 1.2433240413665771\n",
      "epoch: 32, batch: 36, loss: 1.541304588317871\n",
      "epoch: 32, batch: 37, loss: 1.621734619140625\n",
      "epoch: 32, batch: 38, loss: 1.5516589879989624\n",
      "epoch: 32, batch: 39, loss: 1.388200044631958\n",
      "epoch: 32, batch: 40, loss: 1.2924773693084717\n",
      "epoch: 32, batch: 41, loss: 1.7418218851089478\n",
      "epoch: 32, batch: 42, loss: 1.51438307762146\n",
      "epoch: 32, batch: 43, loss: 1.4644603729248047\n",
      "epoch: 32, batch: 44, loss: 1.7063407897949219\n",
      "epoch: 32, batch: 45, loss: 1.4355250597000122\n",
      "epoch: 32, batch: 46, loss: 1.9843412637710571\n",
      "epoch: 32, batch: 47, loss: 1.3536853790283203\n",
      "epoch: 32, batch: 48, loss: 1.2414700984954834\n",
      "epoch: 32, batch: 49, loss: 1.5791517496109009\n",
      "epoch: 32, batch: 50, loss: 1.3654056787490845\n",
      "epoch: 32, batch: 51, loss: 1.4744758605957031\n",
      "epoch: 32, batch: 52, loss: 1.7240028381347656\n",
      "epoch: 32, batch: 53, loss: 1.2176082134246826\n",
      "epoch: 32, batch: 54, loss: 1.6484636068344116\n",
      "epoch: 32, batch: 55, loss: 1.212409257888794\n",
      "epoch: 32, batch: 56, loss: 1.6329052448272705\n",
      "epoch: 32, batch: 57, loss: 1.7737737894058228\n",
      "epoch: 32, batch: 58, loss: 1.446922779083252\n",
      "epoch: 32, batch: 59, loss: 1.9063174724578857\n",
      "epoch: 32, batch: 60, loss: 1.5593189001083374\n",
      "epoch: 32, batch: 61, loss: 1.3311607837677002\n",
      "epoch: 32, batch: 62, loss: 1.5974887609481812\n",
      "epoch: 32, batch: 63, loss: 1.4344186782836914\n",
      "epoch: 32, batch: 64, loss: 1.5280050039291382\n",
      "epoch: 32, batch: 65, loss: 1.928643822669983\n",
      "epoch: 32, batch: 66, loss: 1.5468292236328125\n",
      "epoch: 32, batch: 67, loss: 1.8102540969848633\n",
      "epoch: 32, batch: 68, loss: 1.3237392902374268\n",
      "epoch: 32, batch: 69, loss: 1.625074028968811\n",
      "epoch: 32, batch: 70, loss: 1.187909483909607\n",
      "epoch: 32, batch: 71, loss: 1.482055902481079\n",
      "epoch: 32, batch: 72, loss: 1.5942952632904053\n",
      "epoch: 32, batch: 73, loss: 1.4120380878448486\n",
      "epoch: 32, batch: 74, loss: 1.560950517654419\n",
      "epoch: 32, batch: 75, loss: 1.7102292776107788\n",
      "epoch: 32, batch: 76, loss: 1.7431989908218384\n",
      "epoch: 32, batch: 77, loss: 1.6301177740097046\n",
      "epoch: 32, batch: 78, loss: 1.695959448814392\n",
      "epoch: 32, batch: 79, loss: 1.5412111282348633\n",
      "epoch: 32, batch: 80, loss: 1.7060225009918213\n",
      "epoch: 32, batch: 81, loss: 1.2718688249588013\n",
      "epoch: 32, batch: 82, loss: 1.4126455783843994\n",
      "epoch: 32, batch: 83, loss: 1.4137481451034546\n",
      "epoch: 32, batch: 84, loss: 1.5378501415252686\n",
      "epoch: 32, batch: 85, loss: 1.5099575519561768\n",
      "epoch: 32, batch: 86, loss: 1.6243808269500732\n",
      "epoch: 32, batch: 87, loss: 1.7611042261123657\n",
      "epoch: 32, batch: 88, loss: 1.544836163520813\n",
      "epoch: 32, batch: 89, loss: 1.4008827209472656\n",
      "epoch: 32, batch: 90, loss: 1.382684350013733\n",
      "epoch: 32, batch: 91, loss: 1.386008381843567\n",
      "epoch: 32, batch: 92, loss: 1.6288923025131226\n",
      "epoch: 32, batch: 93, loss: 1.4697540998458862\n",
      "epoch: 32, batch: 94, loss: 1.3616974353790283\n",
      "epoch: 32, batch: 95, loss: 1.6785316467285156\n",
      "epoch: 32, batch: 96, loss: 1.3561261892318726\n",
      "epoch: 32, batch: 97, loss: 1.4426195621490479\n",
      "epoch: 32, batch: 98, loss: 1.6949758529663086\n",
      "epoch: 32, batch: 99, loss: 1.4634679555892944\n",
      "epoch: 32, batch: 100, loss: 1.4513633251190186\n",
      "epoch: 32, batch: 101, loss: 1.3639802932739258\n",
      "epoch: 32, batch: 102, loss: 1.6575276851654053\n",
      "epoch: 32, batch: 103, loss: 1.5113781690597534\n",
      "epoch: 32, batch: 104, loss: 1.6835991144180298\n",
      "epoch: 32, batch: 105, loss: 1.414306402206421\n",
      "epoch: 32, batch: 106, loss: 1.61519455909729\n",
      "epoch: 32, batch: 107, loss: 1.321317434310913\n",
      "epoch: 32, batch: 108, loss: 1.3556327819824219\n",
      "epoch: 32, batch: 109, loss: 1.4261354207992554\n",
      "epoch: 32, batch: 110, loss: 1.735818862915039\n",
      "epoch: 32, batch: 111, loss: 1.5952608585357666\n",
      "epoch: 32, batch: 112, loss: 1.7242904901504517\n",
      "epoch: 32, batch: 113, loss: 1.5568488836288452\n",
      "epoch: 32, batch: 114, loss: 1.6389381885528564\n",
      "epoch: 32, batch: 115, loss: 1.451399803161621\n",
      "epoch: 32, batch: 116, loss: 1.4997221231460571\n",
      "epoch: 32, batch: 117, loss: 1.8185456991195679\n",
      "epoch: 32, batch: 118, loss: 1.6734974384307861\n",
      "epoch: 32, batch: 119, loss: 1.5063481330871582\n",
      "epoch: 32, batch: 120, loss: 1.5458570718765259\n",
      "epoch: 32, batch: 121, loss: 1.800005555152893\n",
      "epoch: 32, batch: 122, loss: 1.5434191226959229\n",
      "epoch: 32, batch: 123, loss: 1.6090314388275146\n",
      "epoch: 32, batch: 124, loss: 1.2547262907028198\n",
      "epoch: 32, batch: 125, loss: 1.2614129781723022\n",
      "epoch: 32, batch: 126, loss: 1.4887644052505493\n",
      "epoch: 32, batch: 127, loss: 1.1779725551605225\n",
      "epoch: 33, batch: 0, loss: 1.5056116580963135\n",
      "epoch: 33, batch: 1, loss: 1.34416663646698\n",
      "epoch: 33, batch: 2, loss: 1.8078851699829102\n",
      "epoch: 33, batch: 3, loss: 1.3710722923278809\n",
      "epoch: 33, batch: 4, loss: 1.155193567276001\n",
      "epoch: 33, batch: 5, loss: 1.5557256937026978\n",
      "epoch: 33, batch: 6, loss: 1.450425386428833\n",
      "epoch: 33, batch: 7, loss: 1.454128623008728\n",
      "epoch: 33, batch: 8, loss: 1.494692087173462\n",
      "epoch: 33, batch: 9, loss: 1.4381680488586426\n",
      "epoch: 33, batch: 10, loss: 1.485634207725525\n",
      "epoch: 33, batch: 11, loss: 1.3757222890853882\n",
      "epoch: 33, batch: 12, loss: 1.569427728652954\n",
      "epoch: 33, batch: 13, loss: 1.5678367614746094\n",
      "epoch: 33, batch: 14, loss: 1.425328016281128\n",
      "epoch: 33, batch: 15, loss: 1.158389687538147\n",
      "epoch: 33, batch: 16, loss: 1.3666636943817139\n",
      "epoch: 33, batch: 17, loss: 1.693213701248169\n",
      "epoch: 33, batch: 18, loss: 1.6293665170669556\n",
      "epoch: 33, batch: 19, loss: 1.8518474102020264\n",
      "epoch: 33, batch: 20, loss: 1.5129272937774658\n",
      "epoch: 33, batch: 21, loss: 1.5688667297363281\n",
      "epoch: 33, batch: 22, loss: 1.491370677947998\n",
      "epoch: 33, batch: 23, loss: 1.3235342502593994\n",
      "epoch: 33, batch: 24, loss: 1.2557299137115479\n",
      "epoch: 33, batch: 25, loss: 1.589812994003296\n",
      "epoch: 33, batch: 26, loss: 1.2205408811569214\n",
      "epoch: 33, batch: 27, loss: 1.2424495220184326\n",
      "epoch: 33, batch: 28, loss: 1.6645734310150146\n",
      "epoch: 33, batch: 29, loss: 1.8046423196792603\n",
      "epoch: 33, batch: 30, loss: 1.3856351375579834\n",
      "epoch: 33, batch: 31, loss: 1.6394259929656982\n",
      "epoch: 33, batch: 32, loss: 1.5469785928726196\n",
      "epoch: 33, batch: 33, loss: 1.748438835144043\n",
      "epoch: 33, batch: 34, loss: 1.39113187789917\n",
      "epoch: 33, batch: 35, loss: 1.4678337574005127\n",
      "epoch: 33, batch: 36, loss: 1.7150108814239502\n",
      "epoch: 33, batch: 37, loss: 1.3285942077636719\n",
      "epoch: 33, batch: 38, loss: 1.1382073163986206\n",
      "epoch: 33, batch: 39, loss: 1.7312467098236084\n",
      "epoch: 33, batch: 40, loss: 1.5148481130599976\n",
      "epoch: 33, batch: 41, loss: 1.3747401237487793\n",
      "epoch: 33, batch: 42, loss: 1.675553560256958\n",
      "epoch: 33, batch: 43, loss: 1.6532920598983765\n",
      "epoch: 33, batch: 44, loss: 1.6414251327514648\n",
      "epoch: 33, batch: 45, loss: 1.7492501735687256\n",
      "epoch: 33, batch: 46, loss: 1.7701116800308228\n",
      "epoch: 33, batch: 47, loss: 1.8713445663452148\n",
      "epoch: 33, batch: 48, loss: 1.5278291702270508\n",
      "epoch: 33, batch: 49, loss: 1.212614893913269\n",
      "epoch: 33, batch: 50, loss: 1.601445198059082\n",
      "epoch: 33, batch: 51, loss: 1.3083757162094116\n",
      "epoch: 33, batch: 52, loss: 1.420727252960205\n",
      "epoch: 33, batch: 53, loss: 1.504276990890503\n",
      "epoch: 33, batch: 54, loss: 1.5290679931640625\n",
      "epoch: 33, batch: 55, loss: 1.171663522720337\n",
      "epoch: 33, batch: 56, loss: 1.4250510931015015\n",
      "epoch: 33, batch: 57, loss: 1.5315093994140625\n",
      "epoch: 33, batch: 58, loss: 1.7301439046859741\n",
      "epoch: 33, batch: 59, loss: 1.5944349765777588\n",
      "epoch: 33, batch: 60, loss: 1.2995110750198364\n",
      "epoch: 33, batch: 61, loss: 1.6988096237182617\n",
      "epoch: 33, batch: 62, loss: 1.463672161102295\n",
      "epoch: 33, batch: 63, loss: 1.6903657913208008\n",
      "epoch: 33, batch: 64, loss: 1.5745104551315308\n",
      "epoch: 33, batch: 65, loss: 1.4652726650238037\n",
      "epoch: 33, batch: 66, loss: 1.6665042638778687\n",
      "epoch: 33, batch: 67, loss: 1.7143218517303467\n",
      "epoch: 33, batch: 68, loss: 1.475629210472107\n",
      "epoch: 33, batch: 69, loss: 1.4520012140274048\n",
      "epoch: 33, batch: 70, loss: 1.4275217056274414\n",
      "epoch: 33, batch: 71, loss: 1.7123388051986694\n",
      "epoch: 33, batch: 72, loss: 1.563206434249878\n",
      "epoch: 33, batch: 73, loss: 1.7409687042236328\n",
      "epoch: 33, batch: 74, loss: 1.5064414739608765\n",
      "epoch: 33, batch: 75, loss: 1.7430318593978882\n",
      "epoch: 33, batch: 76, loss: 1.5952011346817017\n",
      "epoch: 33, batch: 77, loss: 1.2494890689849854\n",
      "epoch: 33, batch: 78, loss: 1.4230105876922607\n",
      "epoch: 33, batch: 79, loss: 1.5597015619277954\n",
      "epoch: 33, batch: 80, loss: 1.3165953159332275\n",
      "epoch: 33, batch: 81, loss: 1.3800346851348877\n",
      "epoch: 33, batch: 82, loss: 1.505247712135315\n",
      "epoch: 33, batch: 83, loss: 1.6619526147842407\n",
      "epoch: 33, batch: 84, loss: 1.575358271598816\n",
      "epoch: 33, batch: 85, loss: 1.7046654224395752\n",
      "epoch: 33, batch: 86, loss: 1.468178153038025\n",
      "epoch: 33, batch: 87, loss: 1.2258652448654175\n",
      "epoch: 33, batch: 88, loss: 1.4703296422958374\n",
      "epoch: 33, batch: 89, loss: 1.8093757629394531\n",
      "epoch: 33, batch: 90, loss: 1.416045904159546\n",
      "epoch: 33, batch: 91, loss: 1.5854809284210205\n",
      "epoch: 33, batch: 92, loss: 1.590261697769165\n",
      "epoch: 33, batch: 93, loss: 1.573986291885376\n",
      "epoch: 33, batch: 94, loss: 1.3891462087631226\n",
      "epoch: 33, batch: 95, loss: 1.5056207180023193\n",
      "epoch: 33, batch: 96, loss: 1.1349363327026367\n",
      "epoch: 33, batch: 97, loss: 1.5428745746612549\n",
      "epoch: 33, batch: 98, loss: 1.6127421855926514\n",
      "epoch: 33, batch: 99, loss: 1.6143426895141602\n",
      "epoch: 33, batch: 100, loss: 1.257153034210205\n",
      "epoch: 33, batch: 101, loss: 1.6733653545379639\n",
      "epoch: 33, batch: 102, loss: 1.551152229309082\n",
      "epoch: 33, batch: 103, loss: 1.2823965549468994\n",
      "epoch: 33, batch: 104, loss: 1.4852285385131836\n",
      "epoch: 33, batch: 105, loss: 1.6075830459594727\n",
      "epoch: 33, batch: 106, loss: 1.6474618911743164\n",
      "epoch: 33, batch: 107, loss: 1.7376253604888916\n",
      "epoch: 33, batch: 108, loss: 1.6224429607391357\n",
      "epoch: 33, batch: 109, loss: 1.7717088460922241\n",
      "epoch: 33, batch: 110, loss: 1.4973766803741455\n",
      "epoch: 33, batch: 111, loss: 1.5249073505401611\n",
      "epoch: 33, batch: 112, loss: 1.7945085763931274\n",
      "epoch: 33, batch: 113, loss: 1.6859900951385498\n",
      "epoch: 33, batch: 114, loss: 1.6262699365615845\n",
      "epoch: 33, batch: 115, loss: 1.7732322216033936\n",
      "epoch: 33, batch: 116, loss: 1.4679523706436157\n",
      "epoch: 33, batch: 117, loss: 1.7271087169647217\n",
      "epoch: 33, batch: 118, loss: 1.6497493982315063\n",
      "epoch: 33, batch: 119, loss: 1.5340055227279663\n",
      "epoch: 33, batch: 120, loss: 1.4245960712432861\n",
      "epoch: 33, batch: 121, loss: 1.7413488626480103\n",
      "epoch: 33, batch: 122, loss: 1.2702677249908447\n",
      "epoch: 33, batch: 123, loss: 1.8906440734863281\n",
      "epoch: 33, batch: 124, loss: 1.6166770458221436\n",
      "epoch: 33, batch: 125, loss: 1.8176867961883545\n",
      "epoch: 33, batch: 126, loss: 1.56153404712677\n",
      "epoch: 33, batch: 127, loss: 1.551915168762207\n",
      "epoch: 34, batch: 0, loss: 1.3767778873443604\n",
      "epoch: 34, batch: 1, loss: 1.657824158668518\n",
      "epoch: 34, batch: 2, loss: 1.3802897930145264\n",
      "epoch: 34, batch: 3, loss: 1.618190050125122\n",
      "epoch: 34, batch: 4, loss: 1.3853265047073364\n",
      "epoch: 34, batch: 5, loss: 1.5008533000946045\n",
      "epoch: 34, batch: 6, loss: 1.3450008630752563\n",
      "epoch: 34, batch: 7, loss: 1.7450920343399048\n",
      "epoch: 34, batch: 8, loss: 1.8031810522079468\n",
      "epoch: 34, batch: 9, loss: 1.622759222984314\n",
      "epoch: 34, batch: 10, loss: 1.6237891912460327\n",
      "epoch: 34, batch: 11, loss: 1.5290582180023193\n",
      "epoch: 34, batch: 12, loss: 1.5482332706451416\n",
      "epoch: 34, batch: 13, loss: 1.388749361038208\n",
      "epoch: 34, batch: 14, loss: 1.4976261854171753\n",
      "epoch: 34, batch: 15, loss: 1.457319974899292\n",
      "epoch: 34, batch: 16, loss: 1.7505779266357422\n",
      "epoch: 34, batch: 17, loss: 1.3698657751083374\n",
      "epoch: 34, batch: 18, loss: 1.7531620264053345\n",
      "epoch: 34, batch: 19, loss: 1.5116816759109497\n",
      "epoch: 34, batch: 20, loss: 1.7942253351211548\n",
      "epoch: 34, batch: 21, loss: 1.5396246910095215\n",
      "epoch: 34, batch: 22, loss: 1.3272862434387207\n",
      "epoch: 34, batch: 23, loss: 1.3749405145645142\n",
      "epoch: 34, batch: 24, loss: 1.7052104473114014\n",
      "epoch: 34, batch: 25, loss: 1.5962116718292236\n",
      "epoch: 34, batch: 26, loss: 1.5889077186584473\n",
      "epoch: 34, batch: 27, loss: 1.8268951177597046\n",
      "epoch: 34, batch: 28, loss: 1.2780344486236572\n",
      "epoch: 34, batch: 29, loss: 1.5543988943099976\n",
      "epoch: 34, batch: 30, loss: 1.2812316417694092\n",
      "epoch: 34, batch: 31, loss: 1.3466287851333618\n",
      "epoch: 34, batch: 32, loss: 1.560974359512329\n",
      "epoch: 34, batch: 33, loss: 1.2823563814163208\n",
      "epoch: 34, batch: 34, loss: 1.538699746131897\n",
      "epoch: 34, batch: 35, loss: 1.621955156326294\n",
      "epoch: 34, batch: 36, loss: 1.4227066040039062\n",
      "epoch: 34, batch: 37, loss: 1.6865745782852173\n",
      "epoch: 34, batch: 38, loss: 1.3023765087127686\n",
      "epoch: 34, batch: 39, loss: 1.4656890630722046\n",
      "epoch: 34, batch: 40, loss: 1.1758496761322021\n",
      "epoch: 34, batch: 41, loss: 1.3147578239440918\n",
      "epoch: 34, batch: 42, loss: 1.8594270944595337\n",
      "epoch: 34, batch: 43, loss: 1.3461450338363647\n",
      "epoch: 34, batch: 44, loss: 1.8299967050552368\n",
      "epoch: 34, batch: 45, loss: 1.438114047050476\n",
      "epoch: 34, batch: 46, loss: 1.519906997680664\n",
      "epoch: 34, batch: 47, loss: 1.635096549987793\n",
      "epoch: 34, batch: 48, loss: 1.6107743978500366\n",
      "epoch: 34, batch: 49, loss: 1.7022979259490967\n",
      "epoch: 34, batch: 50, loss: 1.743790626525879\n",
      "epoch: 34, batch: 51, loss: 1.778700590133667\n",
      "epoch: 34, batch: 52, loss: 1.6504290103912354\n",
      "epoch: 34, batch: 53, loss: 1.3648282289505005\n",
      "epoch: 34, batch: 54, loss: 1.8536784648895264\n",
      "epoch: 34, batch: 55, loss: 1.8472144603729248\n",
      "epoch: 34, batch: 56, loss: 1.446892499923706\n",
      "epoch: 34, batch: 57, loss: 1.6268770694732666\n",
      "epoch: 34, batch: 58, loss: 1.2347972393035889\n",
      "epoch: 34, batch: 59, loss: 1.6260490417480469\n",
      "epoch: 34, batch: 60, loss: 1.3237674236297607\n",
      "epoch: 34, batch: 61, loss: 1.5380560159683228\n",
      "epoch: 34, batch: 62, loss: 1.459646463394165\n",
      "epoch: 34, batch: 63, loss: 1.3163408041000366\n",
      "epoch: 34, batch: 64, loss: 1.6964261531829834\n",
      "epoch: 34, batch: 65, loss: 1.6263082027435303\n",
      "epoch: 34, batch: 66, loss: 1.708237648010254\n",
      "epoch: 34, batch: 67, loss: 1.603798508644104\n",
      "epoch: 34, batch: 68, loss: 1.3954721689224243\n",
      "epoch: 34, batch: 69, loss: 1.646864891052246\n",
      "epoch: 34, batch: 70, loss: 1.4759032726287842\n",
      "epoch: 34, batch: 71, loss: 1.4502670764923096\n",
      "epoch: 34, batch: 72, loss: 1.3952288627624512\n",
      "epoch: 34, batch: 73, loss: 1.633933424949646\n",
      "epoch: 34, batch: 74, loss: 1.5367759466171265\n",
      "epoch: 34, batch: 75, loss: 1.6608610153198242\n",
      "epoch: 34, batch: 76, loss: 1.3229482173919678\n",
      "epoch: 34, batch: 77, loss: 1.7525875568389893\n",
      "epoch: 34, batch: 78, loss: 1.5094149112701416\n",
      "epoch: 34, batch: 79, loss: 1.103536605834961\n",
      "epoch: 34, batch: 80, loss: 1.6071926355361938\n",
      "epoch: 34, batch: 81, loss: 1.6024783849716187\n",
      "epoch: 34, batch: 82, loss: 1.2890563011169434\n",
      "epoch: 34, batch: 83, loss: 1.4208195209503174\n",
      "epoch: 34, batch: 84, loss: 1.508289098739624\n",
      "epoch: 34, batch: 85, loss: 1.30232572555542\n",
      "epoch: 34, batch: 86, loss: 1.6694530248641968\n",
      "epoch: 34, batch: 87, loss: 1.5296456813812256\n",
      "epoch: 34, batch: 88, loss: 1.4158573150634766\n",
      "epoch: 34, batch: 89, loss: 1.4142893552780151\n",
      "epoch: 34, batch: 90, loss: 1.477319359779358\n",
      "epoch: 34, batch: 91, loss: 1.0847499370574951\n",
      "epoch: 34, batch: 92, loss: 1.505125880241394\n",
      "epoch: 34, batch: 93, loss: 1.4893758296966553\n",
      "epoch: 34, batch: 94, loss: 1.636983871459961\n",
      "epoch: 34, batch: 95, loss: 1.556145429611206\n",
      "epoch: 34, batch: 96, loss: 1.8057003021240234\n",
      "epoch: 34, batch: 97, loss: 1.4074043035507202\n",
      "epoch: 34, batch: 98, loss: 1.870539903640747\n",
      "epoch: 34, batch: 99, loss: 1.3645427227020264\n",
      "epoch: 34, batch: 100, loss: 1.5587295293807983\n",
      "epoch: 34, batch: 101, loss: 1.6546478271484375\n",
      "epoch: 34, batch: 102, loss: 1.3508899211883545\n",
      "epoch: 34, batch: 103, loss: 1.377532958984375\n",
      "epoch: 34, batch: 104, loss: 1.6929556131362915\n",
      "epoch: 34, batch: 105, loss: 1.227170705795288\n",
      "epoch: 34, batch: 106, loss: 1.6247453689575195\n",
      "epoch: 34, batch: 107, loss: 1.5011337995529175\n",
      "epoch: 34, batch: 108, loss: 1.7519092559814453\n",
      "epoch: 34, batch: 109, loss: 1.6084976196289062\n",
      "epoch: 34, batch: 110, loss: 1.4976999759674072\n",
      "epoch: 34, batch: 111, loss: 1.9046634435653687\n",
      "epoch: 34, batch: 112, loss: 1.3742101192474365\n",
      "epoch: 34, batch: 113, loss: 1.5750311613082886\n",
      "epoch: 34, batch: 114, loss: 1.6859807968139648\n",
      "epoch: 34, batch: 115, loss: 1.653786063194275\n",
      "epoch: 34, batch: 116, loss: 1.5348286628723145\n",
      "epoch: 34, batch: 117, loss: 1.4266271591186523\n",
      "epoch: 34, batch: 118, loss: 1.4603936672210693\n",
      "epoch: 34, batch: 119, loss: 1.4986379146575928\n",
      "epoch: 34, batch: 120, loss: 1.4483706951141357\n",
      "epoch: 34, batch: 121, loss: 1.1531569957733154\n",
      "epoch: 34, batch: 122, loss: 1.9428930282592773\n",
      "epoch: 34, batch: 123, loss: 1.4734516143798828\n",
      "epoch: 34, batch: 124, loss: 1.2474043369293213\n",
      "epoch: 34, batch: 125, loss: 1.6637904644012451\n",
      "epoch: 34, batch: 126, loss: 1.3915436267852783\n",
      "epoch: 34, batch: 127, loss: 1.7379612922668457\n",
      "epoch: 35, batch: 0, loss: 1.4377567768096924\n",
      "epoch: 35, batch: 1, loss: 1.7321449518203735\n",
      "epoch: 35, batch: 2, loss: 1.5642417669296265\n",
      "epoch: 35, batch: 3, loss: 1.41497802734375\n",
      "epoch: 35, batch: 4, loss: 1.4020717144012451\n",
      "epoch: 35, batch: 5, loss: 1.291637659072876\n",
      "epoch: 35, batch: 6, loss: 1.5365252494812012\n",
      "epoch: 35, batch: 7, loss: 1.3984416723251343\n",
      "epoch: 35, batch: 8, loss: 1.4958093166351318\n",
      "epoch: 35, batch: 9, loss: 1.4144707918167114\n",
      "epoch: 35, batch: 10, loss: 1.6217517852783203\n",
      "epoch: 35, batch: 11, loss: 1.5336291790008545\n",
      "epoch: 35, batch: 12, loss: 1.3228176832199097\n",
      "epoch: 35, batch: 13, loss: 1.6214679479599\n",
      "epoch: 35, batch: 14, loss: 1.6054614782333374\n",
      "epoch: 35, batch: 15, loss: 1.687173843383789\n",
      "epoch: 35, batch: 16, loss: 1.3068647384643555\n",
      "epoch: 35, batch: 17, loss: 1.5806158781051636\n",
      "epoch: 35, batch: 18, loss: 1.415455937385559\n",
      "epoch: 35, batch: 19, loss: 1.5532423257827759\n",
      "epoch: 35, batch: 20, loss: 1.4161956310272217\n",
      "epoch: 35, batch: 21, loss: 1.4732815027236938\n",
      "epoch: 35, batch: 22, loss: 1.4127299785614014\n",
      "epoch: 35, batch: 23, loss: 1.5823023319244385\n",
      "epoch: 35, batch: 24, loss: 1.618187665939331\n",
      "epoch: 35, batch: 25, loss: 1.631023645401001\n",
      "epoch: 35, batch: 26, loss: 1.3374927043914795\n",
      "epoch: 35, batch: 27, loss: 1.4102542400360107\n",
      "epoch: 35, batch: 28, loss: 1.5770413875579834\n",
      "epoch: 35, batch: 29, loss: 1.7519500255584717\n",
      "epoch: 35, batch: 30, loss: 1.437350869178772\n",
      "epoch: 35, batch: 31, loss: 1.535009503364563\n",
      "epoch: 35, batch: 32, loss: 1.718560814857483\n",
      "epoch: 35, batch: 33, loss: 1.4567747116088867\n",
      "epoch: 35, batch: 34, loss: 1.6329866647720337\n",
      "epoch: 35, batch: 35, loss: 1.391107201576233\n",
      "epoch: 35, batch: 36, loss: 1.6063001155853271\n",
      "epoch: 35, batch: 37, loss: 1.5420715808868408\n",
      "epoch: 35, batch: 38, loss: 1.3979547023773193\n",
      "epoch: 35, batch: 39, loss: 1.4627959728240967\n",
      "epoch: 35, batch: 40, loss: 1.5780266523361206\n",
      "epoch: 35, batch: 41, loss: 1.5567634105682373\n",
      "epoch: 35, batch: 42, loss: 1.5432506799697876\n",
      "epoch: 35, batch: 43, loss: 1.5912020206451416\n",
      "epoch: 35, batch: 44, loss: 1.5718774795532227\n",
      "epoch: 35, batch: 45, loss: 1.4461517333984375\n",
      "epoch: 35, batch: 46, loss: 1.5362895727157593\n",
      "epoch: 35, batch: 47, loss: 1.8550450801849365\n",
      "epoch: 35, batch: 48, loss: 1.2271511554718018\n",
      "epoch: 35, batch: 49, loss: 1.7426910400390625\n",
      "epoch: 35, batch: 50, loss: 1.3225696086883545\n",
      "epoch: 35, batch: 51, loss: 1.4243557453155518\n",
      "epoch: 35, batch: 52, loss: 1.5874736309051514\n",
      "epoch: 35, batch: 53, loss: 1.6554054021835327\n",
      "epoch: 35, batch: 54, loss: 1.5796537399291992\n",
      "epoch: 35, batch: 55, loss: 1.7279335260391235\n",
      "epoch: 35, batch: 56, loss: 1.5734574794769287\n",
      "epoch: 35, batch: 57, loss: 1.671520471572876\n",
      "epoch: 35, batch: 58, loss: 1.3560247421264648\n",
      "epoch: 35, batch: 59, loss: 1.6918216943740845\n",
      "epoch: 35, batch: 60, loss: 1.4690380096435547\n",
      "epoch: 35, batch: 61, loss: 1.7915970087051392\n",
      "epoch: 35, batch: 62, loss: 1.5260250568389893\n",
      "epoch: 35, batch: 63, loss: 1.5847740173339844\n",
      "epoch: 35, batch: 64, loss: 1.455448865890503\n",
      "epoch: 35, batch: 65, loss: 1.5120948553085327\n",
      "epoch: 35, batch: 66, loss: 1.509721040725708\n",
      "epoch: 35, batch: 67, loss: 1.5261106491088867\n",
      "epoch: 35, batch: 68, loss: 1.711602807044983\n",
      "epoch: 35, batch: 69, loss: 1.5368832349777222\n",
      "epoch: 35, batch: 70, loss: 1.6587873697280884\n",
      "epoch: 35, batch: 71, loss: 1.6798384189605713\n",
      "epoch: 35, batch: 72, loss: 1.672663927078247\n",
      "epoch: 35, batch: 73, loss: 1.7705131769180298\n",
      "epoch: 35, batch: 74, loss: 1.1824346780776978\n",
      "epoch: 35, batch: 75, loss: 1.592829704284668\n",
      "epoch: 35, batch: 76, loss: 1.39134681224823\n",
      "epoch: 35, batch: 77, loss: 1.705575942993164\n",
      "epoch: 35, batch: 78, loss: 1.5536019802093506\n",
      "epoch: 35, batch: 79, loss: 1.3985061645507812\n",
      "epoch: 35, batch: 80, loss: 1.7023271322250366\n",
      "epoch: 35, batch: 81, loss: 1.2522233724594116\n",
      "epoch: 35, batch: 82, loss: 1.6612660884857178\n",
      "epoch: 35, batch: 83, loss: 1.290429711341858\n",
      "epoch: 35, batch: 84, loss: 1.6118481159210205\n",
      "epoch: 35, batch: 85, loss: 1.2666372060775757\n",
      "epoch: 35, batch: 86, loss: 1.4799058437347412\n",
      "epoch: 35, batch: 87, loss: 1.5722953081130981\n",
      "epoch: 35, batch: 88, loss: 1.5569639205932617\n",
      "epoch: 35, batch: 89, loss: 1.5322548151016235\n",
      "epoch: 35, batch: 90, loss: 1.2171578407287598\n",
      "epoch: 35, batch: 91, loss: 1.253867745399475\n",
      "epoch: 35, batch: 92, loss: 1.4627277851104736\n",
      "epoch: 35, batch: 93, loss: 1.4179250001907349\n",
      "epoch: 35, batch: 94, loss: 1.4654951095581055\n",
      "epoch: 35, batch: 95, loss: 1.7206904888153076\n",
      "epoch: 35, batch: 96, loss: 1.674485445022583\n",
      "epoch: 35, batch: 97, loss: 1.3143116235733032\n",
      "epoch: 35, batch: 98, loss: 1.5180385112762451\n",
      "epoch: 35, batch: 99, loss: 1.277637243270874\n",
      "epoch: 35, batch: 100, loss: 1.5448118448257446\n",
      "epoch: 35, batch: 101, loss: 1.4668487310409546\n",
      "epoch: 35, batch: 102, loss: 1.7719329595565796\n",
      "epoch: 35, batch: 103, loss: 1.6119410991668701\n",
      "epoch: 35, batch: 104, loss: 1.6919968128204346\n",
      "epoch: 35, batch: 105, loss: 1.6716394424438477\n",
      "epoch: 35, batch: 106, loss: 1.6719497442245483\n",
      "epoch: 35, batch: 107, loss: 1.3982645273208618\n",
      "epoch: 35, batch: 108, loss: 1.462714433670044\n",
      "epoch: 35, batch: 109, loss: 1.679738998413086\n",
      "epoch: 35, batch: 110, loss: 1.6037635803222656\n",
      "epoch: 35, batch: 111, loss: 1.798736333847046\n",
      "epoch: 35, batch: 112, loss: 1.3744146823883057\n",
      "epoch: 35, batch: 113, loss: 1.6293327808380127\n",
      "epoch: 35, batch: 114, loss: 1.3165032863616943\n",
      "epoch: 35, batch: 115, loss: 1.5478668212890625\n",
      "epoch: 35, batch: 116, loss: 1.702332854270935\n",
      "epoch: 35, batch: 117, loss: 1.6501855850219727\n",
      "epoch: 35, batch: 118, loss: 1.3416483402252197\n",
      "epoch: 35, batch: 119, loss: 1.4164787530899048\n",
      "epoch: 35, batch: 120, loss: 1.4349793195724487\n",
      "epoch: 35, batch: 121, loss: 1.2552717924118042\n",
      "epoch: 35, batch: 122, loss: 1.3221290111541748\n",
      "epoch: 35, batch: 123, loss: 1.5674574375152588\n",
      "epoch: 35, batch: 124, loss: 1.5423685312271118\n",
      "epoch: 35, batch: 125, loss: 1.538574457168579\n",
      "epoch: 35, batch: 126, loss: 1.7474091053009033\n",
      "epoch: 35, batch: 127, loss: 1.3530588150024414\n",
      "epoch: 36, batch: 0, loss: 1.4498170614242554\n",
      "epoch: 36, batch: 1, loss: 1.243626356124878\n",
      "epoch: 36, batch: 2, loss: 1.640677809715271\n",
      "epoch: 36, batch: 3, loss: 1.444270133972168\n",
      "epoch: 36, batch: 4, loss: 1.1310808658599854\n",
      "epoch: 36, batch: 5, loss: 1.5129445791244507\n",
      "epoch: 36, batch: 6, loss: 1.5156574249267578\n",
      "epoch: 36, batch: 7, loss: 1.4109786748886108\n",
      "epoch: 36, batch: 8, loss: 1.3160059452056885\n",
      "epoch: 36, batch: 9, loss: 1.6725654602050781\n",
      "epoch: 36, batch: 10, loss: 1.468794584274292\n",
      "epoch: 36, batch: 11, loss: 1.6812353134155273\n",
      "epoch: 36, batch: 12, loss: 1.3289340734481812\n",
      "epoch: 36, batch: 13, loss: 1.3789055347442627\n",
      "epoch: 36, batch: 14, loss: 1.7146217823028564\n",
      "epoch: 36, batch: 15, loss: 1.2538130283355713\n",
      "epoch: 36, batch: 16, loss: 1.4428621530532837\n",
      "epoch: 36, batch: 17, loss: 1.5585991144180298\n",
      "epoch: 36, batch: 18, loss: 1.7573893070220947\n",
      "epoch: 36, batch: 19, loss: 1.452348232269287\n",
      "epoch: 36, batch: 20, loss: 1.2418723106384277\n",
      "epoch: 36, batch: 21, loss: 1.480713963508606\n",
      "epoch: 36, batch: 22, loss: 1.650811791419983\n",
      "epoch: 36, batch: 23, loss: 1.703338861465454\n",
      "epoch: 36, batch: 24, loss: 1.5180853605270386\n",
      "epoch: 36, batch: 25, loss: 1.4623134136199951\n",
      "epoch: 36, batch: 26, loss: 1.3848886489868164\n",
      "epoch: 36, batch: 27, loss: 1.1497986316680908\n",
      "epoch: 36, batch: 28, loss: 1.6791467666625977\n",
      "epoch: 36, batch: 29, loss: 1.095065951347351\n",
      "epoch: 36, batch: 30, loss: 1.6633358001708984\n",
      "epoch: 36, batch: 31, loss: 1.3048759698867798\n",
      "epoch: 36, batch: 32, loss: 1.5028332471847534\n",
      "epoch: 36, batch: 33, loss: 1.5319064855575562\n",
      "epoch: 36, batch: 34, loss: 1.7595913410186768\n",
      "epoch: 36, batch: 35, loss: 1.5330365896224976\n",
      "epoch: 36, batch: 36, loss: 1.5179470777511597\n",
      "epoch: 36, batch: 37, loss: 1.5264815092086792\n",
      "epoch: 36, batch: 38, loss: 1.6439249515533447\n",
      "epoch: 36, batch: 39, loss: 1.7949352264404297\n",
      "epoch: 36, batch: 40, loss: 1.6394275426864624\n",
      "epoch: 36, batch: 41, loss: 1.327880620956421\n",
      "epoch: 36, batch: 42, loss: 1.8887993097305298\n",
      "epoch: 36, batch: 43, loss: 1.6354789733886719\n",
      "epoch: 36, batch: 44, loss: 1.5177233219146729\n",
      "epoch: 36, batch: 45, loss: 1.4060710668563843\n",
      "epoch: 36, batch: 46, loss: 1.7761011123657227\n",
      "epoch: 36, batch: 47, loss: 1.2424778938293457\n",
      "epoch: 36, batch: 48, loss: 1.831987738609314\n",
      "epoch: 36, batch: 49, loss: 1.6042512655258179\n",
      "epoch: 36, batch: 50, loss: 1.3106979131698608\n",
      "epoch: 36, batch: 51, loss: 1.5266001224517822\n",
      "epoch: 36, batch: 52, loss: 1.3217636346817017\n",
      "epoch: 36, batch: 53, loss: 1.493473768234253\n",
      "epoch: 36, batch: 54, loss: 1.6850383281707764\n",
      "epoch: 36, batch: 55, loss: 1.5991737842559814\n",
      "epoch: 36, batch: 56, loss: 1.5790002346038818\n",
      "epoch: 36, batch: 57, loss: 1.5656722784042358\n",
      "epoch: 36, batch: 58, loss: 1.5726951360702515\n",
      "epoch: 36, batch: 59, loss: 1.3190796375274658\n",
      "epoch: 36, batch: 60, loss: 1.683487892150879\n",
      "epoch: 36, batch: 61, loss: 1.7177398204803467\n",
      "epoch: 36, batch: 62, loss: 1.4821580648422241\n",
      "epoch: 36, batch: 63, loss: 1.7127132415771484\n",
      "epoch: 36, batch: 64, loss: 1.8969720602035522\n",
      "epoch: 36, batch: 65, loss: 1.4054360389709473\n",
      "epoch: 36, batch: 66, loss: 1.699304223060608\n",
      "epoch: 36, batch: 67, loss: 1.380234956741333\n",
      "epoch: 36, batch: 68, loss: 1.6127153635025024\n",
      "epoch: 36, batch: 69, loss: 1.6387230157852173\n",
      "epoch: 36, batch: 70, loss: 1.4627859592437744\n",
      "epoch: 36, batch: 71, loss: 1.776343584060669\n",
      "epoch: 36, batch: 72, loss: 1.6304385662078857\n",
      "epoch: 36, batch: 73, loss: 1.2568542957305908\n",
      "epoch: 36, batch: 74, loss: 1.5605652332305908\n",
      "epoch: 36, batch: 75, loss: 1.5910369157791138\n",
      "epoch: 36, batch: 76, loss: 1.4581323862075806\n",
      "epoch: 36, batch: 77, loss: 1.6762454509735107\n",
      "epoch: 36, batch: 78, loss: 1.4064940214157104\n",
      "epoch: 36, batch: 79, loss: 1.5246347188949585\n",
      "epoch: 36, batch: 80, loss: 1.9170351028442383\n",
      "epoch: 36, batch: 81, loss: 1.2928094863891602\n",
      "epoch: 36, batch: 82, loss: 1.4438613653182983\n",
      "epoch: 36, batch: 83, loss: 1.3572744131088257\n",
      "epoch: 36, batch: 84, loss: 1.5320085287094116\n",
      "epoch: 36, batch: 85, loss: 1.3698875904083252\n",
      "epoch: 36, batch: 86, loss: 1.6460012197494507\n",
      "epoch: 36, batch: 87, loss: 1.5344520807266235\n",
      "epoch: 36, batch: 88, loss: 1.7728255987167358\n",
      "epoch: 36, batch: 89, loss: 1.4662103652954102\n",
      "epoch: 36, batch: 90, loss: 1.7047595977783203\n",
      "epoch: 36, batch: 91, loss: 1.2253360748291016\n",
      "epoch: 36, batch: 92, loss: 1.5601531267166138\n",
      "epoch: 36, batch: 93, loss: 1.5548207759857178\n",
      "epoch: 36, batch: 94, loss: 1.5272810459136963\n",
      "epoch: 36, batch: 95, loss: 1.895615577697754\n",
      "epoch: 36, batch: 96, loss: 1.628535509109497\n",
      "epoch: 36, batch: 97, loss: 1.4501936435699463\n",
      "epoch: 36, batch: 98, loss: 1.5861270427703857\n",
      "epoch: 36, batch: 99, loss: 1.618448257446289\n",
      "epoch: 36, batch: 100, loss: 1.5917688608169556\n",
      "epoch: 36, batch: 101, loss: 1.4517219066619873\n",
      "epoch: 36, batch: 102, loss: 1.2997372150421143\n",
      "epoch: 36, batch: 103, loss: 1.6579183340072632\n",
      "epoch: 36, batch: 104, loss: 1.8624153137207031\n",
      "epoch: 36, batch: 105, loss: 1.9458926916122437\n",
      "epoch: 36, batch: 106, loss: 1.3650972843170166\n",
      "epoch: 36, batch: 107, loss: 1.7345062494277954\n",
      "epoch: 36, batch: 108, loss: 1.2750818729400635\n",
      "epoch: 36, batch: 109, loss: 1.7068519592285156\n",
      "epoch: 36, batch: 110, loss: 1.6663486957550049\n",
      "epoch: 36, batch: 111, loss: 1.473780870437622\n",
      "epoch: 36, batch: 112, loss: 1.1273727416992188\n",
      "epoch: 36, batch: 113, loss: 1.396576166152954\n",
      "epoch: 36, batch: 114, loss: 1.6447327136993408\n",
      "epoch: 36, batch: 115, loss: 1.2239158153533936\n",
      "epoch: 36, batch: 116, loss: 1.4210975170135498\n",
      "epoch: 36, batch: 117, loss: 1.703466773033142\n",
      "epoch: 36, batch: 118, loss: 1.4614189863204956\n",
      "epoch: 36, batch: 119, loss: 1.4920305013656616\n",
      "epoch: 36, batch: 120, loss: 1.655928373336792\n",
      "epoch: 36, batch: 121, loss: 1.1620577573776245\n",
      "epoch: 36, batch: 122, loss: 1.7309287786483765\n",
      "epoch: 36, batch: 123, loss: 1.7988297939300537\n",
      "epoch: 36, batch: 124, loss: 1.245482087135315\n",
      "epoch: 36, batch: 125, loss: 1.5862809419631958\n",
      "epoch: 36, batch: 126, loss: 1.3148281574249268\n",
      "epoch: 36, batch: 127, loss: 1.6624400615692139\n",
      "epoch: 37, batch: 0, loss: 2.0483970642089844\n",
      "epoch: 37, batch: 1, loss: 1.6308151483535767\n",
      "epoch: 37, batch: 2, loss: 1.6263256072998047\n",
      "epoch: 37, batch: 3, loss: 1.5286279916763306\n",
      "epoch: 37, batch: 4, loss: 1.5115517377853394\n",
      "epoch: 37, batch: 5, loss: 1.59578275680542\n",
      "epoch: 37, batch: 6, loss: 1.6221284866333008\n",
      "epoch: 37, batch: 7, loss: 1.3993407487869263\n",
      "epoch: 37, batch: 8, loss: 1.6143089532852173\n",
      "epoch: 37, batch: 9, loss: 1.6723082065582275\n",
      "epoch: 37, batch: 10, loss: 1.5681564807891846\n",
      "epoch: 37, batch: 11, loss: 1.4840519428253174\n",
      "epoch: 37, batch: 12, loss: 1.5475414991378784\n",
      "epoch: 37, batch: 13, loss: 1.7784191370010376\n",
      "epoch: 37, batch: 14, loss: 1.3685376644134521\n",
      "epoch: 37, batch: 15, loss: 1.567733645439148\n",
      "epoch: 37, batch: 16, loss: 1.534999132156372\n",
      "epoch: 37, batch: 17, loss: 1.4937666654586792\n",
      "epoch: 37, batch: 18, loss: 1.4917421340942383\n",
      "epoch: 37, batch: 19, loss: 1.6332361698150635\n",
      "epoch: 37, batch: 20, loss: 1.3603744506835938\n",
      "epoch: 37, batch: 21, loss: 1.3895032405853271\n",
      "epoch: 37, batch: 22, loss: 1.403726577758789\n",
      "epoch: 37, batch: 23, loss: 1.4543554782867432\n",
      "epoch: 37, batch: 24, loss: 1.5522210597991943\n",
      "epoch: 37, batch: 25, loss: 1.1541500091552734\n",
      "epoch: 37, batch: 26, loss: 1.7815440893173218\n",
      "epoch: 37, batch: 27, loss: 1.7319433689117432\n",
      "epoch: 37, batch: 28, loss: 1.6060947179794312\n",
      "epoch: 37, batch: 29, loss: 1.409731149673462\n",
      "epoch: 37, batch: 30, loss: 1.3507766723632812\n",
      "epoch: 37, batch: 31, loss: 1.5506019592285156\n",
      "epoch: 37, batch: 32, loss: 1.4601290225982666\n",
      "epoch: 37, batch: 33, loss: 1.4197120666503906\n",
      "epoch: 37, batch: 34, loss: 1.463740348815918\n",
      "epoch: 37, batch: 35, loss: 1.3011891841888428\n",
      "epoch: 37, batch: 36, loss: 1.8609778881072998\n",
      "epoch: 37, batch: 37, loss: 1.694239616394043\n",
      "epoch: 37, batch: 38, loss: 1.3761473894119263\n",
      "epoch: 37, batch: 39, loss: 1.5335986614227295\n",
      "epoch: 37, batch: 40, loss: 1.622907042503357\n",
      "epoch: 37, batch: 41, loss: 1.609492540359497\n",
      "epoch: 37, batch: 42, loss: 1.683598518371582\n",
      "epoch: 37, batch: 43, loss: 1.69222891330719\n",
      "epoch: 37, batch: 44, loss: 1.188798189163208\n",
      "epoch: 37, batch: 45, loss: 1.5275782346725464\n",
      "epoch: 37, batch: 46, loss: 1.4519203901290894\n",
      "epoch: 37, batch: 47, loss: 1.1137522459030151\n",
      "epoch: 37, batch: 48, loss: 1.4188127517700195\n",
      "epoch: 37, batch: 49, loss: 1.5405677556991577\n",
      "epoch: 37, batch: 50, loss: 1.386443853378296\n",
      "epoch: 37, batch: 51, loss: 1.5622458457946777\n",
      "epoch: 37, batch: 52, loss: 1.627859354019165\n",
      "epoch: 37, batch: 53, loss: 1.3555474281311035\n",
      "epoch: 37, batch: 54, loss: 1.5694761276245117\n",
      "epoch: 37, batch: 55, loss: 1.4592711925506592\n",
      "epoch: 37, batch: 56, loss: 1.6691163778305054\n",
      "epoch: 37, batch: 57, loss: 1.5765670537948608\n",
      "epoch: 37, batch: 58, loss: 1.6383380889892578\n",
      "epoch: 37, batch: 59, loss: 1.4167509078979492\n",
      "epoch: 37, batch: 60, loss: 1.7145963907241821\n",
      "epoch: 37, batch: 61, loss: 1.5766260623931885\n",
      "epoch: 37, batch: 62, loss: 1.6398528814315796\n",
      "epoch: 37, batch: 63, loss: 1.1439626216888428\n",
      "epoch: 37, batch: 64, loss: 1.3219239711761475\n",
      "epoch: 37, batch: 65, loss: 1.3858880996704102\n",
      "epoch: 37, batch: 66, loss: 1.4508253335952759\n",
      "epoch: 37, batch: 67, loss: 1.439175009727478\n",
      "epoch: 37, batch: 68, loss: 1.8599903583526611\n",
      "epoch: 37, batch: 69, loss: 1.1893424987792969\n",
      "epoch: 37, batch: 70, loss: 1.6021400690078735\n",
      "epoch: 37, batch: 71, loss: 1.6250457763671875\n",
      "epoch: 37, batch: 72, loss: 1.6927417516708374\n",
      "epoch: 37, batch: 73, loss: 1.5658631324768066\n",
      "epoch: 37, batch: 74, loss: 1.5761083364486694\n",
      "epoch: 37, batch: 75, loss: 1.505112886428833\n",
      "epoch: 37, batch: 76, loss: 1.7381633520126343\n",
      "epoch: 37, batch: 77, loss: 1.242251992225647\n",
      "epoch: 37, batch: 78, loss: 1.3159633874893188\n",
      "epoch: 37, batch: 79, loss: 1.47730553150177\n",
      "epoch: 37, batch: 80, loss: 1.6298978328704834\n",
      "epoch: 37, batch: 81, loss: 1.6376190185546875\n",
      "epoch: 37, batch: 82, loss: 1.534501552581787\n",
      "epoch: 37, batch: 83, loss: 1.6519711017608643\n",
      "epoch: 37, batch: 84, loss: 1.6097362041473389\n",
      "epoch: 37, batch: 85, loss: 1.6833093166351318\n",
      "epoch: 37, batch: 86, loss: 1.4656909704208374\n",
      "epoch: 37, batch: 87, loss: 1.3274648189544678\n",
      "epoch: 37, batch: 88, loss: 1.6800143718719482\n",
      "epoch: 37, batch: 89, loss: 1.776246428489685\n",
      "epoch: 37, batch: 90, loss: 1.4657405614852905\n",
      "epoch: 37, batch: 91, loss: 1.7016319036483765\n",
      "epoch: 37, batch: 92, loss: 1.4601633548736572\n",
      "epoch: 37, batch: 93, loss: 1.649909257888794\n",
      "epoch: 37, batch: 94, loss: 1.3853912353515625\n",
      "epoch: 37, batch: 95, loss: 1.4896063804626465\n",
      "epoch: 37, batch: 96, loss: 1.5406605005264282\n",
      "epoch: 37, batch: 97, loss: 1.4261833429336548\n",
      "epoch: 37, batch: 98, loss: 1.2478735446929932\n",
      "epoch: 37, batch: 99, loss: 1.7081470489501953\n",
      "epoch: 37, batch: 100, loss: 1.6402511596679688\n",
      "epoch: 37, batch: 101, loss: 1.6875288486480713\n",
      "epoch: 37, batch: 102, loss: 1.6925790309906006\n",
      "epoch: 37, batch: 103, loss: 1.4694955348968506\n",
      "epoch: 37, batch: 104, loss: 1.2211859226226807\n",
      "epoch: 37, batch: 105, loss: 1.4096826314926147\n",
      "epoch: 37, batch: 106, loss: 1.867685317993164\n",
      "epoch: 37, batch: 107, loss: 1.4973093271255493\n",
      "epoch: 37, batch: 108, loss: 1.3606246709823608\n",
      "epoch: 37, batch: 109, loss: 1.4874238967895508\n",
      "epoch: 37, batch: 110, loss: 1.6069848537445068\n",
      "epoch: 37, batch: 111, loss: 1.8343013525009155\n",
      "epoch: 37, batch: 112, loss: 1.536665916442871\n",
      "epoch: 37, batch: 113, loss: 1.4030437469482422\n",
      "epoch: 37, batch: 114, loss: 1.8065919876098633\n",
      "epoch: 37, batch: 115, loss: 1.4698994159698486\n",
      "epoch: 37, batch: 116, loss: 1.3903875350952148\n",
      "epoch: 37, batch: 117, loss: 1.6973206996917725\n",
      "epoch: 37, batch: 118, loss: 1.7722740173339844\n",
      "epoch: 37, batch: 119, loss: 1.1819225549697876\n",
      "epoch: 37, batch: 120, loss: 1.1777349710464478\n",
      "epoch: 37, batch: 121, loss: 1.6328449249267578\n",
      "epoch: 37, batch: 122, loss: 1.5441359281539917\n",
      "epoch: 37, batch: 123, loss: 1.580861210823059\n",
      "epoch: 37, batch: 124, loss: 1.656427025794983\n",
      "epoch: 37, batch: 125, loss: 1.5756436586380005\n",
      "epoch: 37, batch: 126, loss: 1.5509580373764038\n",
      "epoch: 37, batch: 127, loss: 1.5020923614501953\n",
      "epoch: 38, batch: 0, loss: 1.363781213760376\n",
      "epoch: 38, batch: 1, loss: 1.2258063554763794\n",
      "epoch: 38, batch: 2, loss: 1.746262788772583\n",
      "epoch: 38, batch: 3, loss: 1.7884645462036133\n",
      "epoch: 38, batch: 4, loss: 1.6717746257781982\n",
      "epoch: 38, batch: 5, loss: 1.1595888137817383\n",
      "epoch: 38, batch: 6, loss: 1.7609384059906006\n",
      "epoch: 38, batch: 7, loss: 1.2887120246887207\n",
      "epoch: 38, batch: 8, loss: 1.4840302467346191\n",
      "epoch: 38, batch: 9, loss: 1.2396498918533325\n",
      "epoch: 38, batch: 10, loss: 1.2589963674545288\n",
      "epoch: 38, batch: 11, loss: 1.6101925373077393\n",
      "epoch: 38, batch: 12, loss: 1.5980632305145264\n",
      "epoch: 38, batch: 13, loss: 1.3578615188598633\n",
      "epoch: 38, batch: 14, loss: 1.669837236404419\n",
      "epoch: 38, batch: 15, loss: 1.4123296737670898\n",
      "epoch: 38, batch: 16, loss: 1.5246267318725586\n",
      "epoch: 38, batch: 17, loss: 1.6455023288726807\n",
      "epoch: 38, batch: 18, loss: 1.624590516090393\n",
      "epoch: 38, batch: 19, loss: 1.7728004455566406\n",
      "epoch: 38, batch: 20, loss: 1.7414567470550537\n",
      "epoch: 38, batch: 21, loss: 1.3803799152374268\n",
      "epoch: 38, batch: 22, loss: 1.736486792564392\n",
      "epoch: 38, batch: 23, loss: 1.8301067352294922\n",
      "epoch: 38, batch: 24, loss: 1.3780980110168457\n",
      "epoch: 38, batch: 25, loss: 1.746214509010315\n",
      "epoch: 38, batch: 26, loss: 1.5336490869522095\n",
      "epoch: 38, batch: 27, loss: 1.8196723461151123\n",
      "epoch: 38, batch: 28, loss: 1.5327528715133667\n",
      "epoch: 38, batch: 29, loss: 1.6244779825210571\n",
      "epoch: 38, batch: 30, loss: 1.462393045425415\n",
      "epoch: 38, batch: 31, loss: 1.4109848737716675\n",
      "epoch: 38, batch: 32, loss: 1.3726094961166382\n",
      "epoch: 38, batch: 33, loss: 1.7861707210540771\n",
      "epoch: 38, batch: 34, loss: 1.4798835515975952\n",
      "epoch: 38, batch: 35, loss: 1.4804191589355469\n",
      "epoch: 38, batch: 36, loss: 1.5505138635635376\n",
      "epoch: 38, batch: 37, loss: 1.810239553451538\n",
      "epoch: 38, batch: 38, loss: 1.4076091051101685\n",
      "epoch: 38, batch: 39, loss: 1.4998048543930054\n",
      "epoch: 38, batch: 40, loss: 1.6876564025878906\n",
      "epoch: 38, batch: 41, loss: 1.544447660446167\n",
      "epoch: 38, batch: 42, loss: 1.455201506614685\n",
      "epoch: 38, batch: 43, loss: 1.5216056108474731\n",
      "epoch: 38, batch: 44, loss: 1.3362953662872314\n",
      "epoch: 38, batch: 45, loss: 1.348418951034546\n",
      "epoch: 38, batch: 46, loss: 1.927635908126831\n",
      "epoch: 38, batch: 47, loss: 1.7026504278182983\n",
      "epoch: 38, batch: 48, loss: 1.4066680669784546\n",
      "epoch: 38, batch: 49, loss: 1.4913763999938965\n",
      "epoch: 38, batch: 50, loss: 1.7527496814727783\n",
      "epoch: 38, batch: 51, loss: 1.2758280038833618\n",
      "epoch: 38, batch: 52, loss: 1.5613043308258057\n",
      "epoch: 38, batch: 53, loss: 1.621875524520874\n",
      "epoch: 38, batch: 54, loss: 1.634113073348999\n",
      "epoch: 38, batch: 55, loss: 1.5491608381271362\n",
      "epoch: 38, batch: 56, loss: 1.4546689987182617\n",
      "epoch: 38, batch: 57, loss: 1.5197042226791382\n",
      "epoch: 38, batch: 58, loss: 1.365434169769287\n",
      "epoch: 38, batch: 59, loss: 1.2718819379806519\n",
      "epoch: 38, batch: 60, loss: 1.4328765869140625\n",
      "epoch: 38, batch: 61, loss: 1.6793582439422607\n",
      "epoch: 38, batch: 62, loss: 1.583148717880249\n",
      "epoch: 38, batch: 63, loss: 1.6164591312408447\n",
      "epoch: 38, batch: 64, loss: 1.2878378629684448\n",
      "epoch: 38, batch: 65, loss: 1.398101568222046\n",
      "epoch: 38, batch: 66, loss: 1.3586008548736572\n",
      "epoch: 38, batch: 67, loss: 1.71359121799469\n",
      "epoch: 38, batch: 68, loss: 1.4769957065582275\n",
      "epoch: 38, batch: 69, loss: 1.2223371267318726\n",
      "epoch: 38, batch: 70, loss: 1.758907675743103\n",
      "epoch: 38, batch: 71, loss: 1.6744846105575562\n",
      "epoch: 38, batch: 72, loss: 1.6362909078598022\n",
      "epoch: 38, batch: 73, loss: 1.3079288005828857\n",
      "epoch: 38, batch: 74, loss: 1.5423517227172852\n",
      "epoch: 38, batch: 75, loss: 1.597184419631958\n",
      "epoch: 38, batch: 76, loss: 1.6381595134735107\n",
      "epoch: 38, batch: 77, loss: 1.4080069065093994\n",
      "epoch: 38, batch: 78, loss: 1.6453536748886108\n",
      "epoch: 38, batch: 79, loss: 1.4265373945236206\n",
      "epoch: 38, batch: 80, loss: 1.4054481983184814\n",
      "epoch: 38, batch: 81, loss: 1.6843732595443726\n",
      "epoch: 38, batch: 82, loss: 1.5624595880508423\n",
      "epoch: 38, batch: 83, loss: 1.4810125827789307\n",
      "epoch: 38, batch: 84, loss: 1.5450623035430908\n",
      "epoch: 38, batch: 85, loss: 1.4991354942321777\n",
      "epoch: 38, batch: 86, loss: 1.6717050075531006\n",
      "epoch: 38, batch: 87, loss: 1.549429178237915\n",
      "epoch: 38, batch: 88, loss: 1.5754247903823853\n",
      "epoch: 38, batch: 89, loss: 1.5904117822647095\n",
      "epoch: 38, batch: 90, loss: 1.7220379114151\n",
      "epoch: 38, batch: 91, loss: 1.474758505821228\n",
      "epoch: 38, batch: 92, loss: 1.6288816928863525\n",
      "epoch: 38, batch: 93, loss: 1.305280089378357\n",
      "epoch: 38, batch: 94, loss: 1.5941016674041748\n",
      "epoch: 38, batch: 95, loss: 1.4802786111831665\n",
      "epoch: 38, batch: 96, loss: 1.4860999584197998\n",
      "epoch: 38, batch: 97, loss: 1.4178001880645752\n",
      "epoch: 38, batch: 98, loss: 1.4824522733688354\n",
      "epoch: 38, batch: 99, loss: 1.5145498514175415\n",
      "epoch: 38, batch: 100, loss: 1.4903550148010254\n",
      "epoch: 38, batch: 101, loss: 1.4674466848373413\n",
      "epoch: 38, batch: 102, loss: 1.3559335470199585\n",
      "epoch: 38, batch: 103, loss: 1.6350963115692139\n",
      "epoch: 38, batch: 104, loss: 1.2763793468475342\n",
      "epoch: 38, batch: 105, loss: 1.6208728551864624\n",
      "epoch: 38, batch: 106, loss: 1.5468690395355225\n",
      "epoch: 38, batch: 107, loss: 1.3466535806655884\n",
      "epoch: 38, batch: 108, loss: 1.4279441833496094\n",
      "epoch: 38, batch: 109, loss: 1.5448765754699707\n",
      "epoch: 38, batch: 110, loss: 1.5313307046890259\n",
      "epoch: 38, batch: 111, loss: 1.5099828243255615\n",
      "epoch: 38, batch: 112, loss: 1.4566538333892822\n",
      "epoch: 38, batch: 113, loss: 1.6865742206573486\n",
      "epoch: 38, batch: 114, loss: 1.659074068069458\n",
      "epoch: 38, batch: 115, loss: 1.57411789894104\n",
      "epoch: 38, batch: 116, loss: 1.5739489793777466\n",
      "epoch: 38, batch: 117, loss: 1.4806444644927979\n",
      "epoch: 38, batch: 118, loss: 1.7324119806289673\n",
      "epoch: 38, batch: 119, loss: 1.2868731021881104\n",
      "epoch: 38, batch: 120, loss: 1.7181323766708374\n",
      "epoch: 38, batch: 121, loss: 1.2877715826034546\n",
      "epoch: 38, batch: 122, loss: 1.6247127056121826\n",
      "epoch: 38, batch: 123, loss: 1.413408875465393\n",
      "epoch: 38, batch: 124, loss: 1.8195873498916626\n",
      "epoch: 38, batch: 125, loss: 1.2474490404129028\n",
      "epoch: 38, batch: 126, loss: 1.6436984539031982\n",
      "epoch: 38, batch: 127, loss: 1.3818050622940063\n",
      "epoch: 39, batch: 0, loss: 1.6567453145980835\n",
      "epoch: 39, batch: 1, loss: 1.5642168521881104\n",
      "epoch: 39, batch: 2, loss: 1.3922868967056274\n",
      "epoch: 39, batch: 3, loss: 1.4783928394317627\n",
      "epoch: 39, batch: 4, loss: 1.7139685153961182\n",
      "epoch: 39, batch: 5, loss: 1.821874976158142\n",
      "epoch: 39, batch: 6, loss: 1.633042573928833\n",
      "epoch: 39, batch: 7, loss: 1.6113522052764893\n",
      "epoch: 39, batch: 8, loss: 1.580834984779358\n",
      "epoch: 39, batch: 9, loss: 1.7821115255355835\n",
      "epoch: 39, batch: 10, loss: 1.5482428073883057\n",
      "epoch: 39, batch: 11, loss: 1.3175913095474243\n",
      "epoch: 39, batch: 12, loss: 1.6243892908096313\n",
      "epoch: 39, batch: 13, loss: 1.4652758836746216\n",
      "epoch: 39, batch: 14, loss: 1.4442495107650757\n",
      "epoch: 39, batch: 15, loss: 1.339575171470642\n",
      "epoch: 39, batch: 16, loss: 1.8072593212127686\n",
      "epoch: 39, batch: 17, loss: 1.6158090829849243\n",
      "epoch: 39, batch: 18, loss: 1.6130151748657227\n",
      "epoch: 39, batch: 19, loss: 1.4187182188034058\n",
      "epoch: 39, batch: 20, loss: 1.5113445520401\n",
      "epoch: 39, batch: 21, loss: 1.65869140625\n",
      "epoch: 39, batch: 22, loss: 1.6137752532958984\n",
      "epoch: 39, batch: 23, loss: 1.891235589981079\n",
      "epoch: 39, batch: 24, loss: 1.6733955144882202\n",
      "epoch: 39, batch: 25, loss: 1.392217993736267\n",
      "epoch: 39, batch: 26, loss: 1.2377125024795532\n",
      "epoch: 39, batch: 27, loss: 1.352670431137085\n",
      "epoch: 39, batch: 28, loss: 1.545828104019165\n",
      "epoch: 39, batch: 29, loss: 1.5467411279678345\n",
      "epoch: 39, batch: 30, loss: 1.4410278797149658\n",
      "epoch: 39, batch: 31, loss: 1.6021798849105835\n",
      "epoch: 39, batch: 32, loss: 1.329242467880249\n",
      "epoch: 39, batch: 33, loss: 1.4051029682159424\n",
      "epoch: 39, batch: 34, loss: 1.1669682264328003\n",
      "epoch: 39, batch: 35, loss: 1.5142940282821655\n",
      "epoch: 39, batch: 36, loss: 1.6371142864227295\n",
      "epoch: 39, batch: 37, loss: 1.6129859685897827\n",
      "epoch: 39, batch: 38, loss: 1.379446268081665\n",
      "epoch: 39, batch: 39, loss: 1.4833667278289795\n",
      "epoch: 39, batch: 40, loss: 1.4939152002334595\n",
      "epoch: 39, batch: 41, loss: 1.3384119272232056\n",
      "epoch: 39, batch: 42, loss: 1.7303435802459717\n",
      "epoch: 39, batch: 43, loss: 1.4768924713134766\n",
      "epoch: 39, batch: 44, loss: 1.567273736000061\n",
      "epoch: 39, batch: 45, loss: 1.6795294284820557\n",
      "epoch: 39, batch: 46, loss: 1.3471187353134155\n",
      "epoch: 39, batch: 47, loss: 1.7864856719970703\n",
      "epoch: 39, batch: 48, loss: 1.2075519561767578\n",
      "epoch: 39, batch: 49, loss: 1.5187325477600098\n",
      "epoch: 39, batch: 50, loss: 1.521266222000122\n",
      "epoch: 39, batch: 51, loss: 1.1797068119049072\n",
      "epoch: 39, batch: 52, loss: 1.2423837184906006\n",
      "epoch: 39, batch: 53, loss: 1.6273243427276611\n",
      "epoch: 39, batch: 54, loss: 1.4897552728652954\n",
      "epoch: 39, batch: 55, loss: 1.6737892627716064\n",
      "epoch: 39, batch: 56, loss: 1.4532538652420044\n",
      "epoch: 39, batch: 57, loss: 1.5527572631835938\n",
      "epoch: 39, batch: 58, loss: 1.2867062091827393\n",
      "epoch: 39, batch: 59, loss: 1.480905532836914\n",
      "epoch: 39, batch: 60, loss: 1.7465217113494873\n",
      "epoch: 39, batch: 61, loss: 1.7593278884887695\n",
      "epoch: 39, batch: 62, loss: 1.6958366632461548\n",
      "epoch: 39, batch: 63, loss: 1.5546348094940186\n",
      "epoch: 39, batch: 64, loss: 1.2802560329437256\n",
      "epoch: 39, batch: 65, loss: 1.5340543985366821\n",
      "epoch: 39, batch: 66, loss: 1.5691795349121094\n",
      "epoch: 39, batch: 67, loss: 1.5972769260406494\n",
      "epoch: 39, batch: 68, loss: 1.6435081958770752\n",
      "epoch: 39, batch: 69, loss: 1.44752037525177\n",
      "epoch: 39, batch: 70, loss: 1.520842432975769\n",
      "epoch: 39, batch: 71, loss: 1.5482890605926514\n",
      "epoch: 39, batch: 72, loss: 1.5353705883026123\n",
      "epoch: 39, batch: 73, loss: 1.6444095373153687\n",
      "epoch: 39, batch: 74, loss: 1.7847816944122314\n",
      "epoch: 39, batch: 75, loss: 1.5557763576507568\n",
      "epoch: 39, batch: 76, loss: 1.3020954132080078\n",
      "epoch: 39, batch: 77, loss: 1.6729605197906494\n",
      "epoch: 39, batch: 78, loss: 1.6986591815948486\n",
      "epoch: 39, batch: 79, loss: 1.4756858348846436\n",
      "epoch: 39, batch: 80, loss: 1.8343799114227295\n",
      "epoch: 39, batch: 81, loss: 1.0750877857208252\n",
      "epoch: 39, batch: 82, loss: 1.1690711975097656\n",
      "epoch: 39, batch: 83, loss: 1.7873531579971313\n",
      "epoch: 39, batch: 84, loss: 1.621692419052124\n",
      "epoch: 39, batch: 85, loss: 1.5190876722335815\n",
      "epoch: 39, batch: 86, loss: 1.6059997081756592\n",
      "epoch: 39, batch: 87, loss: 1.3416026830673218\n",
      "epoch: 39, batch: 88, loss: 1.4233869314193726\n",
      "epoch: 39, batch: 89, loss: 1.389412522315979\n",
      "epoch: 39, batch: 90, loss: 1.5805470943450928\n",
      "epoch: 39, batch: 91, loss: 1.5550538301467896\n",
      "epoch: 39, batch: 92, loss: 1.5628454685211182\n",
      "epoch: 39, batch: 93, loss: 1.586346983909607\n",
      "epoch: 39, batch: 94, loss: 1.4918347597122192\n",
      "epoch: 39, batch: 95, loss: 1.3172285556793213\n",
      "epoch: 39, batch: 96, loss: 1.4029664993286133\n",
      "epoch: 39, batch: 97, loss: 1.6110258102416992\n",
      "epoch: 39, batch: 98, loss: 1.6423012018203735\n",
      "epoch: 39, batch: 99, loss: 1.46322500705719\n",
      "epoch: 39, batch: 100, loss: 1.5407912731170654\n",
      "epoch: 39, batch: 101, loss: 1.532548189163208\n",
      "epoch: 39, batch: 102, loss: 1.3493196964263916\n",
      "epoch: 39, batch: 103, loss: 1.4413831233978271\n",
      "epoch: 39, batch: 104, loss: 1.4592595100402832\n",
      "epoch: 39, batch: 105, loss: 1.0306065082550049\n",
      "epoch: 39, batch: 106, loss: 1.3951438665390015\n",
      "epoch: 39, batch: 107, loss: 1.7541630268096924\n",
      "epoch: 39, batch: 108, loss: 1.5635861158370972\n",
      "epoch: 39, batch: 109, loss: 1.8050296306610107\n",
      "epoch: 39, batch: 110, loss: 1.638027548789978\n",
      "epoch: 39, batch: 111, loss: 1.7101293802261353\n",
      "epoch: 39, batch: 112, loss: 1.3044307231903076\n",
      "epoch: 39, batch: 113, loss: 1.4845682382583618\n",
      "epoch: 39, batch: 114, loss: 1.3764989376068115\n",
      "epoch: 39, batch: 115, loss: 1.4720659255981445\n",
      "epoch: 39, batch: 116, loss: 1.7757211923599243\n",
      "epoch: 39, batch: 117, loss: 1.6901628971099854\n",
      "epoch: 39, batch: 118, loss: 1.5378220081329346\n",
      "epoch: 39, batch: 119, loss: 1.6041408777236938\n",
      "epoch: 39, batch: 120, loss: 1.4225517511367798\n",
      "epoch: 39, batch: 121, loss: 1.8088808059692383\n",
      "epoch: 39, batch: 122, loss: 1.379144310951233\n",
      "epoch: 39, batch: 123, loss: 1.5385046005249023\n",
      "epoch: 39, batch: 124, loss: 1.4031885862350464\n",
      "epoch: 39, batch: 125, loss: 1.015383005142212\n",
      "epoch: 39, batch: 126, loss: 1.8016973733901978\n",
      "epoch: 39, batch: 127, loss: 1.7883179187774658\n",
      "epoch: 40, batch: 0, loss: 1.7970844507217407\n",
      "epoch: 40, batch: 1, loss: 1.48171865940094\n",
      "epoch: 40, batch: 2, loss: 1.5007902383804321\n",
      "epoch: 40, batch: 3, loss: 1.445039987564087\n",
      "epoch: 40, batch: 4, loss: 1.3508245944976807\n",
      "epoch: 40, batch: 5, loss: 1.5309562683105469\n",
      "epoch: 40, batch: 6, loss: 1.4312652349472046\n",
      "epoch: 40, batch: 7, loss: 1.189284324645996\n",
      "epoch: 40, batch: 8, loss: 1.4686973094940186\n",
      "epoch: 40, batch: 9, loss: 1.2158845663070679\n",
      "epoch: 40, batch: 10, loss: 1.3128162622451782\n",
      "epoch: 40, batch: 11, loss: 1.2358458042144775\n",
      "epoch: 40, batch: 12, loss: 1.3831919431686401\n",
      "epoch: 40, batch: 13, loss: 1.6325931549072266\n",
      "epoch: 40, batch: 14, loss: 1.4570088386535645\n",
      "epoch: 40, batch: 15, loss: 1.845271348953247\n",
      "epoch: 40, batch: 16, loss: 1.7289940118789673\n",
      "epoch: 40, batch: 17, loss: 1.2462981939315796\n",
      "epoch: 40, batch: 18, loss: 1.4110864400863647\n",
      "epoch: 40, batch: 19, loss: 1.693992257118225\n",
      "epoch: 40, batch: 20, loss: 1.6802990436553955\n",
      "epoch: 40, batch: 21, loss: 1.486291766166687\n",
      "epoch: 40, batch: 22, loss: 2.1548805236816406\n",
      "epoch: 40, batch: 23, loss: 1.36623215675354\n",
      "epoch: 40, batch: 24, loss: 1.9444223642349243\n",
      "epoch: 40, batch: 25, loss: 1.450530767440796\n",
      "epoch: 40, batch: 26, loss: 1.39493727684021\n",
      "epoch: 40, batch: 27, loss: 1.8276193141937256\n",
      "epoch: 40, batch: 28, loss: 1.3496954441070557\n",
      "epoch: 40, batch: 29, loss: 1.655527114868164\n",
      "epoch: 40, batch: 30, loss: 1.4658626317977905\n",
      "epoch: 40, batch: 31, loss: 1.4832066297531128\n",
      "epoch: 40, batch: 32, loss: 2.097080945968628\n",
      "epoch: 40, batch: 33, loss: 1.8322616815567017\n",
      "epoch: 40, batch: 34, loss: 1.5463078022003174\n",
      "epoch: 40, batch: 35, loss: 1.5681405067443848\n",
      "epoch: 40, batch: 36, loss: 1.5843465328216553\n",
      "epoch: 40, batch: 37, loss: 1.2490980625152588\n",
      "epoch: 40, batch: 38, loss: 1.475202202796936\n",
      "epoch: 40, batch: 39, loss: 1.5210398435592651\n",
      "epoch: 40, batch: 40, loss: 1.5243499279022217\n",
      "epoch: 40, batch: 41, loss: 1.9652955532073975\n",
      "epoch: 40, batch: 42, loss: 1.474575161933899\n",
      "epoch: 40, batch: 43, loss: 1.3773913383483887\n",
      "epoch: 40, batch: 44, loss: 1.5740907192230225\n",
      "epoch: 40, batch: 45, loss: 1.3356375694274902\n",
      "epoch: 40, batch: 46, loss: 1.6971476078033447\n",
      "epoch: 40, batch: 47, loss: 1.3446018695831299\n",
      "epoch: 40, batch: 48, loss: 1.2068207263946533\n",
      "epoch: 40, batch: 49, loss: 1.4307103157043457\n",
      "epoch: 40, batch: 50, loss: 1.4413553476333618\n",
      "epoch: 40, batch: 51, loss: 1.6408050060272217\n",
      "epoch: 40, batch: 52, loss: 1.9605687856674194\n",
      "epoch: 40, batch: 53, loss: 1.653571367263794\n",
      "epoch: 40, batch: 54, loss: 1.6868959665298462\n",
      "epoch: 40, batch: 55, loss: 1.6236531734466553\n",
      "epoch: 40, batch: 56, loss: 1.1464813947677612\n",
      "epoch: 40, batch: 57, loss: 1.473142385482788\n",
      "epoch: 40, batch: 58, loss: 1.7174724340438843\n",
      "epoch: 40, batch: 59, loss: 1.5485684871673584\n",
      "epoch: 40, batch: 60, loss: 1.3416712284088135\n",
      "epoch: 40, batch: 61, loss: 1.3183180093765259\n",
      "epoch: 40, batch: 62, loss: 1.693874716758728\n",
      "epoch: 40, batch: 63, loss: 1.8975368738174438\n",
      "epoch: 40, batch: 64, loss: 1.5070316791534424\n",
      "epoch: 40, batch: 65, loss: 1.3943496942520142\n",
      "epoch: 40, batch: 66, loss: 1.8559623956680298\n",
      "epoch: 40, batch: 67, loss: 1.435600996017456\n",
      "epoch: 40, batch: 68, loss: 1.5207602977752686\n",
      "epoch: 40, batch: 69, loss: 1.624029517173767\n",
      "epoch: 40, batch: 70, loss: 1.6270850896835327\n",
      "epoch: 40, batch: 71, loss: 1.192507028579712\n",
      "epoch: 40, batch: 72, loss: 1.3491634130477905\n",
      "epoch: 40, batch: 73, loss: 1.79647958278656\n",
      "epoch: 40, batch: 74, loss: 1.616721749305725\n",
      "epoch: 40, batch: 75, loss: 1.5577435493469238\n",
      "epoch: 40, batch: 76, loss: 1.5289455652236938\n",
      "epoch: 40, batch: 77, loss: 1.5846275091171265\n",
      "epoch: 40, batch: 78, loss: 1.2473206520080566\n",
      "epoch: 40, batch: 79, loss: 1.5520118474960327\n",
      "epoch: 40, batch: 80, loss: 1.431581735610962\n",
      "epoch: 40, batch: 81, loss: 1.432917833328247\n",
      "epoch: 40, batch: 82, loss: 1.4488985538482666\n",
      "epoch: 40, batch: 83, loss: 1.3269703388214111\n",
      "epoch: 40, batch: 84, loss: 1.069754958152771\n",
      "epoch: 40, batch: 85, loss: 1.4324326515197754\n",
      "epoch: 40, batch: 86, loss: 1.63064444065094\n",
      "epoch: 40, batch: 87, loss: 1.4492496252059937\n",
      "epoch: 40, batch: 88, loss: 1.4463858604431152\n",
      "epoch: 40, batch: 89, loss: 1.4058250188827515\n",
      "epoch: 40, batch: 90, loss: 1.606774926185608\n",
      "epoch: 40, batch: 91, loss: 1.620566964149475\n",
      "epoch: 40, batch: 92, loss: 1.519173502922058\n",
      "epoch: 40, batch: 93, loss: 1.342468500137329\n",
      "epoch: 40, batch: 94, loss: 1.9473695755004883\n",
      "epoch: 40, batch: 95, loss: 1.6143547296524048\n",
      "epoch: 40, batch: 96, loss: 1.5274131298065186\n",
      "epoch: 40, batch: 97, loss: 1.5334044694900513\n",
      "epoch: 40, batch: 98, loss: 1.4159507751464844\n",
      "epoch: 40, batch: 99, loss: 1.8389695882797241\n",
      "epoch: 40, batch: 100, loss: 1.4308756589889526\n",
      "epoch: 40, batch: 101, loss: 1.6416950225830078\n",
      "epoch: 40, batch: 102, loss: 1.5719082355499268\n",
      "epoch: 40, batch: 103, loss: 1.7240171432495117\n",
      "epoch: 40, batch: 104, loss: 1.5652287006378174\n",
      "epoch: 40, batch: 105, loss: 1.5286214351654053\n",
      "epoch: 40, batch: 106, loss: 1.3859691619873047\n",
      "epoch: 40, batch: 107, loss: 1.2718666791915894\n",
      "epoch: 40, batch: 108, loss: 1.2891638278961182\n",
      "epoch: 40, batch: 109, loss: 1.2548768520355225\n",
      "epoch: 40, batch: 110, loss: 1.3254377841949463\n",
      "epoch: 40, batch: 111, loss: 1.7101900577545166\n",
      "epoch: 40, batch: 112, loss: 1.3676941394805908\n",
      "epoch: 40, batch: 113, loss: 1.5025545358657837\n",
      "epoch: 40, batch: 114, loss: 1.3515911102294922\n",
      "epoch: 40, batch: 115, loss: 1.1940873861312866\n",
      "epoch: 40, batch: 116, loss: 1.4606159925460815\n",
      "epoch: 40, batch: 117, loss: 1.5560173988342285\n",
      "epoch: 40, batch: 118, loss: 1.5166475772857666\n",
      "epoch: 40, batch: 119, loss: 1.6153814792633057\n",
      "epoch: 40, batch: 120, loss: 1.1973941326141357\n",
      "epoch: 40, batch: 121, loss: 1.9239168167114258\n",
      "epoch: 40, batch: 122, loss: 1.6785697937011719\n",
      "epoch: 40, batch: 123, loss: 1.470673680305481\n",
      "epoch: 40, batch: 124, loss: 1.7020374536514282\n",
      "epoch: 40, batch: 125, loss: 1.5674740076065063\n",
      "epoch: 40, batch: 126, loss: 1.9992644786834717\n",
      "epoch: 40, batch: 127, loss: 1.6359127759933472\n",
      "epoch: 41, batch: 0, loss: 1.386106252670288\n",
      "epoch: 41, batch: 1, loss: 1.4813638925552368\n",
      "epoch: 41, batch: 2, loss: 1.5765256881713867\n",
      "epoch: 41, batch: 3, loss: 1.9592430591583252\n",
      "epoch: 41, batch: 4, loss: 1.6340233087539673\n",
      "epoch: 41, batch: 5, loss: 1.0714668035507202\n",
      "epoch: 41, batch: 6, loss: 1.530670166015625\n",
      "epoch: 41, batch: 7, loss: 1.5891746282577515\n",
      "epoch: 41, batch: 8, loss: 1.3774137496948242\n",
      "epoch: 41, batch: 9, loss: 1.3012330532073975\n",
      "epoch: 41, batch: 10, loss: 1.4187967777252197\n",
      "epoch: 41, batch: 11, loss: 1.3460638523101807\n",
      "epoch: 41, batch: 12, loss: 1.507510781288147\n",
      "epoch: 41, batch: 13, loss: 1.545264482498169\n",
      "epoch: 41, batch: 14, loss: 1.6556711196899414\n",
      "epoch: 41, batch: 15, loss: 1.7967052459716797\n",
      "epoch: 41, batch: 16, loss: 1.3520315885543823\n",
      "epoch: 41, batch: 17, loss: 1.4481117725372314\n",
      "epoch: 41, batch: 18, loss: 1.5449384450912476\n",
      "epoch: 41, batch: 19, loss: 1.4761474132537842\n",
      "epoch: 41, batch: 20, loss: 1.5477964878082275\n",
      "epoch: 41, batch: 21, loss: 1.4639580249786377\n",
      "epoch: 41, batch: 22, loss: 1.7217814922332764\n",
      "epoch: 41, batch: 23, loss: 1.4545576572418213\n",
      "epoch: 41, batch: 24, loss: 1.5673792362213135\n",
      "epoch: 41, batch: 25, loss: 1.5852559804916382\n",
      "epoch: 41, batch: 26, loss: 1.8915141820907593\n",
      "epoch: 41, batch: 27, loss: 1.5361112356185913\n",
      "epoch: 41, batch: 28, loss: 1.4171677827835083\n",
      "epoch: 41, batch: 29, loss: 1.4502718448638916\n",
      "epoch: 41, batch: 30, loss: 1.252396821975708\n",
      "epoch: 41, batch: 31, loss: 1.3687900304794312\n",
      "epoch: 41, batch: 32, loss: 1.716487169265747\n",
      "epoch: 41, batch: 33, loss: 1.4644192457199097\n",
      "epoch: 41, batch: 34, loss: 1.6796255111694336\n",
      "epoch: 41, batch: 35, loss: 1.451796293258667\n",
      "epoch: 41, batch: 36, loss: 1.3192636966705322\n",
      "epoch: 41, batch: 37, loss: 1.2422921657562256\n",
      "epoch: 41, batch: 38, loss: 1.6203603744506836\n",
      "epoch: 41, batch: 39, loss: 1.618268609046936\n",
      "epoch: 41, batch: 40, loss: 1.5055301189422607\n",
      "epoch: 41, batch: 41, loss: 1.4289510250091553\n",
      "epoch: 41, batch: 42, loss: 1.4479976892471313\n",
      "epoch: 41, batch: 43, loss: 1.420677900314331\n",
      "epoch: 41, batch: 44, loss: 1.4440909624099731\n",
      "epoch: 41, batch: 45, loss: 1.5858352184295654\n",
      "epoch: 41, batch: 46, loss: 1.6768255233764648\n",
      "epoch: 41, batch: 47, loss: 1.3054145574569702\n",
      "epoch: 41, batch: 48, loss: 1.4611403942108154\n",
      "epoch: 41, batch: 49, loss: 1.3652530908584595\n",
      "epoch: 41, batch: 50, loss: 1.6994578838348389\n",
      "epoch: 41, batch: 51, loss: 1.6910591125488281\n",
      "epoch: 41, batch: 52, loss: 1.7157262563705444\n",
      "epoch: 41, batch: 53, loss: 1.1674907207489014\n",
      "epoch: 41, batch: 54, loss: 1.6571029424667358\n",
      "epoch: 41, batch: 55, loss: 1.7972829341888428\n",
      "epoch: 41, batch: 56, loss: 1.779030442237854\n",
      "epoch: 41, batch: 57, loss: 1.1372274160385132\n",
      "epoch: 41, batch: 58, loss: 1.4219067096710205\n",
      "epoch: 41, batch: 59, loss: 1.6495587825775146\n",
      "epoch: 41, batch: 60, loss: 1.5514012575149536\n",
      "epoch: 41, batch: 61, loss: 1.4344103336334229\n",
      "epoch: 41, batch: 62, loss: 1.580207109451294\n",
      "epoch: 41, batch: 63, loss: 1.1988357305526733\n",
      "epoch: 41, batch: 64, loss: 1.8258415460586548\n",
      "epoch: 41, batch: 65, loss: 1.4614665508270264\n",
      "epoch: 41, batch: 66, loss: 1.3309615850448608\n",
      "epoch: 41, batch: 67, loss: 1.3526294231414795\n",
      "epoch: 41, batch: 68, loss: 1.2732386589050293\n",
      "epoch: 41, batch: 69, loss: 1.6289211511611938\n",
      "epoch: 41, batch: 70, loss: 1.5237512588500977\n",
      "epoch: 41, batch: 71, loss: 1.6090549230575562\n",
      "epoch: 41, batch: 72, loss: 1.335076093673706\n",
      "epoch: 41, batch: 73, loss: 1.7228257656097412\n",
      "epoch: 41, batch: 74, loss: 1.4486815929412842\n",
      "epoch: 41, batch: 75, loss: 1.4509111642837524\n",
      "epoch: 41, batch: 76, loss: 1.4349228143692017\n",
      "epoch: 41, batch: 77, loss: 1.8116304874420166\n",
      "epoch: 41, batch: 78, loss: 1.5028079748153687\n",
      "epoch: 41, batch: 79, loss: 1.8009933233261108\n",
      "epoch: 41, batch: 80, loss: 1.505221962928772\n",
      "epoch: 41, batch: 81, loss: 1.4704488515853882\n",
      "epoch: 41, batch: 82, loss: 1.453049898147583\n",
      "epoch: 41, batch: 83, loss: 1.772709608078003\n",
      "epoch: 41, batch: 84, loss: 1.6435009241104126\n",
      "epoch: 41, batch: 85, loss: 1.5404024124145508\n",
      "epoch: 41, batch: 86, loss: 1.4530932903289795\n",
      "epoch: 41, batch: 87, loss: 1.4797289371490479\n",
      "epoch: 41, batch: 88, loss: 1.8757102489471436\n",
      "epoch: 41, batch: 89, loss: 1.4805715084075928\n",
      "epoch: 41, batch: 90, loss: 1.527597188949585\n",
      "epoch: 41, batch: 91, loss: 1.187774658203125\n",
      "epoch: 41, batch: 92, loss: 1.7258628606796265\n",
      "epoch: 41, batch: 93, loss: 1.3506410121917725\n",
      "epoch: 41, batch: 94, loss: 1.583873987197876\n",
      "epoch: 41, batch: 95, loss: 1.5930712223052979\n",
      "epoch: 41, batch: 96, loss: 1.1722960472106934\n",
      "epoch: 41, batch: 97, loss: 1.5842454433441162\n",
      "epoch: 41, batch: 98, loss: 1.302825927734375\n",
      "epoch: 41, batch: 99, loss: 1.5417048931121826\n",
      "epoch: 41, batch: 100, loss: 1.4955722093582153\n",
      "epoch: 41, batch: 101, loss: 1.5371583700180054\n",
      "epoch: 41, batch: 102, loss: 1.6109189987182617\n",
      "epoch: 41, batch: 103, loss: 1.5525341033935547\n",
      "epoch: 41, batch: 104, loss: 1.6383775472640991\n",
      "epoch: 41, batch: 105, loss: 1.70168936252594\n",
      "epoch: 41, batch: 106, loss: 1.8013938665390015\n",
      "epoch: 41, batch: 107, loss: 1.4428074359893799\n",
      "epoch: 41, batch: 108, loss: 1.646275520324707\n",
      "epoch: 41, batch: 109, loss: 1.396644949913025\n",
      "epoch: 41, batch: 110, loss: 2.002173662185669\n",
      "epoch: 41, batch: 111, loss: 1.418172836303711\n",
      "epoch: 41, batch: 112, loss: 1.57206130027771\n",
      "epoch: 41, batch: 113, loss: 1.744112253189087\n",
      "epoch: 41, batch: 114, loss: 1.5074503421783447\n",
      "epoch: 41, batch: 115, loss: 1.4007759094238281\n",
      "epoch: 41, batch: 116, loss: 1.496490240097046\n",
      "epoch: 41, batch: 117, loss: 1.357932448387146\n",
      "epoch: 41, batch: 118, loss: 1.504439115524292\n",
      "epoch: 41, batch: 119, loss: 1.5303428173065186\n",
      "epoch: 41, batch: 120, loss: 1.7573312520980835\n",
      "epoch: 41, batch: 121, loss: 1.951369285583496\n",
      "epoch: 41, batch: 122, loss: 1.6138980388641357\n",
      "epoch: 41, batch: 123, loss: 1.4560319185256958\n",
      "epoch: 41, batch: 124, loss: 1.3444509506225586\n",
      "epoch: 41, batch: 125, loss: 1.4351252317428589\n",
      "epoch: 41, batch: 126, loss: 1.3109756708145142\n",
      "epoch: 41, batch: 127, loss: 1.5821399688720703\n",
      "epoch: 42, batch: 0, loss: 1.4445521831512451\n",
      "epoch: 42, batch: 1, loss: 1.4800007343292236\n",
      "epoch: 42, batch: 2, loss: 1.210717797279358\n",
      "epoch: 42, batch: 3, loss: 1.5065244436264038\n",
      "epoch: 42, batch: 4, loss: 1.5963494777679443\n",
      "epoch: 42, batch: 5, loss: 1.709656000137329\n",
      "epoch: 42, batch: 6, loss: 1.3264154195785522\n",
      "epoch: 42, batch: 7, loss: 1.655843734741211\n",
      "epoch: 42, batch: 8, loss: 1.7956864833831787\n",
      "epoch: 42, batch: 9, loss: 1.4130382537841797\n",
      "epoch: 42, batch: 10, loss: 1.6729291677474976\n",
      "epoch: 42, batch: 11, loss: 1.4939367771148682\n",
      "epoch: 42, batch: 12, loss: 1.4901401996612549\n",
      "epoch: 42, batch: 13, loss: 1.5324777364730835\n",
      "epoch: 42, batch: 14, loss: 1.9050853252410889\n",
      "epoch: 42, batch: 15, loss: 1.4950616359710693\n",
      "epoch: 42, batch: 16, loss: 1.4191077947616577\n",
      "epoch: 42, batch: 17, loss: 1.370693325996399\n",
      "epoch: 42, batch: 18, loss: 1.6996204853057861\n",
      "epoch: 42, batch: 19, loss: 1.51004958152771\n",
      "epoch: 42, batch: 20, loss: 1.4792702198028564\n",
      "epoch: 42, batch: 21, loss: 1.3782862424850464\n",
      "epoch: 42, batch: 22, loss: 1.6109180450439453\n",
      "epoch: 42, batch: 23, loss: 1.636204481124878\n",
      "epoch: 42, batch: 24, loss: 1.3197004795074463\n",
      "epoch: 42, batch: 25, loss: 1.5597354173660278\n",
      "epoch: 42, batch: 26, loss: 1.557570457458496\n",
      "epoch: 42, batch: 27, loss: 1.5208899974822998\n",
      "epoch: 42, batch: 28, loss: 1.3379768133163452\n",
      "epoch: 42, batch: 29, loss: 1.3328555822372437\n",
      "epoch: 42, batch: 30, loss: 1.503383755683899\n",
      "epoch: 42, batch: 31, loss: 1.6283057928085327\n",
      "epoch: 42, batch: 32, loss: 1.6907012462615967\n",
      "epoch: 42, batch: 33, loss: 1.4568202495574951\n",
      "epoch: 42, batch: 34, loss: 1.5341174602508545\n",
      "epoch: 42, batch: 35, loss: 1.5379064083099365\n",
      "epoch: 42, batch: 36, loss: 1.5696735382080078\n",
      "epoch: 42, batch: 37, loss: 1.5014218091964722\n",
      "epoch: 42, batch: 38, loss: 1.4981541633605957\n",
      "epoch: 42, batch: 39, loss: 1.4691699743270874\n",
      "epoch: 42, batch: 40, loss: 1.457324743270874\n",
      "epoch: 42, batch: 41, loss: 1.7166073322296143\n",
      "epoch: 42, batch: 42, loss: 1.6457210779190063\n",
      "epoch: 42, batch: 43, loss: 1.4944673776626587\n",
      "epoch: 42, batch: 44, loss: 1.4290951490402222\n",
      "epoch: 42, batch: 45, loss: 1.5013498067855835\n",
      "epoch: 42, batch: 46, loss: 1.4023023843765259\n",
      "epoch: 42, batch: 47, loss: 1.3480288982391357\n",
      "epoch: 42, batch: 48, loss: 1.5962896347045898\n",
      "epoch: 42, batch: 49, loss: 1.4567244052886963\n",
      "epoch: 42, batch: 50, loss: 1.7303104400634766\n",
      "epoch: 42, batch: 51, loss: 1.3523173332214355\n",
      "epoch: 42, batch: 52, loss: 1.6783863306045532\n",
      "epoch: 42, batch: 53, loss: 1.4204902648925781\n",
      "epoch: 42, batch: 54, loss: 1.6946712732315063\n",
      "epoch: 42, batch: 55, loss: 1.3869034051895142\n",
      "epoch: 42, batch: 56, loss: 1.3645875453948975\n",
      "epoch: 42, batch: 57, loss: 1.5114381313323975\n",
      "epoch: 42, batch: 58, loss: 1.5807380676269531\n",
      "epoch: 42, batch: 59, loss: 1.7604042291641235\n",
      "epoch: 42, batch: 60, loss: 1.6506249904632568\n",
      "epoch: 42, batch: 61, loss: 1.5434813499450684\n",
      "epoch: 42, batch: 62, loss: 1.6261146068572998\n",
      "epoch: 42, batch: 63, loss: 1.6283413171768188\n",
      "epoch: 42, batch: 64, loss: 1.2864195108413696\n",
      "epoch: 42, batch: 65, loss: 1.6533243656158447\n",
      "epoch: 42, batch: 66, loss: 1.4448003768920898\n",
      "epoch: 42, batch: 67, loss: 1.692051649093628\n",
      "epoch: 42, batch: 68, loss: 1.2397258281707764\n",
      "epoch: 42, batch: 69, loss: 1.516028881072998\n",
      "epoch: 42, batch: 70, loss: 1.6340309381484985\n",
      "epoch: 42, batch: 71, loss: 1.5234827995300293\n",
      "epoch: 42, batch: 72, loss: 1.5392553806304932\n",
      "epoch: 42, batch: 73, loss: 1.4553992748260498\n",
      "epoch: 42, batch: 74, loss: 1.5816248655319214\n",
      "epoch: 42, batch: 75, loss: 1.5939977169036865\n",
      "epoch: 42, batch: 76, loss: 1.79995596408844\n",
      "epoch: 42, batch: 77, loss: 1.5465669631958008\n",
      "epoch: 42, batch: 78, loss: 1.6443582773208618\n",
      "epoch: 42, batch: 79, loss: 1.3607596158981323\n",
      "epoch: 42, batch: 80, loss: 1.2531898021697998\n",
      "epoch: 42, batch: 81, loss: 1.381697177886963\n",
      "epoch: 42, batch: 82, loss: 1.5453189611434937\n",
      "epoch: 42, batch: 83, loss: 1.6134250164031982\n",
      "epoch: 42, batch: 84, loss: 1.2456644773483276\n",
      "epoch: 42, batch: 85, loss: 1.516075849533081\n",
      "epoch: 42, batch: 86, loss: 1.4347693920135498\n",
      "epoch: 42, batch: 87, loss: 1.3610187768936157\n",
      "epoch: 42, batch: 88, loss: 1.7274852991104126\n",
      "epoch: 42, batch: 89, loss: 1.5222523212432861\n",
      "epoch: 42, batch: 90, loss: 1.6388378143310547\n",
      "epoch: 42, batch: 91, loss: 1.7590866088867188\n",
      "epoch: 42, batch: 92, loss: 1.5159298181533813\n",
      "epoch: 42, batch: 93, loss: 1.404739499092102\n",
      "epoch: 42, batch: 94, loss: 1.597311019897461\n",
      "epoch: 42, batch: 95, loss: 1.7205448150634766\n",
      "epoch: 42, batch: 96, loss: 1.6253719329833984\n",
      "epoch: 42, batch: 97, loss: 1.3872243165969849\n",
      "epoch: 42, batch: 98, loss: 1.366255760192871\n",
      "epoch: 42, batch: 99, loss: 1.600640892982483\n",
      "epoch: 42, batch: 100, loss: 1.4110045433044434\n",
      "epoch: 42, batch: 101, loss: 1.5468432903289795\n",
      "epoch: 42, batch: 102, loss: 1.3170459270477295\n",
      "epoch: 42, batch: 103, loss: 1.649629831314087\n",
      "epoch: 42, batch: 104, loss: 1.5092990398406982\n",
      "epoch: 42, batch: 105, loss: 1.4152281284332275\n",
      "epoch: 42, batch: 106, loss: 1.2043672800064087\n",
      "epoch: 42, batch: 107, loss: 1.7295353412628174\n",
      "epoch: 42, batch: 108, loss: 1.6975061893463135\n",
      "epoch: 42, batch: 109, loss: 1.4881327152252197\n",
      "epoch: 42, batch: 110, loss: 1.6192760467529297\n",
      "epoch: 42, batch: 111, loss: 1.6370817422866821\n",
      "epoch: 42, batch: 112, loss: 1.5997363328933716\n",
      "epoch: 42, batch: 113, loss: 1.5121179819107056\n",
      "epoch: 42, batch: 114, loss: 1.8623377084732056\n",
      "epoch: 42, batch: 115, loss: 1.5458905696868896\n",
      "epoch: 42, batch: 116, loss: 1.7117770910263062\n",
      "epoch: 42, batch: 117, loss: 1.6050264835357666\n",
      "epoch: 42, batch: 118, loss: 1.1015700101852417\n",
      "epoch: 42, batch: 119, loss: 1.557896375656128\n",
      "epoch: 42, batch: 120, loss: 1.5899977684020996\n",
      "epoch: 42, batch: 121, loss: 1.4885965585708618\n",
      "epoch: 42, batch: 122, loss: 1.591840386390686\n",
      "epoch: 42, batch: 123, loss: 1.5569602251052856\n",
      "epoch: 42, batch: 124, loss: 1.4720780849456787\n",
      "epoch: 42, batch: 125, loss: 1.6718080043792725\n",
      "epoch: 42, batch: 126, loss: 1.386813998222351\n",
      "epoch: 42, batch: 127, loss: 1.504111409187317\n",
      "epoch: 43, batch: 0, loss: 1.5156551599502563\n",
      "epoch: 43, batch: 1, loss: 1.437164545059204\n",
      "epoch: 43, batch: 2, loss: 1.5150001049041748\n",
      "epoch: 43, batch: 3, loss: 1.6905311346054077\n",
      "epoch: 43, batch: 4, loss: 1.4930967092514038\n",
      "epoch: 43, batch: 5, loss: 1.7543108463287354\n",
      "epoch: 43, batch: 6, loss: 1.7108545303344727\n",
      "epoch: 43, batch: 7, loss: 1.7686021327972412\n",
      "epoch: 43, batch: 8, loss: 1.333223581314087\n",
      "epoch: 43, batch: 9, loss: 1.5012943744659424\n",
      "epoch: 43, batch: 10, loss: 1.2415025234222412\n",
      "epoch: 43, batch: 11, loss: 1.4538804292678833\n",
      "epoch: 43, batch: 12, loss: 1.4138157367706299\n",
      "epoch: 43, batch: 13, loss: 1.7298400402069092\n",
      "epoch: 43, batch: 14, loss: 1.5624818801879883\n",
      "epoch: 43, batch: 15, loss: 1.462448000907898\n",
      "epoch: 43, batch: 16, loss: 1.6180408000946045\n",
      "epoch: 43, batch: 17, loss: 1.6591746807098389\n",
      "epoch: 43, batch: 18, loss: 1.6366775035858154\n",
      "epoch: 43, batch: 19, loss: 1.4772450923919678\n",
      "epoch: 43, batch: 20, loss: 1.392698884010315\n",
      "epoch: 43, batch: 21, loss: 1.0654690265655518\n",
      "epoch: 43, batch: 22, loss: 1.5503437519073486\n",
      "epoch: 43, batch: 23, loss: 1.569440484046936\n",
      "epoch: 43, batch: 24, loss: 1.7092745304107666\n",
      "epoch: 43, batch: 25, loss: 1.368027925491333\n",
      "epoch: 43, batch: 26, loss: 1.691361665725708\n",
      "epoch: 43, batch: 27, loss: 1.3617092370986938\n",
      "epoch: 43, batch: 28, loss: 1.5877482891082764\n",
      "epoch: 43, batch: 29, loss: 1.4712438583374023\n",
      "epoch: 43, batch: 30, loss: 1.4780211448669434\n",
      "epoch: 43, batch: 31, loss: 1.559816598892212\n",
      "epoch: 43, batch: 32, loss: 1.418856143951416\n",
      "epoch: 43, batch: 33, loss: 1.7873783111572266\n",
      "epoch: 43, batch: 34, loss: 1.5476889610290527\n",
      "epoch: 43, batch: 35, loss: 1.6069904565811157\n",
      "epoch: 43, batch: 36, loss: 1.6533416509628296\n",
      "epoch: 43, batch: 37, loss: 1.4093197584152222\n",
      "epoch: 43, batch: 38, loss: 1.6151330471038818\n",
      "epoch: 43, batch: 39, loss: 1.6539252996444702\n",
      "epoch: 43, batch: 40, loss: 1.319604754447937\n",
      "epoch: 43, batch: 41, loss: 1.442166805267334\n",
      "epoch: 43, batch: 42, loss: 1.6509956121444702\n",
      "epoch: 43, batch: 43, loss: 1.2801004648208618\n",
      "epoch: 43, batch: 44, loss: 1.5613062381744385\n",
      "epoch: 43, batch: 45, loss: 1.4033231735229492\n",
      "epoch: 43, batch: 46, loss: 1.3105673789978027\n",
      "epoch: 43, batch: 47, loss: 1.7224359512329102\n",
      "epoch: 43, batch: 48, loss: 1.2910441160202026\n",
      "epoch: 43, batch: 49, loss: 1.3891428709030151\n",
      "epoch: 43, batch: 50, loss: 1.5272588729858398\n",
      "epoch: 43, batch: 51, loss: 1.1949729919433594\n",
      "epoch: 43, batch: 52, loss: 1.542560338973999\n",
      "epoch: 43, batch: 53, loss: 1.1612857580184937\n",
      "epoch: 43, batch: 54, loss: 1.6032726764678955\n",
      "epoch: 43, batch: 55, loss: 1.6525862216949463\n",
      "epoch: 43, batch: 56, loss: 1.24980890750885\n",
      "epoch: 43, batch: 57, loss: 1.487247109413147\n",
      "epoch: 43, batch: 58, loss: 1.408071517944336\n",
      "epoch: 43, batch: 59, loss: 1.1780105829238892\n",
      "epoch: 43, batch: 60, loss: 1.4035745859146118\n",
      "epoch: 43, batch: 61, loss: 1.5988792181015015\n",
      "epoch: 43, batch: 62, loss: 1.9030206203460693\n",
      "epoch: 43, batch: 63, loss: 1.4249298572540283\n",
      "epoch: 43, batch: 64, loss: 1.2952772378921509\n",
      "epoch: 43, batch: 65, loss: 1.7256543636322021\n",
      "epoch: 43, batch: 66, loss: 1.6886322498321533\n",
      "epoch: 43, batch: 67, loss: 1.6546156406402588\n",
      "epoch: 43, batch: 68, loss: 1.6430031061172485\n",
      "epoch: 43, batch: 69, loss: 1.8162568807601929\n",
      "epoch: 43, batch: 70, loss: 1.5890617370605469\n",
      "epoch: 43, batch: 71, loss: 1.3196704387664795\n",
      "epoch: 43, batch: 72, loss: 1.7364680767059326\n",
      "epoch: 43, batch: 73, loss: 1.6120163202285767\n",
      "epoch: 43, batch: 74, loss: 1.4667713642120361\n",
      "epoch: 43, batch: 75, loss: 1.4228228330612183\n",
      "epoch: 43, batch: 76, loss: 1.439131498336792\n",
      "epoch: 43, batch: 77, loss: 1.5724799633026123\n",
      "epoch: 43, batch: 78, loss: 1.3675140142440796\n",
      "epoch: 43, batch: 79, loss: 1.3867491483688354\n",
      "epoch: 43, batch: 80, loss: 1.6583735942840576\n",
      "epoch: 43, batch: 81, loss: 1.7875490188598633\n",
      "epoch: 43, batch: 82, loss: 1.7322075366973877\n",
      "epoch: 43, batch: 83, loss: 1.6285922527313232\n",
      "epoch: 43, batch: 84, loss: 1.4980167150497437\n",
      "epoch: 43, batch: 85, loss: 1.4695045948028564\n",
      "epoch: 43, batch: 86, loss: 1.5256798267364502\n",
      "epoch: 43, batch: 87, loss: 1.8123422861099243\n",
      "epoch: 43, batch: 88, loss: 1.5148805379867554\n",
      "epoch: 43, batch: 89, loss: 1.560887098312378\n",
      "epoch: 43, batch: 90, loss: 1.855480432510376\n",
      "epoch: 43, batch: 91, loss: 1.3681920766830444\n",
      "epoch: 43, batch: 92, loss: 1.3219890594482422\n",
      "epoch: 43, batch: 93, loss: 1.5578422546386719\n",
      "epoch: 43, batch: 94, loss: 1.4132416248321533\n",
      "epoch: 43, batch: 95, loss: 1.6890907287597656\n",
      "epoch: 43, batch: 96, loss: 1.402141809463501\n",
      "epoch: 43, batch: 97, loss: 1.5770044326782227\n",
      "epoch: 43, batch: 98, loss: 1.5465638637542725\n",
      "epoch: 43, batch: 99, loss: 1.6889030933380127\n",
      "epoch: 43, batch: 100, loss: 1.6673622131347656\n",
      "epoch: 43, batch: 101, loss: 1.4166109561920166\n",
      "epoch: 43, batch: 102, loss: 1.4770095348358154\n",
      "epoch: 43, batch: 103, loss: 1.5374552011489868\n",
      "epoch: 43, batch: 104, loss: 1.5993132591247559\n",
      "epoch: 43, batch: 105, loss: 1.7243207693099976\n",
      "epoch: 43, batch: 106, loss: 1.4584972858428955\n",
      "epoch: 43, batch: 107, loss: 1.6931276321411133\n",
      "epoch: 43, batch: 108, loss: 1.690840721130371\n",
      "epoch: 43, batch: 109, loss: 1.6533668041229248\n",
      "epoch: 43, batch: 110, loss: 1.3444099426269531\n",
      "epoch: 43, batch: 111, loss: 1.3796297311782837\n",
      "epoch: 43, batch: 112, loss: 1.6894371509552002\n",
      "epoch: 43, batch: 113, loss: 1.6376914978027344\n",
      "epoch: 43, batch: 114, loss: 1.2922532558441162\n",
      "epoch: 43, batch: 115, loss: 1.64593505859375\n",
      "epoch: 43, batch: 116, loss: 1.434609055519104\n",
      "epoch: 43, batch: 117, loss: 1.6833432912826538\n",
      "epoch: 43, batch: 118, loss: 1.1060818433761597\n",
      "epoch: 43, batch: 119, loss: 1.666852593421936\n",
      "epoch: 43, batch: 120, loss: 1.8452907800674438\n",
      "epoch: 43, batch: 121, loss: 1.1760399341583252\n",
      "epoch: 43, batch: 122, loss: 1.6032766103744507\n",
      "epoch: 43, batch: 123, loss: 1.4626528024673462\n",
      "epoch: 43, batch: 124, loss: 1.5588860511779785\n",
      "epoch: 43, batch: 125, loss: 1.2195693254470825\n",
      "epoch: 43, batch: 126, loss: 1.4786016941070557\n",
      "epoch: 43, batch: 127, loss: 1.550864338874817\n",
      "epoch: 44, batch: 0, loss: 1.7707704305648804\n",
      "epoch: 44, batch: 1, loss: 1.4519277811050415\n",
      "epoch: 44, batch: 2, loss: 1.5447717905044556\n",
      "epoch: 44, batch: 3, loss: 1.7660331726074219\n",
      "epoch: 44, batch: 4, loss: 1.6198558807373047\n",
      "epoch: 44, batch: 5, loss: 1.5523731708526611\n",
      "epoch: 44, batch: 6, loss: 1.7810032367706299\n",
      "epoch: 44, batch: 7, loss: 1.155490517616272\n",
      "epoch: 44, batch: 8, loss: 1.6996341943740845\n",
      "epoch: 44, batch: 9, loss: 1.8917930126190186\n",
      "epoch: 44, batch: 10, loss: 1.7060987949371338\n",
      "epoch: 44, batch: 11, loss: 1.7499885559082031\n",
      "epoch: 44, batch: 12, loss: 1.208498239517212\n",
      "epoch: 44, batch: 13, loss: 1.5647478103637695\n",
      "epoch: 44, batch: 14, loss: 1.7384201288223267\n",
      "epoch: 44, batch: 15, loss: 1.8746229410171509\n",
      "epoch: 44, batch: 16, loss: 1.967711091041565\n",
      "epoch: 44, batch: 17, loss: 1.4523555040359497\n",
      "epoch: 44, batch: 18, loss: 1.1186163425445557\n",
      "epoch: 44, batch: 19, loss: 1.5765701532363892\n",
      "epoch: 44, batch: 20, loss: 1.4494268894195557\n",
      "epoch: 44, batch: 21, loss: 1.1849520206451416\n",
      "epoch: 44, batch: 22, loss: 1.7575604915618896\n",
      "epoch: 44, batch: 23, loss: 1.6616075038909912\n",
      "epoch: 44, batch: 24, loss: 1.8472850322723389\n",
      "epoch: 44, batch: 25, loss: 1.471828579902649\n",
      "epoch: 44, batch: 26, loss: 1.165048599243164\n",
      "epoch: 44, batch: 27, loss: 1.130916953086853\n",
      "epoch: 44, batch: 28, loss: 1.279374122619629\n",
      "epoch: 44, batch: 29, loss: 1.3761396408081055\n",
      "epoch: 44, batch: 30, loss: 1.6955007314682007\n",
      "epoch: 44, batch: 31, loss: 1.5442323684692383\n",
      "epoch: 44, batch: 32, loss: 2.001570224761963\n",
      "epoch: 44, batch: 33, loss: 1.6044838428497314\n",
      "epoch: 44, batch: 34, loss: 1.229622721672058\n",
      "epoch: 44, batch: 35, loss: 1.5935581922531128\n",
      "epoch: 44, batch: 36, loss: 1.6143172979354858\n",
      "epoch: 44, batch: 37, loss: 1.506015419960022\n",
      "epoch: 44, batch: 38, loss: 1.8253955841064453\n",
      "epoch: 44, batch: 39, loss: 1.3351809978485107\n",
      "epoch: 44, batch: 40, loss: 1.4938808679580688\n",
      "epoch: 44, batch: 41, loss: 1.6446670293807983\n",
      "epoch: 44, batch: 42, loss: 1.352595567703247\n",
      "epoch: 44, batch: 43, loss: 1.2734959125518799\n",
      "epoch: 44, batch: 44, loss: 1.5321954488754272\n",
      "epoch: 44, batch: 45, loss: 1.340409517288208\n",
      "epoch: 44, batch: 46, loss: 1.4090931415557861\n",
      "epoch: 44, batch: 47, loss: 1.4655647277832031\n",
      "epoch: 44, batch: 48, loss: 1.3472440242767334\n",
      "epoch: 44, batch: 49, loss: 1.741145372390747\n",
      "epoch: 44, batch: 50, loss: 1.3613431453704834\n",
      "epoch: 44, batch: 51, loss: 1.5197550058364868\n",
      "epoch: 44, batch: 52, loss: 1.5810977220535278\n",
      "epoch: 44, batch: 53, loss: 1.478589415550232\n",
      "epoch: 44, batch: 54, loss: 1.403740406036377\n",
      "epoch: 44, batch: 55, loss: 1.5915899276733398\n",
      "epoch: 44, batch: 56, loss: 1.6154489517211914\n",
      "epoch: 44, batch: 57, loss: 1.6260898113250732\n",
      "epoch: 44, batch: 58, loss: 1.647587776184082\n",
      "epoch: 44, batch: 59, loss: 1.457358956336975\n",
      "epoch: 44, batch: 60, loss: 1.553226351737976\n",
      "epoch: 44, batch: 61, loss: 1.569737195968628\n",
      "epoch: 44, batch: 62, loss: 1.4494976997375488\n",
      "epoch: 44, batch: 63, loss: 1.5785109996795654\n",
      "epoch: 44, batch: 64, loss: 1.9510462284088135\n",
      "epoch: 44, batch: 65, loss: 1.0874173641204834\n",
      "epoch: 44, batch: 66, loss: 1.300036072731018\n",
      "epoch: 44, batch: 67, loss: 1.2853810787200928\n",
      "epoch: 44, batch: 68, loss: 1.4881398677825928\n",
      "epoch: 44, batch: 69, loss: 1.7146011590957642\n",
      "epoch: 44, batch: 70, loss: 1.4209611415863037\n",
      "epoch: 44, batch: 71, loss: 1.4365875720977783\n",
      "epoch: 44, batch: 72, loss: 1.3377649784088135\n",
      "epoch: 44, batch: 73, loss: 1.4576795101165771\n",
      "epoch: 44, batch: 74, loss: 1.7999866008758545\n",
      "epoch: 44, batch: 75, loss: 1.8040145635604858\n",
      "epoch: 44, batch: 76, loss: 1.0896552801132202\n",
      "epoch: 44, batch: 77, loss: 1.6306438446044922\n",
      "epoch: 44, batch: 78, loss: 1.6363086700439453\n",
      "epoch: 44, batch: 79, loss: 1.2389096021652222\n",
      "epoch: 44, batch: 80, loss: 1.4768640995025635\n",
      "epoch: 44, batch: 81, loss: 1.5328484773635864\n",
      "epoch: 44, batch: 82, loss: 1.0688384771347046\n",
      "epoch: 44, batch: 83, loss: 1.250128984451294\n",
      "epoch: 44, batch: 84, loss: 1.7945940494537354\n",
      "epoch: 44, batch: 85, loss: 1.8097912073135376\n",
      "epoch: 44, batch: 86, loss: 1.4802659749984741\n",
      "epoch: 44, batch: 87, loss: 1.607216238975525\n",
      "epoch: 44, batch: 88, loss: 1.4138295650482178\n",
      "epoch: 44, batch: 89, loss: 1.7758795022964478\n",
      "epoch: 44, batch: 90, loss: 1.4195802211761475\n",
      "epoch: 44, batch: 91, loss: 1.4239683151245117\n",
      "epoch: 44, batch: 92, loss: 1.2156306505203247\n",
      "epoch: 44, batch: 93, loss: 1.9265152215957642\n",
      "epoch: 44, batch: 94, loss: 1.4667952060699463\n",
      "epoch: 44, batch: 95, loss: 1.4044946432113647\n",
      "epoch: 44, batch: 96, loss: 1.4141533374786377\n",
      "epoch: 44, batch: 97, loss: 1.6061958074569702\n",
      "epoch: 44, batch: 98, loss: 1.9771697521209717\n",
      "epoch: 44, batch: 99, loss: 1.7802009582519531\n",
      "epoch: 44, batch: 100, loss: 2.0017523765563965\n",
      "epoch: 44, batch: 101, loss: 1.7061054706573486\n",
      "epoch: 44, batch: 102, loss: 1.651426911354065\n",
      "epoch: 44, batch: 103, loss: 1.6904484033584595\n",
      "epoch: 44, batch: 104, loss: 1.4099410772323608\n",
      "epoch: 44, batch: 105, loss: 1.4474210739135742\n",
      "epoch: 44, batch: 106, loss: 1.1972911357879639\n",
      "epoch: 44, batch: 107, loss: 1.2877538204193115\n",
      "epoch: 44, batch: 108, loss: 1.5649102926254272\n",
      "epoch: 44, batch: 109, loss: 1.5064480304718018\n",
      "epoch: 44, batch: 110, loss: 1.2989168167114258\n",
      "epoch: 44, batch: 111, loss: 2.130861520767212\n",
      "epoch: 44, batch: 112, loss: 1.8703800439834595\n",
      "epoch: 44, batch: 113, loss: 1.60871160030365\n",
      "epoch: 44, batch: 114, loss: 1.7296139001846313\n",
      "epoch: 44, batch: 115, loss: 1.5791255235671997\n",
      "epoch: 44, batch: 116, loss: 1.6196292638778687\n",
      "epoch: 44, batch: 117, loss: 1.560421347618103\n",
      "epoch: 44, batch: 118, loss: 1.5971935987472534\n",
      "epoch: 44, batch: 119, loss: 1.3138372898101807\n",
      "epoch: 44, batch: 120, loss: 1.1501628160476685\n",
      "epoch: 44, batch: 121, loss: 1.2943947315216064\n",
      "epoch: 44, batch: 122, loss: 1.5359388589859009\n",
      "epoch: 44, batch: 123, loss: 1.4945995807647705\n",
      "epoch: 44, batch: 124, loss: 1.4809767007827759\n",
      "epoch: 44, batch: 125, loss: 1.4467285871505737\n",
      "epoch: 44, batch: 126, loss: 1.299154281616211\n",
      "epoch: 44, batch: 127, loss: 1.326815128326416\n",
      "epoch: 45, batch: 0, loss: 1.8364832401275635\n",
      "epoch: 45, batch: 1, loss: 1.6271041631698608\n",
      "epoch: 45, batch: 2, loss: 1.4482176303863525\n",
      "epoch: 45, batch: 3, loss: 1.8843733072280884\n",
      "epoch: 45, batch: 4, loss: 1.3685417175292969\n",
      "epoch: 45, batch: 5, loss: 1.3226990699768066\n",
      "epoch: 45, batch: 6, loss: 1.846545934677124\n",
      "epoch: 45, batch: 7, loss: 1.6697723865509033\n",
      "epoch: 45, batch: 8, loss: 1.6099449396133423\n",
      "epoch: 45, batch: 9, loss: 1.202028512954712\n",
      "epoch: 45, batch: 10, loss: 1.5737299919128418\n",
      "epoch: 45, batch: 11, loss: 1.3992027044296265\n",
      "epoch: 45, batch: 12, loss: 1.5294116735458374\n",
      "epoch: 45, batch: 13, loss: 1.784925103187561\n",
      "epoch: 45, batch: 14, loss: 1.44259774684906\n",
      "epoch: 45, batch: 15, loss: 1.6422412395477295\n",
      "epoch: 45, batch: 16, loss: 1.255226969718933\n",
      "epoch: 45, batch: 17, loss: 1.7509632110595703\n",
      "epoch: 45, batch: 18, loss: 1.1502691507339478\n",
      "epoch: 45, batch: 19, loss: 1.1454527378082275\n",
      "epoch: 45, batch: 20, loss: 1.5685813426971436\n",
      "epoch: 45, batch: 21, loss: 1.4999234676361084\n",
      "epoch: 45, batch: 22, loss: 1.4250986576080322\n",
      "epoch: 45, batch: 23, loss: 1.5280468463897705\n",
      "epoch: 45, batch: 24, loss: 1.4512803554534912\n",
      "epoch: 45, batch: 25, loss: 1.4964826107025146\n",
      "epoch: 45, batch: 26, loss: 1.7143665552139282\n",
      "epoch: 45, batch: 27, loss: 1.6215274333953857\n",
      "epoch: 45, batch: 28, loss: 1.2744284868240356\n",
      "epoch: 45, batch: 29, loss: 1.1002196073532104\n",
      "epoch: 45, batch: 30, loss: 1.4564869403839111\n",
      "epoch: 45, batch: 31, loss: 1.3961116075515747\n",
      "epoch: 45, batch: 32, loss: 1.5328428745269775\n",
      "epoch: 45, batch: 33, loss: 1.4171578884124756\n",
      "epoch: 45, batch: 34, loss: 1.4584903717041016\n",
      "epoch: 45, batch: 35, loss: 1.8010591268539429\n",
      "epoch: 45, batch: 36, loss: 1.422628402709961\n",
      "epoch: 45, batch: 37, loss: 1.7873070240020752\n",
      "epoch: 45, batch: 38, loss: 1.709416151046753\n",
      "epoch: 45, batch: 39, loss: 1.755171537399292\n",
      "epoch: 45, batch: 40, loss: 1.6928911209106445\n",
      "epoch: 45, batch: 41, loss: 1.3246688842773438\n",
      "epoch: 45, batch: 42, loss: 1.329502820968628\n",
      "epoch: 45, batch: 43, loss: 1.5409910678863525\n",
      "epoch: 45, batch: 44, loss: 1.628864049911499\n",
      "epoch: 45, batch: 45, loss: 1.5555195808410645\n",
      "epoch: 45, batch: 46, loss: 1.318448781967163\n",
      "epoch: 45, batch: 47, loss: 1.2854567766189575\n",
      "epoch: 45, batch: 48, loss: 1.229229211807251\n",
      "epoch: 45, batch: 49, loss: 1.7381694316864014\n",
      "epoch: 45, batch: 50, loss: 1.4953781366348267\n",
      "epoch: 45, batch: 51, loss: 1.559086561203003\n",
      "epoch: 45, batch: 52, loss: 1.676741600036621\n",
      "epoch: 45, batch: 53, loss: 1.5644748210906982\n",
      "epoch: 45, batch: 54, loss: 1.7176144123077393\n",
      "epoch: 45, batch: 55, loss: 1.4963605403900146\n",
      "epoch: 45, batch: 56, loss: 1.1977906227111816\n",
      "epoch: 45, batch: 57, loss: 1.501680850982666\n",
      "epoch: 45, batch: 58, loss: 1.5505057573318481\n",
      "epoch: 45, batch: 59, loss: 1.5256354808807373\n",
      "epoch: 45, batch: 60, loss: 1.4355487823486328\n",
      "epoch: 45, batch: 61, loss: 1.670676827430725\n",
      "epoch: 45, batch: 62, loss: 1.7012803554534912\n",
      "epoch: 45, batch: 63, loss: 1.3649476766586304\n",
      "epoch: 45, batch: 64, loss: 1.450029969215393\n",
      "epoch: 45, batch: 65, loss: 1.3593323230743408\n",
      "epoch: 45, batch: 66, loss: 1.6381416320800781\n",
      "epoch: 45, batch: 67, loss: 1.5735853910446167\n",
      "epoch: 45, batch: 68, loss: 1.6686763763427734\n",
      "epoch: 45, batch: 69, loss: 1.5995455980300903\n",
      "epoch: 45, batch: 70, loss: 1.4438556432724\n",
      "epoch: 45, batch: 71, loss: 1.487974762916565\n",
      "epoch: 45, batch: 72, loss: 1.5351797342300415\n",
      "epoch: 45, batch: 73, loss: 1.236844778060913\n",
      "epoch: 45, batch: 74, loss: 1.6849479675292969\n",
      "epoch: 45, batch: 75, loss: 1.530393123626709\n",
      "epoch: 45, batch: 76, loss: 1.6029411554336548\n",
      "epoch: 45, batch: 77, loss: 1.4421637058258057\n",
      "epoch: 45, batch: 78, loss: 1.803788423538208\n",
      "epoch: 45, batch: 79, loss: 1.7478973865509033\n",
      "epoch: 45, batch: 80, loss: 1.275031566619873\n",
      "epoch: 45, batch: 81, loss: 1.6662193536758423\n",
      "epoch: 45, batch: 82, loss: 1.3743083477020264\n",
      "epoch: 45, batch: 83, loss: 1.3545658588409424\n",
      "epoch: 45, batch: 84, loss: 1.3412904739379883\n",
      "epoch: 45, batch: 85, loss: 1.3261407613754272\n",
      "epoch: 45, batch: 86, loss: 1.2670154571533203\n",
      "epoch: 45, batch: 87, loss: 1.7112979888916016\n",
      "epoch: 45, batch: 88, loss: 1.4697322845458984\n",
      "epoch: 45, batch: 89, loss: 1.685925841331482\n",
      "epoch: 45, batch: 90, loss: 1.5035604238510132\n",
      "epoch: 45, batch: 91, loss: 1.4798825979232788\n",
      "epoch: 45, batch: 92, loss: 1.5918468236923218\n",
      "epoch: 45, batch: 93, loss: 1.6191368103027344\n",
      "epoch: 45, batch: 94, loss: 1.443341612815857\n",
      "epoch: 45, batch: 95, loss: 1.863242506980896\n",
      "epoch: 45, batch: 96, loss: 1.4059823751449585\n",
      "epoch: 45, batch: 97, loss: 1.3404083251953125\n",
      "epoch: 45, batch: 98, loss: 1.7805442810058594\n",
      "epoch: 45, batch: 99, loss: 1.2442213296890259\n",
      "epoch: 45, batch: 100, loss: 1.669456124305725\n",
      "epoch: 45, batch: 101, loss: 1.7852039337158203\n",
      "epoch: 45, batch: 102, loss: 1.3858585357666016\n",
      "epoch: 45, batch: 103, loss: 1.4890912771224976\n",
      "epoch: 45, batch: 104, loss: 1.2248438596725464\n",
      "epoch: 45, batch: 105, loss: 1.679718017578125\n",
      "epoch: 45, batch: 106, loss: 1.7708158493041992\n",
      "epoch: 45, batch: 107, loss: 1.540880560874939\n",
      "epoch: 45, batch: 108, loss: 1.5514317750930786\n",
      "epoch: 45, batch: 109, loss: 1.598032832145691\n",
      "epoch: 45, batch: 110, loss: 1.7521164417266846\n",
      "epoch: 45, batch: 111, loss: 1.5474213361740112\n",
      "epoch: 45, batch: 112, loss: 1.737317681312561\n",
      "epoch: 45, batch: 113, loss: 1.3904110193252563\n",
      "epoch: 45, batch: 114, loss: 1.3767436742782593\n",
      "epoch: 45, batch: 115, loss: 1.330153465270996\n",
      "epoch: 45, batch: 116, loss: 1.4878754615783691\n",
      "epoch: 45, batch: 117, loss: 1.2339211702346802\n",
      "epoch: 45, batch: 118, loss: 1.6028430461883545\n",
      "epoch: 45, batch: 119, loss: 1.6263688802719116\n",
      "epoch: 45, batch: 120, loss: 1.539700984954834\n",
      "epoch: 45, batch: 121, loss: 1.4064124822616577\n",
      "epoch: 45, batch: 122, loss: 1.8102054595947266\n",
      "epoch: 45, batch: 123, loss: 1.7288610935211182\n",
      "epoch: 45, batch: 124, loss: 1.8760229349136353\n",
      "epoch: 45, batch: 125, loss: 1.6527392864227295\n",
      "epoch: 45, batch: 126, loss: 1.3451316356658936\n",
      "epoch: 45, batch: 127, loss: 1.7053978443145752\n",
      "epoch: 46, batch: 0, loss: 1.4344508647918701\n",
      "epoch: 46, batch: 1, loss: 1.5358574390411377\n",
      "epoch: 46, batch: 2, loss: 1.4389384984970093\n",
      "epoch: 46, batch: 3, loss: 1.5284839868545532\n",
      "epoch: 46, batch: 4, loss: 1.7116057872772217\n",
      "epoch: 46, batch: 5, loss: 1.6753361225128174\n",
      "epoch: 46, batch: 6, loss: 1.4411379098892212\n",
      "epoch: 46, batch: 7, loss: 1.6631736755371094\n",
      "epoch: 46, batch: 8, loss: 1.6166260242462158\n",
      "epoch: 46, batch: 9, loss: 1.5146093368530273\n",
      "epoch: 46, batch: 10, loss: 1.7217309474945068\n",
      "epoch: 46, batch: 11, loss: 1.5417063236236572\n",
      "epoch: 46, batch: 12, loss: 1.5356590747833252\n",
      "epoch: 46, batch: 13, loss: 1.6976398229599\n",
      "epoch: 46, batch: 14, loss: 1.5242669582366943\n",
      "epoch: 46, batch: 15, loss: 1.4389731884002686\n",
      "epoch: 46, batch: 16, loss: 1.2617864608764648\n",
      "epoch: 46, batch: 17, loss: 1.6025092601776123\n",
      "epoch: 46, batch: 18, loss: 1.6544334888458252\n",
      "epoch: 46, batch: 19, loss: 1.4803016185760498\n",
      "epoch: 46, batch: 20, loss: 1.651864767074585\n",
      "epoch: 46, batch: 21, loss: 1.2871620655059814\n",
      "epoch: 46, batch: 22, loss: 1.6549421548843384\n",
      "epoch: 46, batch: 23, loss: 1.378645658493042\n",
      "epoch: 46, batch: 24, loss: 1.226811170578003\n",
      "epoch: 46, batch: 25, loss: 1.3452945947647095\n",
      "epoch: 46, batch: 26, loss: 1.333909273147583\n",
      "epoch: 46, batch: 27, loss: 1.3325237035751343\n",
      "epoch: 46, batch: 28, loss: 1.506075143814087\n",
      "epoch: 46, batch: 29, loss: 1.540970802307129\n",
      "epoch: 46, batch: 30, loss: 1.5567655563354492\n",
      "epoch: 46, batch: 31, loss: 1.5987569093704224\n",
      "epoch: 46, batch: 32, loss: 1.56558096408844\n",
      "epoch: 46, batch: 33, loss: 1.4390990734100342\n",
      "epoch: 46, batch: 34, loss: 1.7807403802871704\n",
      "epoch: 46, batch: 35, loss: 1.5369116067886353\n",
      "epoch: 46, batch: 36, loss: 1.5396486520767212\n",
      "epoch: 46, batch: 37, loss: 1.5198837518692017\n",
      "epoch: 46, batch: 38, loss: 1.3885008096694946\n",
      "epoch: 46, batch: 39, loss: 1.6393768787384033\n",
      "epoch: 46, batch: 40, loss: 1.4562695026397705\n",
      "epoch: 46, batch: 41, loss: 1.4559800624847412\n",
      "epoch: 46, batch: 42, loss: 1.4806588888168335\n",
      "epoch: 46, batch: 43, loss: 1.798835039138794\n",
      "epoch: 46, batch: 44, loss: 1.477482795715332\n",
      "epoch: 46, batch: 45, loss: 1.4419893026351929\n",
      "epoch: 46, batch: 46, loss: 1.322957992553711\n",
      "epoch: 46, batch: 47, loss: 1.4351123571395874\n",
      "epoch: 46, batch: 48, loss: 1.6976760625839233\n",
      "epoch: 46, batch: 49, loss: 1.5191541910171509\n",
      "epoch: 46, batch: 50, loss: 1.6690155267715454\n",
      "epoch: 46, batch: 51, loss: 1.4425989389419556\n",
      "epoch: 46, batch: 52, loss: 1.2265326976776123\n",
      "epoch: 46, batch: 53, loss: 1.6606098413467407\n",
      "epoch: 46, batch: 54, loss: 1.6467090845108032\n",
      "epoch: 46, batch: 55, loss: 1.7417205572128296\n",
      "epoch: 46, batch: 56, loss: 1.5185303688049316\n",
      "epoch: 46, batch: 57, loss: 1.6841741800308228\n",
      "epoch: 46, batch: 58, loss: 1.2094178199768066\n",
      "epoch: 46, batch: 59, loss: 1.3936121463775635\n",
      "epoch: 46, batch: 60, loss: 1.5788781642913818\n",
      "epoch: 46, batch: 61, loss: 1.5865195989608765\n",
      "epoch: 46, batch: 62, loss: 1.5923179388046265\n",
      "epoch: 46, batch: 63, loss: 1.846294641494751\n",
      "epoch: 46, batch: 64, loss: 1.6338199377059937\n",
      "epoch: 46, batch: 65, loss: 1.4892960786819458\n",
      "epoch: 46, batch: 66, loss: 1.4080655574798584\n",
      "epoch: 46, batch: 67, loss: 1.3298978805541992\n",
      "epoch: 46, batch: 68, loss: 1.4472734928131104\n",
      "epoch: 46, batch: 69, loss: 1.882671594619751\n",
      "epoch: 46, batch: 70, loss: 1.3568063974380493\n",
      "epoch: 46, batch: 71, loss: 1.8063064813613892\n",
      "epoch: 46, batch: 72, loss: 1.701403260231018\n",
      "epoch: 46, batch: 73, loss: 1.3534828424453735\n",
      "epoch: 46, batch: 74, loss: 1.3875198364257812\n",
      "epoch: 46, batch: 75, loss: 1.3862727880477905\n",
      "epoch: 46, batch: 76, loss: 1.669019103050232\n",
      "epoch: 46, batch: 77, loss: 1.4846854209899902\n",
      "epoch: 46, batch: 78, loss: 1.6031898260116577\n",
      "epoch: 46, batch: 79, loss: 1.3718783855438232\n",
      "epoch: 46, batch: 80, loss: 1.5536843538284302\n",
      "epoch: 46, batch: 81, loss: 1.7978156805038452\n",
      "epoch: 46, batch: 82, loss: 1.7188701629638672\n",
      "epoch: 46, batch: 83, loss: 1.3798730373382568\n",
      "epoch: 46, batch: 84, loss: 1.2151572704315186\n",
      "epoch: 46, batch: 85, loss: 1.4797601699829102\n",
      "epoch: 46, batch: 86, loss: 1.636493444442749\n",
      "epoch: 46, batch: 87, loss: 1.756832480430603\n",
      "epoch: 46, batch: 88, loss: 1.3693251609802246\n",
      "epoch: 46, batch: 89, loss: 1.531381368637085\n",
      "epoch: 46, batch: 90, loss: 1.730194091796875\n",
      "epoch: 46, batch: 91, loss: 1.5953983068466187\n",
      "epoch: 46, batch: 92, loss: 1.5814861059188843\n",
      "epoch: 46, batch: 93, loss: 1.6350568532943726\n",
      "epoch: 46, batch: 94, loss: 1.6765094995498657\n",
      "epoch: 46, batch: 95, loss: 1.239069938659668\n",
      "epoch: 46, batch: 96, loss: 1.5443416833877563\n",
      "epoch: 46, batch: 97, loss: 1.4869405031204224\n",
      "epoch: 46, batch: 98, loss: 1.549221158027649\n",
      "epoch: 46, batch: 99, loss: 1.5905095338821411\n",
      "epoch: 46, batch: 100, loss: 1.3868685960769653\n",
      "epoch: 46, batch: 101, loss: 1.1950761079788208\n",
      "epoch: 46, batch: 102, loss: 1.5862915515899658\n",
      "epoch: 46, batch: 103, loss: 1.771418809890747\n",
      "epoch: 46, batch: 104, loss: 1.5228841304779053\n",
      "epoch: 46, batch: 105, loss: 1.4130476713180542\n",
      "epoch: 46, batch: 106, loss: 1.5422592163085938\n",
      "epoch: 46, batch: 107, loss: 1.407559871673584\n",
      "epoch: 46, batch: 108, loss: 1.3032861948013306\n",
      "epoch: 46, batch: 109, loss: 1.7694381475448608\n",
      "epoch: 46, batch: 110, loss: 1.2161957025527954\n",
      "epoch: 46, batch: 111, loss: 1.8437410593032837\n",
      "epoch: 46, batch: 112, loss: 1.749446153640747\n",
      "epoch: 46, batch: 113, loss: 1.467415690422058\n",
      "epoch: 46, batch: 114, loss: 1.5776910781860352\n",
      "epoch: 46, batch: 115, loss: 1.5531713962554932\n",
      "epoch: 46, batch: 116, loss: 1.5099844932556152\n",
      "epoch: 46, batch: 117, loss: 1.6016525030136108\n",
      "epoch: 46, batch: 118, loss: 1.4772746562957764\n",
      "epoch: 46, batch: 119, loss: 1.5545696020126343\n",
      "epoch: 46, batch: 120, loss: 1.3453114032745361\n",
      "epoch: 46, batch: 121, loss: 1.4491281509399414\n",
      "epoch: 46, batch: 122, loss: 1.5690113306045532\n",
      "epoch: 46, batch: 123, loss: 1.4669735431671143\n",
      "epoch: 46, batch: 124, loss: 1.552804708480835\n",
      "epoch: 46, batch: 125, loss: 1.5926967859268188\n",
      "epoch: 46, batch: 126, loss: 1.3906182050704956\n",
      "epoch: 46, batch: 127, loss: 1.4806549549102783\n",
      "epoch: 47, batch: 0, loss: 1.1398334503173828\n",
      "epoch: 47, batch: 1, loss: 1.4035907983779907\n",
      "epoch: 47, batch: 2, loss: 1.252730131149292\n",
      "epoch: 47, batch: 3, loss: 1.3190609216690063\n",
      "epoch: 47, batch: 4, loss: 1.1503105163574219\n",
      "epoch: 47, batch: 5, loss: 1.3152401447296143\n",
      "epoch: 47, batch: 6, loss: 1.7125393152236938\n",
      "epoch: 47, batch: 7, loss: 1.5488669872283936\n",
      "epoch: 47, batch: 8, loss: 1.5514018535614014\n",
      "epoch: 47, batch: 9, loss: 1.6485035419464111\n",
      "epoch: 47, batch: 10, loss: 1.5930469036102295\n",
      "epoch: 47, batch: 11, loss: 1.2559417486190796\n",
      "epoch: 47, batch: 12, loss: 1.8426116704940796\n",
      "epoch: 47, batch: 13, loss: 1.5994459390640259\n",
      "epoch: 47, batch: 14, loss: 1.532857060432434\n",
      "epoch: 47, batch: 15, loss: 1.1277934312820435\n",
      "epoch: 47, batch: 16, loss: 1.4301865100860596\n",
      "epoch: 47, batch: 17, loss: 1.5467684268951416\n",
      "epoch: 47, batch: 18, loss: 1.122982382774353\n",
      "epoch: 47, batch: 19, loss: 1.754060983657837\n",
      "epoch: 47, batch: 20, loss: 1.7508913278579712\n",
      "epoch: 47, batch: 21, loss: 1.5824530124664307\n",
      "epoch: 47, batch: 22, loss: 1.1001993417739868\n",
      "epoch: 47, batch: 23, loss: 1.4350188970565796\n",
      "epoch: 47, batch: 24, loss: 1.6486610174179077\n",
      "epoch: 47, batch: 25, loss: 1.7397687435150146\n",
      "epoch: 47, batch: 26, loss: 1.586245059967041\n",
      "epoch: 47, batch: 27, loss: 1.1318730115890503\n",
      "epoch: 47, batch: 28, loss: 1.4474483728408813\n",
      "epoch: 47, batch: 29, loss: 1.5106983184814453\n",
      "epoch: 47, batch: 30, loss: 1.3782970905303955\n",
      "epoch: 47, batch: 31, loss: 1.4531075954437256\n",
      "epoch: 47, batch: 32, loss: 1.4947937726974487\n",
      "epoch: 47, batch: 33, loss: 1.7029085159301758\n",
      "epoch: 47, batch: 34, loss: 1.746023178100586\n",
      "epoch: 47, batch: 35, loss: 1.363417387008667\n",
      "epoch: 47, batch: 36, loss: 1.8358014822006226\n",
      "epoch: 47, batch: 37, loss: 1.0993797779083252\n",
      "epoch: 47, batch: 38, loss: 1.2468290328979492\n",
      "epoch: 47, batch: 39, loss: 1.4473265409469604\n",
      "epoch: 47, batch: 40, loss: 1.5128189325332642\n",
      "epoch: 47, batch: 41, loss: 1.6211040019989014\n",
      "epoch: 47, batch: 42, loss: 1.4554799795150757\n",
      "epoch: 47, batch: 43, loss: 1.7566359043121338\n",
      "epoch: 47, batch: 44, loss: 1.6718361377716064\n",
      "epoch: 47, batch: 45, loss: 1.923439621925354\n",
      "epoch: 47, batch: 46, loss: 1.7609150409698486\n",
      "epoch: 47, batch: 47, loss: 2.0567538738250732\n",
      "epoch: 47, batch: 48, loss: 1.790024995803833\n",
      "epoch: 47, batch: 49, loss: 1.5133198499679565\n",
      "epoch: 47, batch: 50, loss: 1.2652158737182617\n",
      "epoch: 47, batch: 51, loss: 1.7889366149902344\n",
      "epoch: 47, batch: 52, loss: 1.587244987487793\n",
      "epoch: 47, batch: 53, loss: 1.0934358835220337\n",
      "epoch: 47, batch: 54, loss: 1.5654284954071045\n",
      "epoch: 47, batch: 55, loss: 1.7856872081756592\n",
      "epoch: 47, batch: 56, loss: 1.5529581308364868\n",
      "epoch: 47, batch: 57, loss: 1.5464916229248047\n",
      "epoch: 47, batch: 58, loss: 1.6925519704818726\n",
      "epoch: 47, batch: 59, loss: 1.4709662199020386\n",
      "epoch: 47, batch: 60, loss: 1.5415242910385132\n",
      "epoch: 47, batch: 61, loss: 1.300981044769287\n",
      "epoch: 47, batch: 62, loss: 2.0214247703552246\n",
      "epoch: 47, batch: 63, loss: 1.6394214630126953\n",
      "epoch: 47, batch: 64, loss: 1.845442533493042\n",
      "epoch: 47, batch: 65, loss: 1.8522619009017944\n",
      "epoch: 47, batch: 66, loss: 1.388662338256836\n",
      "epoch: 47, batch: 67, loss: 1.542915940284729\n",
      "epoch: 47, batch: 68, loss: 1.8755970001220703\n",
      "epoch: 47, batch: 69, loss: 1.0680423974990845\n",
      "epoch: 47, batch: 70, loss: 1.3701673746109009\n",
      "epoch: 47, batch: 71, loss: 1.6558339595794678\n",
      "epoch: 47, batch: 72, loss: 1.5109230279922485\n",
      "epoch: 47, batch: 73, loss: 1.8104832172393799\n",
      "epoch: 47, batch: 74, loss: 1.243206262588501\n",
      "epoch: 47, batch: 75, loss: 1.6548871994018555\n",
      "epoch: 47, batch: 76, loss: 1.267958164215088\n",
      "epoch: 47, batch: 77, loss: 1.6653391122817993\n",
      "epoch: 47, batch: 78, loss: 1.54942786693573\n",
      "epoch: 47, batch: 79, loss: 1.460889458656311\n",
      "epoch: 47, batch: 80, loss: 1.7587089538574219\n",
      "epoch: 47, batch: 81, loss: 1.4304656982421875\n",
      "epoch: 47, batch: 82, loss: 1.323311448097229\n",
      "epoch: 47, batch: 83, loss: 1.7637395858764648\n",
      "epoch: 47, batch: 84, loss: 1.3136223554611206\n",
      "epoch: 47, batch: 85, loss: 0.8845425844192505\n",
      "epoch: 47, batch: 86, loss: 1.7055139541625977\n",
      "epoch: 47, batch: 87, loss: 1.2876193523406982\n",
      "epoch: 47, batch: 88, loss: 1.756260633468628\n",
      "epoch: 47, batch: 89, loss: 1.359892725944519\n",
      "epoch: 47, batch: 90, loss: 1.4201786518096924\n",
      "epoch: 47, batch: 91, loss: 1.618438720703125\n",
      "epoch: 47, batch: 92, loss: 1.8397142887115479\n",
      "epoch: 47, batch: 93, loss: 1.6229650974273682\n",
      "epoch: 47, batch: 94, loss: 1.3891733884811401\n",
      "epoch: 47, batch: 95, loss: 1.8453495502471924\n",
      "epoch: 47, batch: 96, loss: 1.8626463413238525\n",
      "epoch: 47, batch: 97, loss: 1.3336622714996338\n",
      "epoch: 47, batch: 98, loss: 1.5039074420928955\n",
      "epoch: 47, batch: 99, loss: 1.628675103187561\n",
      "epoch: 47, batch: 100, loss: 1.8326448202133179\n",
      "epoch: 47, batch: 101, loss: 1.3770147562026978\n",
      "epoch: 47, batch: 102, loss: 1.9397071599960327\n",
      "epoch: 47, batch: 103, loss: 1.5727698802947998\n",
      "epoch: 47, batch: 104, loss: 1.6199089288711548\n",
      "epoch: 47, batch: 105, loss: 1.3005857467651367\n",
      "epoch: 47, batch: 106, loss: 1.5810178518295288\n",
      "epoch: 47, batch: 107, loss: 1.303282380104065\n",
      "epoch: 47, batch: 108, loss: 1.5032246112823486\n",
      "epoch: 47, batch: 109, loss: 1.5352692604064941\n",
      "epoch: 47, batch: 110, loss: 1.3379416465759277\n",
      "epoch: 47, batch: 111, loss: 1.6589813232421875\n",
      "epoch: 47, batch: 112, loss: 1.674441933631897\n",
      "epoch: 47, batch: 113, loss: 1.1655325889587402\n",
      "epoch: 47, batch: 114, loss: 1.6168835163116455\n",
      "epoch: 47, batch: 115, loss: 1.2386876344680786\n",
      "epoch: 47, batch: 116, loss: 1.833961844444275\n",
      "epoch: 47, batch: 117, loss: 1.5746990442276\n",
      "epoch: 47, batch: 118, loss: 1.436553955078125\n",
      "epoch: 47, batch: 119, loss: 1.7135356664657593\n",
      "epoch: 47, batch: 120, loss: 1.5049394369125366\n",
      "epoch: 47, batch: 121, loss: 1.5002844333648682\n",
      "epoch: 47, batch: 122, loss: 2.283747911453247\n",
      "epoch: 47, batch: 123, loss: 1.5802055597305298\n",
      "epoch: 47, batch: 124, loss: 1.3475360870361328\n",
      "epoch: 47, batch: 125, loss: 1.4602441787719727\n",
      "epoch: 47, batch: 126, loss: 1.586105465888977\n",
      "epoch: 47, batch: 127, loss: 1.307594656944275\n",
      "epoch: 48, batch: 0, loss: 1.729750633239746\n",
      "epoch: 48, batch: 1, loss: 1.4064737558364868\n",
      "epoch: 48, batch: 2, loss: 1.3928329944610596\n",
      "epoch: 48, batch: 3, loss: 1.9561926126480103\n",
      "epoch: 48, batch: 4, loss: 1.5421946048736572\n",
      "epoch: 48, batch: 5, loss: 1.5721691846847534\n",
      "epoch: 48, batch: 6, loss: 1.060696005821228\n",
      "epoch: 48, batch: 7, loss: 1.5583550930023193\n",
      "epoch: 48, batch: 8, loss: 1.5941193103790283\n",
      "epoch: 48, batch: 9, loss: 1.4386322498321533\n",
      "epoch: 48, batch: 10, loss: 1.8899939060211182\n",
      "epoch: 48, batch: 11, loss: 1.8294413089752197\n",
      "epoch: 48, batch: 12, loss: 1.7551158666610718\n",
      "epoch: 48, batch: 13, loss: 1.642065405845642\n",
      "epoch: 48, batch: 14, loss: 1.6848669052124023\n",
      "epoch: 48, batch: 15, loss: 1.7170991897583008\n",
      "epoch: 48, batch: 16, loss: 1.1167904138565063\n",
      "epoch: 48, batch: 17, loss: 1.4158350229263306\n",
      "epoch: 48, batch: 18, loss: 1.4084173440933228\n",
      "epoch: 48, batch: 19, loss: 0.8894838094711304\n",
      "epoch: 48, batch: 20, loss: 1.6443040370941162\n",
      "epoch: 48, batch: 21, loss: 1.8095827102661133\n",
      "epoch: 48, batch: 22, loss: 1.4698586463928223\n",
      "epoch: 48, batch: 23, loss: 1.6986582279205322\n",
      "epoch: 48, batch: 24, loss: 1.0608083009719849\n",
      "epoch: 48, batch: 25, loss: 1.7938220500946045\n",
      "epoch: 48, batch: 26, loss: 1.3685994148254395\n",
      "epoch: 48, batch: 27, loss: 1.4095041751861572\n",
      "epoch: 48, batch: 28, loss: 1.784041404724121\n",
      "epoch: 48, batch: 29, loss: 1.4653903245925903\n",
      "epoch: 48, batch: 30, loss: 2.0078837871551514\n",
      "epoch: 48, batch: 31, loss: 1.1883161067962646\n",
      "epoch: 48, batch: 32, loss: 1.2803165912628174\n",
      "epoch: 48, batch: 33, loss: 0.9462019205093384\n",
      "epoch: 48, batch: 34, loss: 1.777984380722046\n",
      "epoch: 48, batch: 35, loss: 1.4122329950332642\n",
      "epoch: 48, batch: 36, loss: 1.7647016048431396\n",
      "epoch: 48, batch: 37, loss: 1.5638251304626465\n",
      "epoch: 48, batch: 38, loss: 1.4049068689346313\n",
      "epoch: 48, batch: 39, loss: 1.3719205856323242\n",
      "epoch: 48, batch: 40, loss: 1.2027159929275513\n",
      "epoch: 48, batch: 41, loss: 1.6416164636611938\n",
      "epoch: 48, batch: 42, loss: 1.6801948547363281\n",
      "epoch: 48, batch: 43, loss: 1.7683181762695312\n",
      "epoch: 48, batch: 44, loss: 1.641939401626587\n",
      "epoch: 48, batch: 45, loss: 1.238947868347168\n",
      "epoch: 48, batch: 46, loss: 1.1027193069458008\n",
      "epoch: 48, batch: 47, loss: 1.3214490413665771\n",
      "epoch: 48, batch: 48, loss: 1.8303171396255493\n",
      "epoch: 48, batch: 49, loss: 1.473432183265686\n",
      "epoch: 48, batch: 50, loss: 1.0381947755813599\n",
      "epoch: 48, batch: 51, loss: 1.4008065462112427\n",
      "epoch: 48, batch: 52, loss: 1.5505363941192627\n",
      "epoch: 48, batch: 53, loss: 1.310692548751831\n",
      "epoch: 48, batch: 54, loss: 1.1441255807876587\n",
      "epoch: 48, batch: 55, loss: 2.1785311698913574\n",
      "epoch: 48, batch: 56, loss: 1.7669763565063477\n",
      "epoch: 48, batch: 57, loss: 1.4765783548355103\n",
      "epoch: 48, batch: 58, loss: 1.7215080261230469\n",
      "epoch: 48, batch: 59, loss: 1.649563193321228\n",
      "epoch: 48, batch: 60, loss: 1.2341511249542236\n",
      "epoch: 48, batch: 61, loss: 1.4450862407684326\n",
      "epoch: 48, batch: 62, loss: 1.4744107723236084\n",
      "epoch: 48, batch: 63, loss: 1.753414511680603\n",
      "epoch: 48, batch: 64, loss: 1.1392650604248047\n",
      "epoch: 48, batch: 65, loss: 1.4530233144760132\n",
      "epoch: 48, batch: 66, loss: 1.488890290260315\n",
      "epoch: 48, batch: 67, loss: 1.5319225788116455\n",
      "epoch: 48, batch: 68, loss: 1.5425630807876587\n",
      "epoch: 48, batch: 69, loss: 1.573805570602417\n",
      "epoch: 48, batch: 70, loss: 1.6562244892120361\n",
      "epoch: 48, batch: 71, loss: 1.9302905797958374\n",
      "epoch: 48, batch: 72, loss: 1.5860228538513184\n",
      "epoch: 48, batch: 73, loss: 1.1620664596557617\n",
      "epoch: 48, batch: 74, loss: 1.6655534505844116\n",
      "epoch: 48, batch: 75, loss: 1.4603217840194702\n",
      "epoch: 48, batch: 76, loss: 1.241776704788208\n",
      "epoch: 48, batch: 77, loss: 1.650935173034668\n",
      "epoch: 48, batch: 78, loss: 1.4744889736175537\n",
      "epoch: 48, batch: 79, loss: 1.4348924160003662\n",
      "epoch: 48, batch: 80, loss: 1.6187305450439453\n",
      "epoch: 48, batch: 81, loss: 1.1095216274261475\n",
      "epoch: 48, batch: 82, loss: 1.5396581888198853\n",
      "epoch: 48, batch: 83, loss: 1.3930180072784424\n",
      "epoch: 48, batch: 84, loss: 1.5997976064682007\n",
      "epoch: 48, batch: 85, loss: 1.7402708530426025\n",
      "epoch: 48, batch: 86, loss: 1.5619277954101562\n",
      "epoch: 48, batch: 87, loss: 1.9412418603897095\n",
      "epoch: 48, batch: 88, loss: 1.5256315469741821\n",
      "epoch: 48, batch: 89, loss: 1.319947600364685\n",
      "epoch: 48, batch: 90, loss: 1.3859074115753174\n",
      "epoch: 48, batch: 91, loss: 1.2547508478164673\n",
      "epoch: 48, batch: 92, loss: 1.6074554920196533\n",
      "epoch: 48, batch: 93, loss: 1.4436883926391602\n",
      "epoch: 48, batch: 94, loss: 1.5671192407608032\n",
      "epoch: 48, batch: 95, loss: 1.4579894542694092\n",
      "epoch: 48, batch: 96, loss: 1.451711893081665\n",
      "epoch: 48, batch: 97, loss: 1.292284369468689\n",
      "epoch: 48, batch: 98, loss: 1.5568886995315552\n",
      "epoch: 48, batch: 99, loss: 1.432085633277893\n",
      "epoch: 48, batch: 100, loss: 1.8372535705566406\n",
      "epoch: 48, batch: 101, loss: 1.2858091592788696\n",
      "epoch: 48, batch: 102, loss: 1.6612632274627686\n",
      "epoch: 48, batch: 103, loss: 1.747441291809082\n",
      "epoch: 48, batch: 104, loss: 1.4428753852844238\n",
      "epoch: 48, batch: 105, loss: 1.734518051147461\n",
      "epoch: 48, batch: 106, loss: 1.4803431034088135\n",
      "epoch: 48, batch: 107, loss: 1.7844043970108032\n",
      "epoch: 48, batch: 108, loss: 1.4001920223236084\n",
      "epoch: 48, batch: 109, loss: 1.2583497762680054\n",
      "epoch: 48, batch: 110, loss: 1.5850906372070312\n",
      "epoch: 48, batch: 111, loss: 1.7622934579849243\n",
      "epoch: 48, batch: 112, loss: 1.4549027681350708\n",
      "epoch: 48, batch: 113, loss: 1.4341180324554443\n",
      "epoch: 48, batch: 114, loss: 1.2980064153671265\n",
      "epoch: 48, batch: 115, loss: 1.7769734859466553\n",
      "epoch: 48, batch: 116, loss: 1.434743881225586\n",
      "epoch: 48, batch: 117, loss: 1.916128158569336\n",
      "epoch: 48, batch: 118, loss: 1.4395602941513062\n",
      "epoch: 48, batch: 119, loss: 1.4717230796813965\n",
      "epoch: 48, batch: 120, loss: 1.3035242557525635\n",
      "epoch: 48, batch: 121, loss: 1.675532579421997\n",
      "epoch: 48, batch: 122, loss: 1.8681539297103882\n",
      "epoch: 48, batch: 123, loss: 1.6821315288543701\n",
      "epoch: 48, batch: 124, loss: 1.276914358139038\n",
      "epoch: 48, batch: 125, loss: 1.7125740051269531\n",
      "epoch: 48, batch: 126, loss: 1.4346193075180054\n",
      "epoch: 48, batch: 127, loss: 1.692342758178711\n",
      "epoch: 49, batch: 0, loss: 1.5583832263946533\n",
      "epoch: 49, batch: 1, loss: 1.4999345541000366\n",
      "epoch: 49, batch: 2, loss: 1.6221282482147217\n",
      "epoch: 49, batch: 3, loss: 1.1565316915512085\n",
      "epoch: 49, batch: 4, loss: 1.717573881149292\n",
      "epoch: 49, batch: 5, loss: 1.7669391632080078\n",
      "epoch: 49, batch: 6, loss: 1.5089843273162842\n",
      "epoch: 49, batch: 7, loss: 1.744948387145996\n",
      "epoch: 49, batch: 8, loss: 1.5209318399429321\n",
      "epoch: 49, batch: 9, loss: 1.502720594406128\n",
      "epoch: 49, batch: 10, loss: 1.3505806922912598\n",
      "epoch: 49, batch: 11, loss: 1.4400770664215088\n",
      "epoch: 49, batch: 12, loss: 1.5188255310058594\n",
      "epoch: 49, batch: 13, loss: 1.6444299221038818\n",
      "epoch: 49, batch: 14, loss: 1.7945095300674438\n",
      "epoch: 49, batch: 15, loss: 1.3885743618011475\n",
      "epoch: 49, batch: 16, loss: 1.3088335990905762\n",
      "epoch: 49, batch: 17, loss: 1.4215514659881592\n",
      "epoch: 49, batch: 18, loss: 1.8252918720245361\n",
      "epoch: 49, batch: 19, loss: 1.5189648866653442\n",
      "epoch: 49, batch: 20, loss: 1.6619552373886108\n",
      "epoch: 49, batch: 21, loss: 1.6871883869171143\n",
      "epoch: 49, batch: 22, loss: 1.9708331823349\n",
      "epoch: 49, batch: 23, loss: 1.3049591779708862\n",
      "epoch: 49, batch: 24, loss: 1.63260018825531\n",
      "epoch: 49, batch: 25, loss: 1.6283533573150635\n",
      "epoch: 49, batch: 26, loss: 1.6578350067138672\n",
      "epoch: 49, batch: 27, loss: 1.5376800298690796\n",
      "epoch: 49, batch: 28, loss: 1.5505949258804321\n",
      "epoch: 49, batch: 29, loss: 1.6720432043075562\n",
      "epoch: 49, batch: 30, loss: 1.6799888610839844\n",
      "epoch: 49, batch: 31, loss: 1.4848999977111816\n",
      "epoch: 49, batch: 32, loss: 1.4272692203521729\n",
      "epoch: 49, batch: 33, loss: 1.5134832859039307\n",
      "epoch: 49, batch: 34, loss: 1.3705812692642212\n",
      "epoch: 49, batch: 35, loss: 1.243566632270813\n",
      "epoch: 49, batch: 36, loss: 1.6607805490493774\n",
      "epoch: 49, batch: 37, loss: 1.5476901531219482\n",
      "epoch: 49, batch: 38, loss: 1.4022676944732666\n",
      "epoch: 49, batch: 39, loss: 1.6689815521240234\n",
      "epoch: 49, batch: 40, loss: 1.5069235563278198\n",
      "epoch: 49, batch: 41, loss: 1.5457580089569092\n",
      "epoch: 49, batch: 42, loss: 1.7499374151229858\n",
      "epoch: 49, batch: 43, loss: 1.5202516317367554\n",
      "epoch: 49, batch: 44, loss: 1.5951379537582397\n",
      "epoch: 49, batch: 45, loss: 1.4860851764678955\n",
      "epoch: 49, batch: 46, loss: 1.4865106344223022\n",
      "epoch: 49, batch: 47, loss: 1.5484437942504883\n",
      "epoch: 49, batch: 48, loss: 1.5454449653625488\n",
      "epoch: 49, batch: 49, loss: 1.501528024673462\n",
      "epoch: 49, batch: 50, loss: 1.7105821371078491\n",
      "epoch: 49, batch: 51, loss: 1.370239019393921\n",
      "epoch: 49, batch: 52, loss: 1.8209701776504517\n",
      "epoch: 49, batch: 53, loss: 1.5289169549942017\n",
      "epoch: 49, batch: 54, loss: 1.4803075790405273\n",
      "epoch: 49, batch: 55, loss: 1.3918508291244507\n",
      "epoch: 49, batch: 56, loss: 1.503143548965454\n",
      "epoch: 49, batch: 57, loss: 1.5433627367019653\n",
      "epoch: 49, batch: 58, loss: 1.526903748512268\n",
      "epoch: 49, batch: 59, loss: 1.3247545957565308\n",
      "epoch: 49, batch: 60, loss: 1.3741236925125122\n",
      "epoch: 49, batch: 61, loss: 1.3314043283462524\n",
      "epoch: 49, batch: 62, loss: 1.4826948642730713\n",
      "epoch: 49, batch: 63, loss: 1.6107828617095947\n",
      "epoch: 49, batch: 64, loss: 1.787132978439331\n",
      "epoch: 49, batch: 65, loss: 1.5032939910888672\n",
      "epoch: 49, batch: 66, loss: 1.5543906688690186\n",
      "epoch: 49, batch: 67, loss: 1.6562658548355103\n",
      "epoch: 49, batch: 68, loss: 1.7424360513687134\n",
      "epoch: 49, batch: 69, loss: 1.6571277379989624\n",
      "epoch: 49, batch: 70, loss: 1.3321990966796875\n",
      "epoch: 49, batch: 71, loss: 1.220201849937439\n",
      "epoch: 49, batch: 72, loss: 1.598742127418518\n",
      "epoch: 49, batch: 73, loss: 1.4440958499908447\n",
      "epoch: 49, batch: 74, loss: 1.4859422445297241\n",
      "epoch: 49, batch: 75, loss: 1.4859318733215332\n",
      "epoch: 49, batch: 76, loss: 1.7091445922851562\n",
      "epoch: 49, batch: 77, loss: 1.3746442794799805\n",
      "epoch: 49, batch: 78, loss: 1.6154735088348389\n",
      "epoch: 49, batch: 79, loss: 1.504183053970337\n",
      "epoch: 49, batch: 80, loss: 1.4858793020248413\n",
      "epoch: 49, batch: 81, loss: 1.503539800643921\n",
      "epoch: 49, batch: 82, loss: 1.5278440713882446\n",
      "epoch: 49, batch: 83, loss: 1.6392333507537842\n",
      "epoch: 49, batch: 84, loss: 1.373853087425232\n",
      "epoch: 49, batch: 85, loss: 1.58877694606781\n",
      "epoch: 49, batch: 86, loss: 1.3738300800323486\n",
      "epoch: 49, batch: 87, loss: 1.4347445964813232\n",
      "epoch: 49, batch: 88, loss: 1.6582072973251343\n",
      "epoch: 49, batch: 89, loss: 1.3317101001739502\n",
      "epoch: 49, batch: 90, loss: 1.4862442016601562\n",
      "epoch: 49, batch: 91, loss: 1.3742220401763916\n",
      "epoch: 49, batch: 92, loss: 1.546257734298706\n",
      "epoch: 49, batch: 93, loss: 1.5556001663208008\n",
      "epoch: 49, batch: 94, loss: 1.5646703243255615\n",
      "epoch: 49, batch: 95, loss: 1.6582870483398438\n",
      "epoch: 49, batch: 96, loss: 1.5555330514907837\n",
      "epoch: 49, batch: 97, loss: 1.6395528316497803\n",
      "epoch: 49, batch: 98, loss: 1.3736851215362549\n",
      "epoch: 49, batch: 99, loss: 1.527899146080017\n",
      "epoch: 49, batch: 100, loss: 1.7884966135025024\n",
      "epoch: 49, batch: 101, loss: 1.8123403787612915\n",
      "epoch: 49, batch: 102, loss: 1.504095435142517\n",
      "epoch: 49, batch: 103, loss: 1.2193371057510376\n",
      "epoch: 49, batch: 104, loss: 1.770186424255371\n",
      "epoch: 49, batch: 105, loss: 1.2192821502685547\n",
      "epoch: 49, batch: 106, loss: 1.4856051206588745\n",
      "epoch: 49, batch: 107, loss: 1.546402931213379\n",
      "epoch: 49, batch: 108, loss: 1.528071641921997\n",
      "epoch: 49, batch: 109, loss: 1.3313286304473877\n",
      "epoch: 49, batch: 110, loss: 1.6584583520889282\n",
      "epoch: 49, batch: 111, loss: 1.667039155960083\n",
      "epoch: 49, batch: 112, loss: 1.5551837682724\n",
      "epoch: 49, batch: 113, loss: 1.2191238403320312\n",
      "epoch: 49, batch: 114, loss: 1.2190788984298706\n",
      "epoch: 49, batch: 115, loss: 1.5040487051010132\n",
      "epoch: 49, batch: 116, loss: 1.4430452585220337\n",
      "epoch: 49, batch: 117, loss: 1.6585336923599243\n",
      "epoch: 49, batch: 118, loss: 1.2799265384674072\n",
      "epoch: 49, batch: 119, loss: 1.7096389532089233\n",
      "epoch: 49, batch: 120, loss: 1.3308570384979248\n",
      "epoch: 49, batch: 121, loss: 1.640079140663147\n",
      "epoch: 49, batch: 122, loss: 1.6162564754486084\n",
      "epoch: 49, batch: 123, loss: 1.2187808752059937\n",
      "epoch: 49, batch: 124, loss: 1.4854148626327515\n",
      "epoch: 49, batch: 125, loss: 1.6162109375\n",
      "epoch: 49, batch: 126, loss: 1.4853938817977905\n",
      "epoch: 49, batch: 127, loss: 1.8499493598937988\n",
      "epoch: 50, batch: 0, loss: 1.5464088916778564\n",
      "epoch: 50, batch: 1, loss: 1.6824547052383423\n",
      "epoch: 50, batch: 2, loss: 1.5042164325714111\n",
      "epoch: 50, batch: 3, loss: 1.2185540199279785\n",
      "epoch: 50, batch: 4, loss: 1.4853308200836182\n",
      "epoch: 50, batch: 5, loss: 1.4853200912475586\n",
      "epoch: 50, batch: 6, loss: 1.7287147045135498\n",
      "epoch: 50, batch: 7, loss: 1.503969430923462\n",
      "epoch: 50, batch: 8, loss: 1.7287814617156982\n",
      "epoch: 50, batch: 9, loss: 1.3915349245071411\n",
      "epoch: 50, batch: 10, loss: 1.3728225231170654\n",
      "epoch: 50, batch: 11, loss: 1.3731670379638672\n",
      "epoch: 50, batch: 12, loss: 1.6584174633026123\n",
      "epoch: 50, batch: 13, loss: 1.6584135293960571\n",
      "epoch: 50, batch: 14, loss: 1.5467336177825928\n",
      "epoch: 50, batch: 15, loss: 1.442408561706543\n",
      "epoch: 50, batch: 16, loss: 1.373102068901062\n",
      "epoch: 50, batch: 17, loss: 1.7901742458343506\n",
      "epoch: 50, batch: 18, loss: 1.3725908994674683\n",
      "epoch: 50, batch: 19, loss: 1.485172986984253\n",
      "epoch: 50, batch: 20, loss: 1.7093865871429443\n",
      "epoch: 50, batch: 21, loss: 1.6160259246826172\n",
      "epoch: 50, batch: 22, loss: 1.4529073238372803\n",
      "epoch: 50, batch: 23, loss: 1.442216157913208\n",
      "epoch: 50, batch: 24, loss: 1.6171824932098389\n",
      "epoch: 50, batch: 25, loss: 1.3294460773468018\n",
      "epoch: 50, batch: 26, loss: 1.7630624771118164\n",
      "epoch: 50, batch: 27, loss: 1.7093465328216553\n",
      "epoch: 50, batch: 28, loss: 1.4850786924362183\n",
      "epoch: 50, batch: 29, loss: 1.4844062328338623\n",
      "epoch: 50, batch: 30, loss: 1.894809365272522\n",
      "epoch: 50, batch: 31, loss: 1.5657144784927368\n",
      "epoch: 50, batch: 32, loss: 1.4434181451797485\n",
      "epoch: 50, batch: 33, loss: 1.2169597148895264\n",
      "epoch: 50, batch: 34, loss: 1.702139139175415\n",
      "epoch: 50, batch: 35, loss: 1.4850060939788818\n",
      "epoch: 50, batch: 36, loss: 1.5461671352386475\n",
      "epoch: 50, batch: 37, loss: 1.5038396120071411\n",
      "epoch: 50, batch: 38, loss: 1.4340027570724487\n",
      "epoch: 50, batch: 39, loss: 1.708444595336914\n",
      "epoch: 50, batch: 40, loss: 1.442639708518982\n",
      "epoch: 50, batch: 41, loss: 1.8153419494628906\n",
      "epoch: 50, batch: 42, loss: 1.6678186655044556\n",
      "epoch: 50, batch: 43, loss: 1.3727622032165527\n",
      "epoch: 50, batch: 44, loss: 1.3304431438446045\n",
      "epoch: 50, batch: 45, loss: 1.5047290325164795\n",
      "epoch: 50, batch: 46, loss: 1.484892725944519\n",
      "epoch: 50, batch: 47, loss: 1.504752278327942\n",
      "epoch: 50, batch: 48, loss: 1.3284498453140259\n",
      "epoch: 50, batch: 49, loss: 1.7724123001098633\n",
      "epoch: 50, batch: 50, loss: 1.6403324604034424\n",
      "epoch: 50, batch: 51, loss: 1.5047976970672607\n",
      "epoch: 50, batch: 52, loss: 1.485870599746704\n",
      "epoch: 50, batch: 53, loss: 1.3726389408111572\n",
      "epoch: 50, batch: 54, loss: 1.4414466619491577\n",
      "epoch: 50, batch: 55, loss: 1.4425079822540283\n",
      "epoch: 50, batch: 56, loss: 1.5270816087722778\n",
      "epoch: 50, batch: 57, loss: 1.5037448406219482\n",
      "epoch: 50, batch: 58, loss: 1.7125532627105713\n",
      "epoch: 50, batch: 59, loss: 1.2781959772109985\n",
      "epoch: 50, batch: 60, loss: 1.6403992176055908\n",
      "epoch: 50, batch: 61, loss: 1.5704948902130127\n",
      "epoch: 50, batch: 62, loss: 1.6594041585922241\n",
      "epoch: 50, batch: 63, loss: 1.6392128467559814\n",
      "epoch: 50, batch: 64, loss: 1.639204978942871\n",
      "epoch: 50, batch: 65, loss: 1.599380373954773\n",
      "epoch: 50, batch: 66, loss: 1.4423993825912476\n",
      "epoch: 50, batch: 67, loss: 1.4846769571304321\n",
      "epoch: 50, batch: 68, loss: 1.5705201625823975\n",
      "epoch: 50, batch: 69, loss: 1.4846560955047607\n",
      "epoch: 50, batch: 70, loss: 1.5459684133529663\n",
      "epoch: 50, batch: 71, loss: 1.3288005590438843\n",
      "epoch: 50, batch: 72, loss: 1.6840976476669312\n",
      "epoch: 50, batch: 73, loss: 1.440979242324829\n",
      "epoch: 50, batch: 74, loss: 1.3723688125610352\n",
      "epoch: 50, batch: 75, loss: 1.5473225116729736\n",
      "epoch: 50, batch: 76, loss: 1.5050499439239502\n",
      "epoch: 50, batch: 77, loss: 1.7455533742904663\n",
      "epoch: 50, batch: 78, loss: 1.6786829233169556\n",
      "epoch: 50, batch: 79, loss: 1.328587293624878\n",
      "epoch: 50, batch: 80, loss: 1.3899383544921875\n",
      "epoch: 50, batch: 81, loss: 1.3899174928665161\n",
      "epoch: 50, batch: 82, loss: 1.5459078550338745\n",
      "epoch: 50, batch: 83, loss: 1.3722463846206665\n",
      "epoch: 50, batch: 84, loss: 1.4844982624053955\n",
      "epoch: 50, batch: 85, loss: 1.5544847249984741\n",
      "epoch: 50, batch: 86, loss: 1.659691572189331\n",
      "epoch: 50, batch: 87, loss: 1.642102837562561\n",
      "epoch: 50, batch: 88, loss: 1.4844558238983154\n",
      "epoch: 50, batch: 89, loss: 1.3913224935531616\n",
      "epoch: 50, batch: 90, loss: 1.505188226699829\n",
      "epoch: 50, batch: 91, loss: 1.4828236103057861\n",
      "epoch: 50, batch: 92, loss: 1.6405830383300781\n",
      "epoch: 50, batch: 93, loss: 1.638960838317871\n",
      "epoch: 50, batch: 94, loss: 1.5035860538482666\n",
      "epoch: 50, batch: 95, loss: 1.4404687881469727\n",
      "epoch: 50, batch: 96, loss: 1.3720625638961792\n",
      "epoch: 50, batch: 97, loss: 1.600034475326538\n",
      "epoch: 50, batch: 98, loss: 1.4335095882415771\n",
      "epoch: 50, batch: 99, loss: 1.501859188079834\n",
      "epoch: 50, batch: 100, loss: 1.6598594188690186\n",
      "epoch: 50, batch: 101, loss: 1.3262712955474854\n",
      "epoch: 50, batch: 102, loss: 1.7317136526107788\n",
      "epoch: 50, batch: 103, loss: 1.6158863306045532\n",
      "epoch: 50, batch: 104, loss: 1.6791702508926392\n",
      "epoch: 50, batch: 105, loss: 1.484274983406067\n",
      "epoch: 50, batch: 106, loss: 1.2155293226242065\n",
      "epoch: 50, batch: 107, loss: 1.6617590188980103\n",
      "epoch: 50, batch: 108, loss: 1.5984244346618652\n",
      "epoch: 50, batch: 109, loss: 1.2135963439941406\n",
      "epoch: 50, batch: 110, loss: 1.370006799697876\n",
      "epoch: 50, batch: 111, loss: 1.7022298574447632\n",
      "epoch: 50, batch: 112, loss: 1.2153478860855103\n",
      "epoch: 50, batch: 113, loss: 1.4822947978973389\n",
      "epoch: 50, batch: 114, loss: 1.5035094022750854\n",
      "epoch: 50, batch: 115, loss: 1.327634572982788\n",
      "epoch: 50, batch: 116, loss: 1.6178185939788818\n",
      "epoch: 50, batch: 117, loss: 1.7022974491119385\n",
      "epoch: 50, batch: 118, loss: 1.4841376543045044\n",
      "epoch: 50, batch: 119, loss: 1.8395583629608154\n",
      "epoch: 50, batch: 120, loss: 1.662090539932251\n",
      "epoch: 50, batch: 121, loss: 1.547713041305542\n",
      "epoch: 50, batch: 122, loss: 1.7109196186065674\n",
      "epoch: 50, batch: 123, loss: 1.6850067377090454\n",
      "epoch: 50, batch: 124, loss: 1.642797827720642\n",
      "epoch: 50, batch: 125, loss: 1.484063744544983\n",
      "epoch: 50, batch: 126, loss: 1.4840528964996338\n",
      "epoch: 50, batch: 127, loss: 1.6068291664123535\n",
      "epoch: 51, batch: 0, loss: 1.6581029891967773\n",
      "epoch: 51, batch: 1, loss: 1.660210371017456\n",
      "epoch: 51, batch: 2, loss: 1.3251135349273682\n",
      "epoch: 51, batch: 3, loss: 1.4417822360992432\n",
      "epoch: 51, batch: 4, loss: 1.5262048244476318\n",
      "epoch: 51, batch: 5, loss: 1.503436803817749\n",
      "epoch: 51, batch: 6, loss: 1.4417541027069092\n",
      "epoch: 51, batch: 7, loss: 1.371502161026001\n",
      "epoch: 51, batch: 8, loss: 1.598603367805481\n",
      "epoch: 51, batch: 9, loss: 1.2145897150039673\n",
      "epoch: 51, batch: 10, loss: 1.503420114517212\n",
      "epoch: 51, batch: 11, loss: 1.5478602647781372\n",
      "epoch: 51, batch: 12, loss: 1.503413438796997\n",
      "epoch: 51, batch: 13, loss: 1.6666450500488281\n",
      "epoch: 51, batch: 14, loss: 1.7751238346099854\n",
      "epoch: 51, batch: 15, loss: 1.483870506286621\n",
      "epoch: 51, batch: 16, loss: 1.5564489364624023\n",
      "epoch: 51, batch: 17, loss: 1.6408557891845703\n",
      "epoch: 51, batch: 18, loss: 1.545592188835144\n",
      "epoch: 51, batch: 19, loss: 1.6736427545547485\n",
      "epoch: 51, batch: 20, loss: 1.6830685138702393\n",
      "epoch: 51, batch: 21, loss: 1.3885114192962646\n",
      "epoch: 51, batch: 22, loss: 1.5541118383407593\n",
      "epoch: 51, batch: 23, loss: 1.5033751726150513\n",
      "epoch: 51, batch: 24, loss: 1.6824804544448853\n",
      "epoch: 51, batch: 25, loss: 1.7112228870391846\n",
      "epoch: 51, batch: 26, loss: 1.7247397899627686\n",
      "epoch: 51, batch: 27, loss: 1.326579213142395\n",
      "epoch: 51, batch: 28, loss: 1.4415323734283447\n",
      "epoch: 51, batch: 29, loss: 1.5962618589401245\n",
      "epoch: 51, batch: 30, loss: 1.371163010597229\n",
      "epoch: 51, batch: 31, loss: 1.213923454284668\n",
      "epoch: 51, batch: 32, loss: 1.5033427476882935\n",
      "epoch: 51, batch: 33, loss: 1.4861732721328735\n",
      "epoch: 51, batch: 34, loss: 1.6631145477294922\n",
      "epoch: 51, batch: 35, loss: 1.5987389087677002\n",
      "epoch: 51, batch: 36, loss: 1.6409485340118408\n",
      "epoch: 51, batch: 37, loss: 1.7672868967056274\n",
      "epoch: 51, batch: 38, loss: 1.658106803894043\n",
      "epoch: 51, batch: 39, loss: 1.3710300922393799\n",
      "epoch: 51, batch: 40, loss: 1.548093318939209\n",
      "epoch: 51, batch: 41, loss: 1.2110379934310913\n",
      "epoch: 51, batch: 42, loss: 1.4861689805984497\n",
      "epoch: 51, batch: 43, loss: 1.528384804725647\n",
      "epoch: 51, batch: 44, loss: 1.6858158111572266\n",
      "epoch: 51, batch: 45, loss: 1.5283855199813843\n",
      "epoch: 51, batch: 46, loss: 1.5987770557403564\n",
      "epoch: 51, batch: 47, loss: 1.4808893203735352\n",
      "epoch: 51, batch: 48, loss: 1.7114059925079346\n",
      "epoch: 51, batch: 49, loss: 1.3879978656768799\n",
      "epoch: 51, batch: 50, loss: 1.6436805725097656\n",
      "epoch: 51, batch: 51, loss: 1.5961194038391113\n",
      "epoch: 51, batch: 52, loss: 1.4412422180175781\n",
      "epoch: 51, batch: 53, loss: 1.6102259159088135\n",
      "epoch: 51, batch: 54, loss: 1.328566312789917\n",
      "epoch: 51, batch: 55, loss: 1.5283801555633545\n",
      "epoch: 51, batch: 56, loss: 1.656017541885376\n",
      "epoch: 51, batch: 57, loss: 1.5960830450057983\n",
      "epoch: 51, batch: 58, loss: 1.9004671573638916\n",
      "epoch: 51, batch: 59, loss: 1.638309121131897\n",
      "epoch: 51, batch: 60, loss: 1.6581674814224243\n",
      "epoch: 51, batch: 61, loss: 1.6186901330947876\n",
      "epoch: 51, batch: 62, loss: 1.3256603479385376\n",
      "epoch: 51, batch: 63, loss: 1.6187118291854858\n",
      "epoch: 51, batch: 64, loss: 1.5454784631729126\n",
      "epoch: 51, batch: 65, loss: 1.483328104019165\n",
      "epoch: 51, batch: 66, loss: 1.6215450763702393\n",
      "epoch: 51, batch: 67, loss: 1.7594152688980103\n",
      "epoch: 51, batch: 68, loss: 1.3677544593811035\n",
      "epoch: 51, batch: 69, loss: 1.5482960939407349\n",
      "epoch: 51, batch: 70, loss: 1.452682614326477\n",
      "epoch: 51, batch: 71, loss: 1.3705227375030518\n",
      "epoch: 51, batch: 72, loss: 1.2098125219345093\n",
      "epoch: 51, batch: 73, loss: 1.618817687034607\n",
      "epoch: 51, batch: 74, loss: 1.438109278678894\n",
      "epoch: 51, batch: 75, loss: 1.322457194328308\n",
      "epoch: 51, batch: 76, loss: 1.5483406782150269\n",
      "epoch: 51, batch: 77, loss: 1.5283406972885132\n",
      "epoch: 51, batch: 78, loss: 1.5508060455322266\n",
      "epoch: 51, batch: 79, loss: 1.5454574823379517\n",
      "epoch: 51, batch: 80, loss: 1.5454561710357666\n",
      "epoch: 51, batch: 81, loss: 1.4433523416519165\n",
      "epoch: 51, batch: 82, loss: 1.4408735036849976\n",
      "epoch: 51, batch: 83, loss: 1.3221919536590576\n",
      "epoch: 51, batch: 84, loss: 1.618931770324707\n",
      "epoch: 51, batch: 85, loss: 1.4125621318817139\n",
      "epoch: 51, batch: 86, loss: 1.5565998554229736\n",
      "epoch: 51, batch: 87, loss: 1.7064743041992188\n",
      "epoch: 51, batch: 88, loss: 1.7064956426620483\n",
      "epoch: 51, batch: 89, loss: 1.483062505722046\n",
      "epoch: 51, batch: 90, loss: 1.5031646490097046\n",
      "epoch: 51, batch: 91, loss: 1.5283161401748657\n",
      "epoch: 51, batch: 92, loss: 1.254321813583374\n",
      "epoch: 51, batch: 93, loss: 1.503156065940857\n",
      "epoch: 51, batch: 94, loss: 1.5484570264816284\n",
      "epoch: 51, batch: 95, loss: 1.3278417587280273\n",
      "epoch: 51, batch: 96, loss: 1.6160213947296143\n",
      "epoch: 51, batch: 97, loss: 1.856614351272583\n",
      "epoch: 51, batch: 98, loss: 1.324737310409546\n",
      "epoch: 51, batch: 99, loss: 1.553550124168396\n",
      "epoch: 51, batch: 100, loss: 1.5705950260162354\n",
      "epoch: 51, batch: 101, loss: 1.8196741342544556\n",
      "epoch: 51, batch: 102, loss: 1.8166214227676392\n",
      "epoch: 51, batch: 103, loss: 1.5958212614059448\n",
      "epoch: 51, batch: 104, loss: 1.5485206842422485\n",
      "epoch: 51, batch: 105, loss: 1.2085301876068115\n",
      "epoch: 51, batch: 106, loss: 1.7289783954620361\n",
      "epoch: 51, batch: 107, loss: 1.208452582359314\n",
      "epoch: 51, batch: 108, loss: 1.548546552658081\n",
      "epoch: 51, batch: 109, loss: 1.7069419622421265\n",
      "epoch: 51, batch: 110, loss: 1.7492755651474\n",
      "epoch: 51, batch: 111, loss: 1.48281991481781\n",
      "epoch: 51, batch: 112, loss: 1.7696142196655273\n",
      "epoch: 51, batch: 113, loss: 1.324350357055664\n",
      "epoch: 51, batch: 114, loss: 1.2536695003509521\n",
      "epoch: 51, batch: 115, loss: 1.4372780323028564\n",
      "epoch: 51, batch: 116, loss: 1.5250842571258545\n",
      "epoch: 51, batch: 117, loss: 1.7039152383804321\n",
      "epoch: 51, batch: 118, loss: 1.437218427658081\n",
      "epoch: 51, batch: 119, loss: 1.5062952041625977\n",
      "epoch: 51, batch: 120, loss: 1.3209540843963623\n",
      "epoch: 51, batch: 121, loss: 1.2111352682113647\n",
      "epoch: 51, batch: 122, loss: 1.7982861995697021\n",
      "epoch: 51, batch: 123, loss: 1.7291669845581055\n",
      "epoch: 51, batch: 124, loss: 1.3696465492248535\n",
      "epoch: 51, batch: 125, loss: 1.3867775201797485\n",
      "epoch: 51, batch: 126, loss: 1.6617012023925781\n",
      "epoch: 51, batch: 127, loss: 1.385432243347168\n",
      "epoch: 52, batch: 0, loss: 1.503064751625061\n",
      "epoch: 52, batch: 1, loss: 1.5989692211151123\n",
      "epoch: 52, batch: 2, loss: 1.4369834661483765\n",
      "epoch: 52, batch: 3, loss: 1.3662197589874268\n",
      "epoch: 52, batch: 4, loss: 1.778164267539978\n",
      "epoch: 52, batch: 5, loss: 1.4369255304336548\n",
      "epoch: 52, batch: 6, loss: 1.7577279806137085\n",
      "epoch: 52, batch: 7, loss: 1.60232675075531\n",
      "epoch: 52, batch: 8, loss: 1.729313611984253\n",
      "epoch: 52, batch: 9, loss: 1.2106388807296753\n",
      "epoch: 52, batch: 10, loss: 1.4825190305709839\n",
      "epoch: 52, batch: 11, loss: 1.4825079441070557\n",
      "epoch: 52, batch: 12, loss: 1.3236680030822754\n",
      "epoch: 52, batch: 13, loss: 1.6584924459457397\n",
      "epoch: 52, batch: 14, loss: 1.44354248046875\n",
      "epoch: 52, batch: 15, loss: 1.5990056991577148\n",
      "epoch: 52, batch: 16, loss: 1.3235679864883423\n",
      "epoch: 52, batch: 17, loss: 1.25271737575531\n",
      "epoch: 52, batch: 18, loss: 1.432196855545044\n",
      "epoch: 52, batch: 19, loss: 1.5247457027435303\n",
      "epoch: 52, batch: 20, loss: 1.6127570867538452\n",
      "epoch: 52, batch: 21, loss: 1.6619746685028076\n",
      "epoch: 52, batch: 22, loss: 1.5247126817703247\n",
      "epoch: 52, batch: 23, loss: 1.25253164768219\n",
      "epoch: 52, batch: 24, loss: 1.6620105504989624\n",
      "epoch: 52, batch: 25, loss: 1.3198680877685547\n",
      "epoch: 52, batch: 26, loss: 1.3198357820510864\n",
      "epoch: 52, batch: 27, loss: 1.4823269844055176\n",
      "epoch: 52, batch: 28, loss: 1.7752796411514282\n",
      "epoch: 52, batch: 29, loss: 1.3690763711929321\n",
      "epoch: 52, batch: 30, loss: 1.5911756753921509\n",
      "epoch: 52, batch: 31, loss: 1.4858020544052124\n",
      "epoch: 52, batch: 32, loss: 1.8040012121200562\n",
      "epoch: 52, batch: 33, loss: 1.665653944015503\n",
      "epoch: 52, batch: 34, loss: 1.5030014514923096\n",
      "epoch: 52, batch: 35, loss: 1.708017349243164\n",
      "epoch: 52, batch: 36, loss: 1.6621532440185547\n",
      "epoch: 52, batch: 37, loss: 1.6198428869247437\n",
      "epoch: 52, batch: 38, loss: 1.4398800134658813\n",
      "epoch: 52, batch: 39, loss: 1.7159360647201538\n",
      "epoch: 52, batch: 40, loss: 1.6830165386199951\n",
      "epoch: 52, batch: 41, loss: 1.595470666885376\n",
      "epoch: 52, batch: 42, loss: 1.6120679378509521\n",
      "epoch: 52, batch: 43, loss: 1.4993798732757568\n",
      "epoch: 52, batch: 44, loss: 1.5495219230651855\n",
      "epoch: 52, batch: 45, loss: 1.5244359970092773\n",
      "epoch: 52, batch: 46, loss: 1.5458670854568481\n",
      "epoch: 52, batch: 47, loss: 1.2517716884613037\n",
      "epoch: 52, batch: 48, loss: 1.4857324361801147\n",
      "epoch: 52, batch: 49, loss: 1.2093963623046875\n",
      "epoch: 52, batch: 50, loss: 1.7046301364898682\n",
      "epoch: 52, batch: 51, loss: 1.3896136283874512\n",
      "epoch: 52, batch: 52, loss: 1.6832828521728516\n",
      "epoch: 52, batch: 53, loss: 1.5489726066589355\n",
      "epoch: 52, batch: 54, loss: 1.5489780902862549\n",
      "epoch: 52, batch: 55, loss: 1.5029776096343994\n",
      "epoch: 52, batch: 56, loss: 1.3858708143234253\n",
      "epoch: 52, batch: 57, loss: 1.4359633922576904\n",
      "epoch: 52, batch: 58, loss: 1.3262534141540527\n",
      "epoch: 52, batch: 59, loss: 1.55680251121521\n",
      "epoch: 52, batch: 60, loss: 1.6834598779678345\n",
      "epoch: 52, batch: 61, loss: 1.4108076095581055\n",
      "epoch: 52, batch: 62, loss: 1.6451663970947266\n",
      "epoch: 52, batch: 63, loss: 1.5916019678115845\n",
      "epoch: 52, batch: 64, loss: 1.7796999216079712\n",
      "epoch: 52, batch: 65, loss: 1.591338872909546\n",
      "epoch: 52, batch: 66, loss: 1.4991852045059204\n",
      "epoch: 52, batch: 67, loss: 1.6836135387420654\n",
      "epoch: 52, batch: 68, loss: 1.2088083028793335\n",
      "epoch: 52, batch: 69, loss: 1.4317747354507446\n",
      "epoch: 52, batch: 70, loss: 1.4818342924118042\n",
      "epoch: 52, batch: 71, loss: 1.6625674962997437\n",
      "epoch: 52, batch: 72, loss: 1.7086986303329468\n",
      "epoch: 52, batch: 73, loss: 1.4528942108154297\n",
      "epoch: 52, batch: 74, loss: 1.5240864753723145\n",
      "epoch: 52, batch: 75, loss: 1.7799822092056274\n",
      "epoch: 52, batch: 76, loss: 1.620330810546875\n",
      "epoch: 52, batch: 77, loss: 1.6164875030517578\n",
      "epoch: 52, batch: 78, loss: 1.6877176761627197\n",
      "epoch: 52, batch: 79, loss: 1.318145751953125\n",
      "epoch: 52, batch: 80, loss: 1.5491150617599487\n",
      "epoch: 52, batch: 81, loss: 1.5529831647872925\n",
      "epoch: 52, batch: 82, loss: 1.741760015487671\n",
      "epoch: 52, batch: 83, loss: 1.5991648435592651\n",
      "epoch: 52, batch: 84, loss: 1.5991673469543457\n",
      "epoch: 52, batch: 85, loss: 1.368070125579834\n",
      "epoch: 52, batch: 86, loss: 1.506852388381958\n",
      "epoch: 52, batch: 87, loss: 1.3641111850738525\n",
      "epoch: 52, batch: 88, loss: 1.5068613290786743\n",
      "epoch: 52, batch: 89, loss: 1.5029312372207642\n",
      "epoch: 52, batch: 90, loss: 1.6414737701416016\n",
      "epoch: 52, batch: 91, loss: 1.317773461341858\n",
      "epoch: 52, batch: 92, loss: 1.5278337001800537\n",
      "epoch: 52, batch: 93, loss: 1.2716633081436157\n",
      "epoch: 52, batch: 94, loss: 1.549182653427124\n",
      "epoch: 52, batch: 95, loss: 1.599186897277832\n",
      "epoch: 52, batch: 96, loss: 1.5568879842758179\n",
      "epoch: 52, batch: 97, loss: 1.4815305471420288\n",
      "epoch: 52, batch: 98, loss: 1.6031728982925415\n",
      "epoch: 52, batch: 99, loss: 1.66290283203125\n",
      "epoch: 52, batch: 100, loss: 1.591515302658081\n",
      "epoch: 52, batch: 101, loss: 1.431187391281128\n",
      "epoch: 52, batch: 102, loss: 1.705248236656189\n",
      "epoch: 52, batch: 103, loss: 1.325423002243042\n",
      "epoch: 52, batch: 104, loss: 1.3213943243026733\n",
      "epoch: 52, batch: 105, loss: 1.410010814666748\n",
      "epoch: 52, batch: 106, loss: 1.4854457378387451\n",
      "epoch: 52, batch: 107, loss: 1.5452181100845337\n",
      "epoch: 52, batch: 108, loss: 1.6374962329864502\n",
      "epoch: 52, batch: 109, loss: 1.4390745162963867\n",
      "epoch: 52, batch: 110, loss: 1.7593415975570679\n",
      "epoch: 52, batch: 111, loss: 1.6455625295639038\n",
      "epoch: 52, batch: 112, loss: 1.321195125579834\n",
      "epoch: 52, batch: 113, loss: 1.5667579174041748\n",
      "epoch: 52, batch: 114, loss: 1.5992038249969482\n",
      "epoch: 52, batch: 115, loss: 1.3634525537490845\n",
      "epoch: 52, batch: 116, loss: 1.659054160118103\n",
      "epoch: 52, batch: 117, loss: 1.5700315237045288\n",
      "epoch: 52, batch: 118, loss: 1.4430174827575684\n",
      "epoch: 52, batch: 119, loss: 1.3674230575561523\n",
      "epoch: 52, batch: 120, loss: 1.481271505355835\n",
      "epoch: 52, batch: 121, loss: 1.6590973138809204\n",
      "epoch: 52, batch: 122, loss: 1.3250213861465454\n",
      "epoch: 52, batch: 123, loss: 1.4348182678222656\n",
      "epoch: 52, batch: 124, loss: 1.8276065587997437\n",
      "epoch: 52, batch: 125, loss: 1.4987938404083252\n",
      "epoch: 52, batch: 126, loss: 1.5951224565505981\n",
      "epoch: 52, batch: 127, loss: 1.6593071222305298\n",
      "epoch: 53, batch: 0, loss: 1.4429200887680054\n",
      "epoch: 53, batch: 1, loss: 1.549316644668579\n",
      "epoch: 53, batch: 2, loss: 1.5276063680648804\n",
      "epoch: 53, batch: 3, loss: 1.7637217044830322\n",
      "epoch: 53, batch: 4, loss: 1.502868413925171\n",
      "epoch: 53, batch: 5, loss: 1.367142915725708\n",
      "epoch: 53, batch: 6, loss: 1.3206571340560913\n",
      "epoch: 53, batch: 7, loss: 1.6592235565185547\n",
      "epoch: 53, batch: 8, loss: 1.685121774673462\n",
      "epoch: 53, batch: 9, loss: 1.599204421043396\n",
      "epoch: 53, batch: 10, loss: 1.781507134437561\n",
      "epoch: 53, batch: 11, loss: 1.367020606994629\n",
      "epoch: 53, batch: 12, loss: 1.845741629600525\n",
      "epoch: 53, batch: 13, loss: 1.481029987335205\n",
      "epoch: 53, batch: 14, loss: 1.384678602218628\n",
      "epoch: 53, batch: 15, loss: 1.3245645761489868\n",
      "epoch: 53, batch: 16, loss: 1.3245421648025513\n",
      "epoch: 53, batch: 17, loss: 1.3627698421478271\n",
      "epoch: 53, batch: 18, loss: 1.603325605392456\n",
      "epoch: 53, batch: 19, loss: 1.5028570890426636\n",
      "epoch: 53, batch: 20, loss: 1.431126594543457\n",
      "epoch: 53, batch: 21, loss: 1.2020399570465088\n",
      "epoch: 53, batch: 22, loss: 1.7817751169204712\n",
      "epoch: 53, batch: 23, loss: 1.8078701496124268\n",
      "epoch: 53, batch: 24, loss: 1.4091448783874512\n",
      "epoch: 53, batch: 25, loss: 1.5713441371917725\n",
      "epoch: 53, batch: 26, loss: 1.5917847156524658\n",
      "epoch: 53, batch: 27, loss: 1.3886690139770508\n",
      "epoch: 53, batch: 28, loss: 1.434316873550415\n",
      "epoch: 53, batch: 29, loss: 1.4808452129364014\n",
      "epoch: 53, batch: 30, loss: 1.3666225671768188\n",
      "epoch: 53, batch: 31, loss: 1.2702691555023193\n",
      "epoch: 53, batch: 32, loss: 1.4310303926467896\n",
      "epoch: 53, batch: 33, loss: 1.3665568828582764\n",
      "epoch: 53, batch: 34, loss: 1.480787754058838\n",
      "epoch: 53, batch: 35, loss: 1.759993314743042\n",
      "epoch: 53, batch: 36, loss: 1.4383366107940674\n",
      "epoch: 53, batch: 37, loss: 1.5028406381607056\n",
      "epoch: 53, batch: 38, loss: 1.6457313299179077\n",
      "epoch: 53, batch: 39, loss: 1.201422095298767\n",
      "epoch: 53, batch: 40, loss: 1.506964921951294\n",
      "epoch: 53, batch: 41, loss: 1.709367036819458\n",
      "epoch: 53, batch: 42, loss: 1.4987115859985352\n",
      "epoch: 53, batch: 43, loss: 1.3622099161148071\n",
      "epoch: 53, batch: 44, loss: 1.5452872514724731\n",
      "epoch: 53, batch: 45, loss: 1.5231183767318726\n",
      "epoch: 53, batch: 46, loss: 1.5566949844360352\n",
      "epoch: 53, batch: 47, loss: 1.5918734073638916\n",
      "epoch: 53, batch: 48, loss: 1.8470127582550049\n",
      "epoch: 53, batch: 49, loss: 1.5452982187271118\n",
      "epoch: 53, batch: 50, loss: 1.706316590309143\n",
      "epoch: 53, batch: 51, loss: 1.6638562679290771\n",
      "epoch: 53, batch: 52, loss: 1.442212700843811\n",
      "epoch: 53, batch: 53, loss: 1.4805701971054077\n",
      "epoch: 53, batch: 54, loss: 1.663892388343811\n",
      "epoch: 53, batch: 55, loss: 1.3235714435577393\n",
      "epoch: 53, batch: 56, loss: 1.4380462169647217\n",
      "epoch: 53, batch: 57, loss: 1.6821253299713135\n",
      "epoch: 53, batch: 58, loss: 1.4805129766464233\n",
      "epoch: 53, batch: 59, loss: 1.662976622581482\n",
      "epoch: 53, batch: 60, loss: 1.5270869731903076\n",
      "epoch: 53, batch: 61, loss: 1.361825704574585\n",
      "epoch: 53, batch: 62, loss: 1.641636610031128\n",
      "epoch: 53, batch: 63, loss: 1.637547254562378\n",
      "epoch: 53, batch: 64, loss: 1.7178081274032593\n",
      "epoch: 53, batch: 65, loss: 1.384124755859375\n",
      "epoch: 53, batch: 66, loss: 1.6032081842422485\n",
      "epoch: 53, batch: 67, loss: 1.4804093837738037\n",
      "epoch: 53, batch: 68, loss: 1.8494956493377686\n",
      "epoch: 53, batch: 69, loss: 1.778721570968628\n",
      "epoch: 53, batch: 70, loss: 1.2044440507888794\n",
      "epoch: 53, batch: 71, loss: 1.4296754598617554\n",
      "epoch: 53, batch: 72, loss: 1.7828662395477295\n",
      "epoch: 53, batch: 73, loss: 1.498739242553711\n",
      "epoch: 53, batch: 74, loss: 1.5068849325180054\n",
      "epoch: 53, batch: 75, loss: 1.246813178062439\n",
      "epoch: 53, batch: 76, loss: 1.5269193649291992\n",
      "epoch: 53, batch: 77, loss: 1.4802930355072021\n",
      "epoch: 53, batch: 78, loss: 1.4987471103668213\n",
      "epoch: 53, batch: 79, loss: 1.4336522817611694\n",
      "epoch: 53, batch: 80, loss: 1.7067582607269287\n",
      "epoch: 53, batch: 81, loss: 1.4802461862564087\n",
      "epoch: 53, batch: 82, loss: 1.6416479349136353\n",
      "epoch: 53, batch: 83, loss: 1.3147406578063965\n",
      "epoch: 53, batch: 84, loss: 1.20395827293396\n",
      "epoch: 53, batch: 85, loss: 1.5950255393981934\n",
      "epoch: 53, batch: 86, loss: 1.318726897239685\n",
      "epoch: 53, batch: 87, loss: 1.506862998008728\n",
      "epoch: 53, batch: 88, loss: 1.4335390329360962\n",
      "epoch: 53, batch: 89, loss: 1.7098915576934814\n",
      "epoch: 53, batch: 90, loss: 1.480140209197998\n",
      "epoch: 53, batch: 91, loss: 1.545393705368042\n",
      "epoch: 53, batch: 92, loss: 1.3185858726501465\n",
      "epoch: 53, batch: 93, loss: 1.4841492176055908\n",
      "epoch: 53, batch: 94, loss: 1.437504529953003\n",
      "epoch: 53, batch: 95, loss: 1.4841238260269165\n",
      "epoch: 53, batch: 96, loss: 1.556460976600647\n",
      "epoch: 53, batch: 97, loss: 1.709965705871582\n",
      "epoch: 53, batch: 98, loss: 1.6218147277832031\n",
      "epoch: 53, batch: 99, loss: 1.6882884502410889\n",
      "epoch: 53, batch: 100, loss: 1.6456855535507202\n",
      "epoch: 53, batch: 101, loss: 1.480010747909546\n",
      "epoch: 53, batch: 102, loss: 1.5028104782104492\n",
      "epoch: 53, batch: 103, loss: 1.849958062171936\n",
      "epoch: 53, batch: 104, loss: 1.3609182834625244\n",
      "epoch: 53, batch: 105, loss: 1.7111454010009766\n",
      "epoch: 53, batch: 106, loss: 1.9185737371444702\n",
      "epoch: 53, batch: 107, loss: 1.7340421676635742\n",
      "epoch: 53, batch: 108, loss: 1.545440435409546\n",
      "epoch: 53, batch: 109, loss: 1.4758946895599365\n",
      "epoch: 53, batch: 110, loss: 1.6376276016235352\n",
      "epoch: 53, batch: 111, loss: 1.6874862909317017\n",
      "epoch: 53, batch: 112, loss: 1.1989907026290894\n",
      "epoch: 53, batch: 113, loss: 1.6686004400253296\n",
      "epoch: 53, batch: 114, loss: 1.1989279985427856\n",
      "epoch: 53, batch: 115, loss: 1.6606041193008423\n",
      "epoch: 53, batch: 116, loss: 1.722151517868042\n",
      "epoch: 53, batch: 117, loss: 1.5949817895889282\n",
      "epoch: 53, batch: 118, loss: 1.783820390701294\n",
      "epoch: 53, batch: 119, loss: 1.313955545425415\n",
      "epoch: 53, batch: 120, loss: 1.6606721878051758\n",
      "epoch: 53, batch: 121, loss: 1.506813645362854\n",
      "epoch: 53, batch: 122, loss: 1.5494763851165771\n",
      "epoch: 53, batch: 123, loss: 1.3875949382781982\n",
      "epoch: 53, batch: 124, loss: 1.2452738285064697\n",
      "epoch: 53, batch: 125, loss: 1.5028223991394043\n",
      "epoch: 53, batch: 126, loss: 1.4330532550811768\n",
      "epoch: 53, batch: 127, loss: 1.6610604524612427\n",
      "epoch: 54, batch: 0, loss: 1.6416215896606445\n",
      "epoch: 54, batch: 1, loss: 1.7114289999008179\n",
      "epoch: 54, batch: 2, loss: 1.5949612855911255\n",
      "epoch: 54, batch: 3, loss: 1.5989322662353516\n",
      "epoch: 54, batch: 4, loss: 1.549485445022583\n",
      "epoch: 54, batch: 5, loss: 1.6608492136001587\n",
      "epoch: 54, batch: 6, loss: 1.0829663276672363\n",
      "epoch: 54, batch: 7, loss: 1.360285997390747\n",
      "epoch: 54, batch: 8, loss: 1.3215314149856567\n",
      "epoch: 54, batch: 9, loss: 1.4368798732757568\n",
      "epoch: 54, batch: 10, loss: 1.622174859046936\n",
      "epoch: 54, batch: 11, loss: 1.4755984544754028\n",
      "epoch: 54, batch: 12, loss: 1.3174799680709839\n",
      "epoch: 54, batch: 13, loss: 1.502837061882019\n",
      "epoch: 54, batch: 14, loss: 1.4988901615142822\n",
      "epoch: 54, batch: 15, loss: 1.780368447303772\n",
      "epoch: 54, batch: 16, loss: 1.6688861846923828\n",
      "epoch: 54, batch: 17, loss: 1.556165337562561\n",
      "epoch: 54, batch: 18, loss: 1.5949375629425049\n",
      "epoch: 54, batch: 19, loss: 1.4989054203033447\n",
      "epoch: 54, batch: 20, loss: 1.3133641481399536\n",
      "epoch: 54, batch: 21, loss: 1.3172754049301147\n",
      "epoch: 54, batch: 22, loss: 1.317252516746521\n",
      "epoch: 54, batch: 23, loss: 1.618364691734314\n",
      "epoch: 54, batch: 24, loss: 1.7884175777435303\n",
      "epoch: 54, batch: 25, loss: 1.5067689418792725\n",
      "epoch: 54, batch: 26, loss: 1.5028451681137085\n",
      "epoch: 54, batch: 27, loss: 1.4793610572814941\n",
      "epoch: 54, batch: 28, loss: 1.645503044128418\n",
      "epoch: 54, batch: 29, loss: 1.598833680152893\n",
      "epoch: 54, batch: 30, loss: 1.2442305088043213\n",
      "epoch: 54, batch: 31, loss: 1.7846282720565796\n",
      "epoch: 54, batch: 32, loss: 1.780739188194275\n",
      "epoch: 54, batch: 33, loss: 1.4404277801513672\n",
      "epoch: 54, batch: 34, loss: 1.406419038772583\n",
      "epoch: 54, batch: 35, loss: 1.5220425128936768\n",
      "epoch: 54, batch: 36, loss: 1.6223938465118408\n",
      "epoch: 54, batch: 37, loss: 1.4299697875976562\n",
      "epoch: 54, batch: 38, loss: 1.780869722366333\n",
      "epoch: 54, batch: 39, loss: 1.5949002504348755\n",
      "epoch: 54, batch: 40, loss: 1.5948984622955322\n",
      "epoch: 54, batch: 41, loss: 1.3129262924194336\n",
      "epoch: 54, batch: 42, loss: 1.3167840242385864\n",
      "epoch: 54, batch: 43, loss: 1.7356191873550415\n",
      "epoch: 54, batch: 44, loss: 1.5495356321334839\n",
      "epoch: 54, batch: 45, loss: 1.5987575054168701\n",
      "epoch: 54, batch: 46, loss: 1.5732495784759521\n",
      "epoch: 54, batch: 47, loss: 1.594885230064392\n",
      "epoch: 54, batch: 48, loss: 1.4362739324569702\n",
      "epoch: 54, batch: 49, loss: 1.5732903480529785\n",
      "epoch: 54, batch: 50, loss: 1.5495452880859375\n",
      "epoch: 54, batch: 51, loss: 1.6891191005706787\n",
      "epoch: 54, batch: 52, loss: 1.5028460025787354\n",
      "epoch: 54, batch: 53, loss: 1.59871506690979\n",
      "epoch: 54, batch: 54, loss: 1.8050216436386108\n",
      "epoch: 54, batch: 55, loss: 1.4751949310302734\n",
      "epoch: 54, batch: 56, loss: 1.6025279760360718\n",
      "epoch: 54, batch: 57, loss: 1.7321447134017944\n",
      "epoch: 54, batch: 58, loss: 1.3592987060546875\n",
      "epoch: 54, batch: 59, loss: 1.7851423025131226\n",
      "epoch: 54, batch: 60, loss: 1.6377655267715454\n",
      "epoch: 54, batch: 61, loss: 1.4789575338363647\n",
      "epoch: 54, batch: 62, loss: 1.5986640453338623\n",
      "epoch: 54, batch: 63, loss: 1.4751328229904175\n",
      "epoch: 54, batch: 64, loss: 1.4789222478866577\n",
      "epoch: 54, batch: 65, loss: 1.36296808719635\n",
      "epoch: 54, batch: 66, loss: 1.7791341543197632\n",
      "epoch: 54, batch: 67, loss: 1.54578697681427\n",
      "epoch: 54, batch: 68, loss: 1.405853271484375\n",
      "epoch: 54, batch: 69, loss: 1.6165109872817993\n",
      "epoch: 54, batch: 70, loss: 1.5255863666534424\n",
      "epoch: 54, batch: 71, loss: 1.7363202571868896\n",
      "epoch: 54, batch: 72, loss: 1.3590500354766846\n",
      "epoch: 54, batch: 73, loss: 1.3868213891983032\n",
      "epoch: 54, batch: 74, loss: 1.6023517847061157\n",
      "epoch: 54, batch: 75, loss: 1.6453279256820679\n",
      "epoch: 54, batch: 76, loss: 1.6693875789642334\n",
      "epoch: 54, batch: 77, loss: 1.3122186660766602\n",
      "epoch: 54, batch: 78, loss: 1.5217559337615967\n",
      "epoch: 54, batch: 79, loss: 1.6415694952011108\n",
      "epoch: 54, batch: 80, loss: 1.5217409133911133\n",
      "epoch: 54, batch: 81, loss: 1.35888671875\n",
      "epoch: 54, batch: 82, loss: 1.7221094369888306\n",
      "epoch: 54, batch: 83, loss: 1.5684558153152466\n",
      "epoch: 54, batch: 84, loss: 1.4356586933135986\n",
      "epoch: 54, batch: 85, loss: 1.4991449117660522\n",
      "epoch: 54, batch: 86, loss: 1.3120601177215576\n",
      "epoch: 54, batch: 87, loss: 1.6022155284881592\n",
      "epoch: 54, batch: 88, loss: 1.6620804071426392\n",
      "epoch: 54, batch: 89, loss: 1.439267635345459\n",
      "epoch: 54, batch: 90, loss: 1.4355642795562744\n",
      "epoch: 54, batch: 91, loss: 1.1994364261627197\n",
      "epoch: 54, batch: 92, loss: 1.4355323314666748\n",
      "epoch: 54, batch: 93, loss: 1.684594750404358\n",
      "epoch: 54, batch: 94, loss: 1.3119277954101562\n",
      "epoch: 54, batch: 95, loss: 1.6415319442749023\n",
      "epoch: 54, batch: 96, loss: 1.8325035572052002\n",
      "epoch: 54, batch: 97, loss: 1.5702913999557495\n",
      "epoch: 54, batch: 98, loss: 1.1992042064666748\n",
      "epoch: 54, batch: 99, loss: 1.718380331993103\n",
      "epoch: 54, batch: 100, loss: 1.429640769958496\n",
      "epoch: 54, batch: 101, loss: 1.311816930770874\n",
      "epoch: 54, batch: 102, loss: 1.4784626960754395\n",
      "epoch: 54, batch: 103, loss: 1.1990381479263306\n",
      "epoch: 54, batch: 104, loss: 1.6379039287567139\n",
      "epoch: 54, batch: 105, loss: 1.7895294427871704\n",
      "epoch: 54, batch: 106, loss: 1.4051300287246704\n",
      "epoch: 54, batch: 107, loss: 1.4819824695587158\n",
      "epoch: 54, batch: 108, loss: 1.3583996295928955\n",
      "epoch: 54, batch: 109, loss: 1.435257911682129\n",
      "epoch: 54, batch: 110, loss: 1.3583643436431885\n",
      "epoch: 54, batch: 111, loss: 1.5214769840240479\n",
      "epoch: 54, batch: 112, loss: 1.645026445388794\n",
      "epoch: 54, batch: 113, loss: 1.637933373451233\n",
      "epoch: 54, batch: 114, loss: 1.3186910152435303\n",
      "epoch: 54, batch: 115, loss: 1.622933030128479\n",
      "epoch: 54, batch: 116, loss: 1.8538786172866821\n",
      "epoch: 54, batch: 117, loss: 1.3828752040863037\n",
      "epoch: 54, batch: 118, loss: 1.7127612829208374\n",
      "epoch: 54, batch: 119, loss: 1.4782508611679077\n",
      "epoch: 54, batch: 120, loss: 1.3828672170639038\n",
      "epoch: 54, batch: 121, loss: 1.6264629364013672\n",
      "epoch: 54, batch: 122, loss: 1.5029242038726807\n",
      "epoch: 54, batch: 123, loss: 1.3863284587860107\n",
      "epoch: 54, batch: 124, loss: 1.616079330444336\n",
      "epoch: 54, batch: 125, loss: 1.7341123819351196\n",
      "epoch: 54, batch: 126, loss: 1.4349861145019531\n",
      "epoch: 54, batch: 127, loss: 1.282230019569397\n",
      "epoch: 55, batch: 0, loss: 1.691017508506775\n",
      "epoch: 55, batch: 1, loss: 1.3580272197723389\n",
      "epoch: 55, batch: 2, loss: 1.358009696006775\n",
      "epoch: 55, batch: 3, loss: 1.3182051181793213\n",
      "epoch: 55, batch: 4, loss: 1.712856650352478\n",
      "epoch: 55, batch: 5, loss: 1.6380027532577515\n",
      "epoch: 55, batch: 6, loss: 1.478062391281128\n",
      "epoch: 55, batch: 7, loss: 1.786421537399292\n",
      "epoch: 55, batch: 8, loss: 1.5495383739471436\n",
      "epoch: 55, batch: 9, loss: 1.3113105297088623\n",
      "epoch: 55, batch: 10, loss: 1.4293898344039917\n",
      "epoch: 55, batch: 11, loss: 1.5063108205795288\n",
      "epoch: 55, batch: 12, loss: 1.382813572883606\n",
      "epoch: 55, batch: 13, loss: 1.5062999725341797\n",
      "epoch: 55, batch: 14, loss: 1.8297674655914307\n",
      "epoch: 55, batch: 15, loss: 1.5515739917755127\n",
      "epoch: 55, batch: 16, loss: 1.42808997631073\n",
      "epoch: 55, batch: 17, loss: 1.477925181388855\n",
      "epoch: 55, batch: 18, loss: 1.240826964378357\n",
      "epoch: 55, batch: 19, loss: 1.6845886707305908\n",
      "epoch: 55, batch: 20, loss: 1.3609626293182373\n",
      "epoch: 55, batch: 21, loss: 1.2658483982086182\n",
      "epoch: 55, batch: 22, loss: 1.5915578603744507\n",
      "epoch: 55, batch: 23, loss: 1.3143752813339233\n",
      "epoch: 55, batch: 24, loss: 1.6878271102905273\n",
      "epoch: 55, batch: 25, loss: 1.7097594738006592\n",
      "epoch: 55, batch: 26, loss: 1.5980453491210938\n",
      "epoch: 55, batch: 27, loss: 1.5210721492767334\n",
      "epoch: 55, batch: 28, loss: 1.3859760761260986\n",
      "epoch: 55, batch: 29, loss: 1.7835842370986938\n",
      "epoch: 55, batch: 30, loss: 1.1939728260040283\n",
      "epoch: 55, batch: 31, loss: 1.5547324419021606\n",
      "epoch: 55, batch: 32, loss: 1.5242012739181519\n",
      "epoch: 55, batch: 33, loss: 1.5463078022003174\n",
      "epoch: 55, batch: 34, loss: 1.7351932525634766\n",
      "epoch: 55, batch: 35, loss: 1.382743000984192\n",
      "epoch: 55, batch: 36, loss: 1.7837568521499634\n",
      "epoch: 55, batch: 37, loss: 1.4808192253112793\n",
      "epoch: 55, batch: 38, loss: 1.5061763525009155\n",
      "epoch: 55, batch: 39, loss: 1.5577977895736694\n",
      "epoch: 55, batch: 40, loss: 1.4745148420333862\n",
      "epoch: 55, batch: 41, loss: 1.5030491352081299\n",
      "epoch: 55, batch: 42, loss: 1.313982367515564\n",
      "epoch: 55, batch: 43, loss: 1.5673000812530518\n",
      "epoch: 55, batch: 44, loss: 1.5061506032943726\n",
      "epoch: 55, batch: 45, loss: 1.549450159072876\n",
      "epoch: 55, batch: 46, loss: 1.594840407371521\n",
      "epoch: 55, batch: 47, loss: 1.6667369604110718\n",
      "epoch: 55, batch: 48, loss: 1.6892235279083252\n",
      "epoch: 55, batch: 49, loss: 1.6203960180282593\n",
      "epoch: 55, batch: 50, loss: 1.2397973537445068\n",
      "epoch: 55, batch: 51, loss: 1.5208139419555664\n",
      "epoch: 55, batch: 52, loss: 1.743866205215454\n",
      "epoch: 55, batch: 53, loss: 1.5207923650741577\n",
      "epoch: 55, batch: 54, loss: 1.5978635549545288\n",
      "epoch: 55, batch: 55, loss: 1.8768829107284546\n",
      "epoch: 55, batch: 56, loss: 1.6235122680664062\n",
      "epoch: 55, batch: 57, loss: 1.7152763605117798\n",
      "epoch: 55, batch: 58, loss: 1.6668586730957031\n",
      "epoch: 55, batch: 59, loss: 1.2395049333572388\n",
      "epoch: 55, batch: 60, loss: 1.47738516330719\n",
      "epoch: 55, batch: 61, loss: 1.3106423616409302\n",
      "epoch: 55, batch: 62, loss: 1.663956642150879\n",
      "epoch: 55, batch: 63, loss: 1.4773470163345337\n",
      "epoch: 55, batch: 64, loss: 1.7360540628433228\n",
      "epoch: 55, batch: 65, loss: 1.1930510997772217\n",
      "epoch: 55, batch: 66, loss: 1.6749109029769897\n",
      "epoch: 55, batch: 67, loss: 1.830775499343872\n",
      "epoch: 55, batch: 68, loss: 1.4288984537124634\n",
      "epoch: 55, batch: 69, loss: 1.5544145107269287\n",
      "epoch: 55, batch: 70, loss: 1.6265233755111694\n",
      "epoch: 55, batch: 71, loss: 1.6929080486297607\n",
      "epoch: 55, batch: 72, loss: 1.597737193107605\n",
      "epoch: 55, batch: 73, loss: 1.738471269607544\n",
      "epoch: 55, batch: 74, loss: 1.5724748373031616\n",
      "epoch: 55, batch: 75, loss: 1.47719407081604\n",
      "epoch: 55, batch: 76, loss: 1.5977067947387695\n",
      "epoch: 55, batch: 77, loss: 1.4771686792373657\n",
      "epoch: 55, batch: 78, loss: 1.2388970851898193\n",
      "epoch: 55, batch: 79, loss: 1.4771432876586914\n",
      "epoch: 55, batch: 80, loss: 1.6438350677490234\n",
      "epoch: 55, batch: 81, loss: 1.4743331670761108\n",
      "epoch: 55, batch: 82, loss: 1.6671186685562134\n",
      "epoch: 55, batch: 83, loss: 1.3159271478652954\n",
      "epoch: 55, batch: 84, loss: 1.7182060480117798\n",
      "epoch: 55, batch: 85, loss: 1.8310970067977905\n",
      "epoch: 55, batch: 86, loss: 1.66990065574646\n",
      "epoch: 55, batch: 87, loss: 1.7877483367919922\n",
      "epoch: 55, batch: 88, loss: 1.5466029644012451\n",
      "epoch: 55, batch: 89, loss: 1.6409895420074463\n",
      "epoch: 55, batch: 90, loss: 1.5664880275726318\n",
      "epoch: 55, batch: 91, loss: 1.5466231107711792\n",
      "epoch: 55, batch: 92, loss: 1.479657769203186\n",
      "epoch: 55, batch: 93, loss: 1.1950361728668213\n",
      "epoch: 55, batch: 94, loss: 1.4796113967895508\n",
      "epoch: 55, batch: 95, loss: 1.5493000745773315\n",
      "epoch: 55, batch: 96, loss: 1.6190283298492432\n",
      "epoch: 55, batch: 97, loss: 1.1922755241394043\n",
      "epoch: 55, batch: 98, loss: 1.433486819267273\n",
      "epoch: 55, batch: 99, loss: 1.5975068807601929\n",
      "epoch: 55, batch: 100, loss: 1.5202913284301758\n",
      "epoch: 55, batch: 101, loss: 1.5006885528564453\n",
      "epoch: 55, batch: 102, loss: 1.7181084156036377\n",
      "epoch: 55, batch: 103, loss: 1.741988182067871\n",
      "epoch: 55, batch: 104, loss: 1.433394193649292\n",
      "epoch: 55, batch: 105, loss: 1.5492591857910156\n",
      "epoch: 55, batch: 106, loss: 1.3152462244033813\n",
      "epoch: 55, batch: 107, loss: 1.474267840385437\n",
      "epoch: 55, batch: 108, loss: 1.5058081150054932\n",
      "epoch: 55, batch: 109, loss: 1.3126636743545532\n",
      "epoch: 55, batch: 110, loss: 1.5226657390594482\n",
      "epoch: 55, batch: 111, loss: 1.4742639064788818\n",
      "epoch: 55, batch: 112, loss: 1.7856395244598389\n",
      "epoch: 55, batch: 113, loss: 1.5973659753799438\n",
      "epoch: 55, batch: 114, loss: 1.3149983882904053\n",
      "epoch: 55, batch: 115, loss: 1.4766780138015747\n",
      "epoch: 55, batch: 116, loss: 1.3850769996643066\n",
      "epoch: 55, batch: 117, loss: 1.1942325830459595\n",
      "epoch: 55, batch: 118, loss: 1.3826786279678345\n",
      "epoch: 55, batch: 119, loss: 1.5925719738006592\n",
      "epoch: 55, batch: 120, loss: 1.597290277481079\n",
      "epoch: 55, batch: 121, loss: 1.3147757053375244\n",
      "epoch: 55, batch: 122, loss: 1.4330997467041016\n",
      "epoch: 55, batch: 123, loss: 1.264319658279419\n",
      "epoch: 55, batch: 124, loss: 1.5033786296844482\n",
      "epoch: 55, batch: 125, loss: 1.7110698223114014\n",
      "epoch: 55, batch: 126, loss: 1.5491634607315063\n",
      "epoch: 55, batch: 127, loss: 1.577352523803711\n",
      "epoch: 56, batch: 0, loss: 1.3558166027069092\n",
      "epoch: 56, batch: 1, loss: 1.3849364519119263\n",
      "epoch: 56, batch: 2, loss: 1.428447961807251\n",
      "epoch: 56, batch: 3, loss: 1.4307390451431274\n",
      "epoch: 56, batch: 4, loss: 1.5469443798065186\n",
      "epoch: 56, batch: 5, loss: 1.4350836277008057\n",
      "epoch: 56, batch: 6, loss: 1.3121861219406128\n",
      "epoch: 56, batch: 7, loss: 1.3143151998519897\n",
      "epoch: 56, batch: 8, loss: 1.686349868774414\n",
      "epoch: 56, batch: 9, loss: 1.6863346099853516\n",
      "epoch: 56, batch: 10, loss: 1.640652060508728\n",
      "epoch: 56, batch: 11, loss: 1.434893012046814\n",
      "epoch: 56, batch: 12, loss: 1.5970685482025146\n",
      "epoch: 56, batch: 13, loss: 1.6406329870224\n",
      "epoch: 56, batch: 14, loss: 1.4763282537460327\n",
      "epoch: 56, batch: 15, loss: 1.6928722858428955\n",
      "epoch: 56, batch: 16, loss: 1.5950051546096802\n",
      "epoch: 56, batch: 17, loss: 1.6677837371826172\n",
      "epoch: 56, batch: 18, loss: 1.6222070455551147\n",
      "epoch: 56, batch: 19, loss: 1.3847229480743408\n",
      "epoch: 56, batch: 20, loss: 1.6678154468536377\n",
      "epoch: 56, batch: 21, loss: 1.6678260564804077\n",
      "epoch: 56, batch: 22, loss: 1.6861175298690796\n",
      "epoch: 56, batch: 23, loss: 1.6678472757339478\n",
      "epoch: 56, batch: 24, loss: 1.5053924322128296\n",
      "epoch: 56, batch: 25, loss: 1.1929676532745361\n",
      "epoch: 56, batch: 26, loss: 1.1929320096969604\n",
      "epoch: 56, batch: 27, loss: 1.6678895950317383\n",
      "epoch: 56, batch: 28, loss: 1.593216896057129\n",
      "epoch: 56, batch: 29, loss: 1.5926470756530762\n",
      "epoch: 56, batch: 30, loss: 1.190990686416626\n",
      "epoch: 56, batch: 31, loss: 1.8049074411392212\n",
      "epoch: 56, batch: 32, loss: 1.5035362243652344\n",
      "epoch: 56, batch: 33, loss: 1.6242778301239014\n",
      "epoch: 56, batch: 34, loss: 1.3099157810211182\n",
      "epoch: 56, batch: 35, loss: 1.7133762836456299\n",
      "epoch: 56, batch: 36, loss: 1.551389455795288\n",
      "epoch: 56, batch: 37, loss: 1.3099080324172974\n",
      "epoch: 56, batch: 38, loss: 1.7417083978652954\n",
      "epoch: 56, batch: 39, loss: 1.625967025756836\n",
      "epoch: 56, batch: 40, loss: 1.1907949447631836\n",
      "epoch: 56, batch: 41, loss: 1.553006887435913\n",
      "epoch: 56, batch: 42, loss: 1.3115003108978271\n",
      "epoch: 56, batch: 43, loss: 1.263689637184143\n",
      "epoch: 56, batch: 44, loss: 1.7174285650253296\n",
      "epoch: 56, batch: 45, loss: 1.4759232997894287\n",
      "epoch: 56, batch: 46, loss: 1.595115303993225\n",
      "epoch: 56, batch: 47, loss: 1.5036084651947021\n",
      "epoch: 56, batch: 48, loss: 1.235898494720459\n",
      "epoch: 56, batch: 49, loss: 1.7873761653900146\n",
      "epoch: 56, batch: 50, loss: 1.5196207761764526\n",
      "epoch: 56, batch: 51, loss: 1.433538794517517\n",
      "epoch: 56, batch: 52, loss: 1.5474045276641846\n",
      "epoch: 56, batch: 53, loss: 1.7571547031402588\n",
      "epoch: 56, batch: 54, loss: 1.7849935293197632\n",
      "epoch: 56, batch: 55, loss: 1.595146894454956\n",
      "epoch: 56, batch: 56, loss: 1.7119948863983154\n",
      "epoch: 56, batch: 57, loss: 1.5036523342132568\n",
      "epoch: 56, batch: 58, loss: 1.5767173767089844\n",
      "epoch: 56, batch: 59, loss: 1.5049936771392822\n",
      "epoch: 56, batch: 60, loss: 1.5474870204925537\n",
      "epoch: 56, batch: 61, loss: 1.190385103225708\n",
      "epoch: 56, batch: 62, loss: 1.5475075244903564\n",
      "epoch: 56, batch: 63, loss: 1.9777839183807373\n",
      "epoch: 56, batch: 64, loss: 1.070824146270752\n",
      "epoch: 56, batch: 65, loss: 1.596417784690857\n",
      "epoch: 56, batch: 66, loss: 1.5926182270050049\n",
      "epoch: 56, batch: 67, loss: 1.5194950103759766\n",
      "epoch: 56, batch: 68, loss: 1.548752784729004\n",
      "epoch: 56, batch: 69, loss: 1.4744417667388916\n",
      "epoch: 56, batch: 70, loss: 1.4744447469711304\n",
      "epoch: 56, batch: 71, loss: 1.502582311630249\n",
      "epoch: 56, batch: 72, loss: 1.596334457397461\n",
      "epoch: 56, batch: 73, loss: 1.6390994787216187\n",
      "epoch: 56, batch: 74, loss: 1.4755465984344482\n",
      "epoch: 56, batch: 75, loss: 1.76203191280365\n",
      "epoch: 56, batch: 76, loss: 1.6851348876953125\n",
      "epoch: 56, batch: 77, loss: 1.5534254312515259\n",
      "epoch: 56, batch: 78, loss: 1.667400598526001\n",
      "epoch: 56, batch: 79, loss: 1.3557207584381104\n",
      "epoch: 56, batch: 80, loss: 1.354693055152893\n",
      "epoch: 56, batch: 81, loss: 1.5972131490707397\n",
      "epoch: 56, batch: 82, loss: 1.624559760093689\n",
      "epoch: 56, batch: 83, loss: 1.4315145015716553\n",
      "epoch: 56, batch: 84, loss: 1.6849628686904907\n",
      "epoch: 56, batch: 85, loss: 1.594348669052124\n",
      "epoch: 56, batch: 86, loss: 1.476289987564087\n",
      "epoch: 56, batch: 87, loss: 1.5770599842071533\n",
      "epoch: 56, batch: 88, loss: 1.3993721008300781\n",
      "epoch: 56, batch: 89, loss: 1.3097797632217407\n",
      "epoch: 56, batch: 90, loss: 1.431402325630188\n",
      "epoch: 56, batch: 91, loss: 1.5961105823516846\n",
      "epoch: 56, batch: 92, loss: 1.6246349811553955\n",
      "epoch: 56, batch: 93, loss: 1.31134831905365\n",
      "epoch: 56, batch: 94, loss: 1.4277784824371338\n",
      "epoch: 56, batch: 95, loss: 1.4745174646377563\n",
      "epoch: 56, batch: 96, loss: 1.3112459182739258\n",
      "epoch: 56, batch: 97, loss: 1.6839395761489868\n",
      "epoch: 56, batch: 98, loss: 1.6399768590927124\n",
      "epoch: 56, batch: 99, loss: 1.3104617595672607\n",
      "epoch: 56, batch: 100, loss: 1.5953319072723389\n",
      "epoch: 56, batch: 101, loss: 1.5191514492034912\n",
      "epoch: 56, batch: 102, loss: 1.3104114532470703\n",
      "epoch: 56, batch: 103, loss: 1.190209150314331\n",
      "epoch: 56, batch: 104, loss: 1.5959434509277344\n",
      "epoch: 56, batch: 105, loss: 1.5039324760437012\n",
      "epoch: 56, batch: 106, loss: 1.5039395093917847\n",
      "epoch: 56, batch: 107, loss: 1.5964443683624268\n",
      "epoch: 56, batch: 108, loss: 1.3542943000793457\n",
      "epoch: 56, batch: 109, loss: 1.906902551651001\n",
      "epoch: 56, batch: 110, loss: 1.5034810304641724\n",
      "epoch: 56, batch: 111, loss: 1.475520372390747\n",
      "epoch: 56, batch: 112, loss: 1.354689121246338\n",
      "epoch: 56, batch: 113, loss: 1.503988265991211\n",
      "epoch: 56, batch: 114, loss: 1.8919779062271118\n",
      "epoch: 56, batch: 115, loss: 1.6684112548828125\n",
      "epoch: 56, batch: 116, loss: 1.7602150440216064\n",
      "epoch: 56, batch: 117, loss: 1.310518503189087\n",
      "epoch: 56, batch: 118, loss: 1.5480366945266724\n",
      "epoch: 56, batch: 119, loss: 1.504030466079712\n",
      "epoch: 56, batch: 120, loss: 1.5517103672027588\n",
      "epoch: 56, batch: 121, loss: 1.475199580192566\n",
      "epoch: 56, batch: 122, loss: 1.624586820602417\n",
      "epoch: 56, batch: 123, loss: 1.4306148290634155\n",
      "epoch: 56, batch: 124, loss: 1.5040650367736816\n",
      "epoch: 56, batch: 125, loss: 1.5956611633300781\n",
      "epoch: 56, batch: 126, loss: 1.7716052532196045\n",
      "epoch: 56, batch: 127, loss: 1.5211364030838013\n",
      "epoch: 57, batch: 0, loss: 1.30997633934021\n",
      "epoch: 57, batch: 1, loss: 1.504098653793335\n",
      "epoch: 57, batch: 2, loss: 1.3540009260177612\n",
      "epoch: 57, batch: 3, loss: 1.4307187795639038\n",
      "epoch: 57, batch: 4, loss: 1.5482571125030518\n",
      "epoch: 57, batch: 5, loss: 1.5041242837905884\n",
      "epoch: 57, batch: 6, loss: 1.4274498224258423\n",
      "epoch: 57, batch: 7, loss: 1.309867262840271\n",
      "epoch: 57, batch: 8, loss: 1.3833541870117188\n",
      "epoch: 57, batch: 9, loss: 1.3098204135894775\n",
      "epoch: 57, batch: 10, loss: 1.397956371307373\n",
      "epoch: 57, batch: 11, loss: 1.5042282342910767\n",
      "epoch: 57, batch: 12, loss: 1.3097671270370483\n",
      "epoch: 57, batch: 13, loss: 1.5186609029769897\n",
      "epoch: 57, batch: 14, loss: 1.3538552522659302\n",
      "epoch: 57, batch: 15, loss: 1.5040359497070312\n",
      "epoch: 57, batch: 16, loss: 1.4303078651428223\n",
      "epoch: 57, batch: 17, loss: 1.6836612224578857\n",
      "epoch: 57, batch: 18, loss: 1.548140525817871\n",
      "epoch: 57, batch: 19, loss: 1.50397789478302\n",
      "epoch: 57, batch: 20, loss: 1.1888468265533447\n",
      "epoch: 57, batch: 21, loss: 1.6691583395004272\n",
      "epoch: 57, batch: 22, loss: 1.309586763381958\n",
      "epoch: 57, batch: 23, loss: 1.5484068393707275\n",
      "epoch: 57, batch: 24, loss: 1.578145146369934\n",
      "epoch: 57, batch: 25, loss: 1.8338123559951782\n",
      "epoch: 57, batch: 26, loss: 1.429911732673645\n",
      "epoch: 57, batch: 27, loss: 1.6394412517547607\n",
      "epoch: 57, batch: 28, loss: 1.787040114402771\n",
      "epoch: 57, batch: 29, loss: 1.5514285564422607\n",
      "epoch: 57, batch: 30, loss: 1.7870814800262451\n",
      "epoch: 57, batch: 31, loss: 1.353173851966858\n",
      "epoch: 57, batch: 32, loss: 1.5181397199630737\n",
      "epoch: 57, batch: 33, loss: 1.2622369527816772\n",
      "epoch: 57, batch: 34, loss: 1.803853988647461\n",
      "epoch: 57, batch: 35, loss: 1.669309377670288\n",
      "epoch: 57, batch: 36, loss: 1.7129932641983032\n",
      "epoch: 57, batch: 37, loss: 1.4306821823120117\n",
      "epoch: 57, batch: 38, loss: 1.1879098415374756\n",
      "epoch: 57, batch: 39, loss: 1.1878712177276611\n",
      "epoch: 57, batch: 40, loss: 1.7478621006011963\n",
      "epoch: 57, batch: 41, loss: 1.7593847513198853\n",
      "epoch: 57, batch: 42, loss: 1.309227705001831\n",
      "epoch: 57, batch: 43, loss: 1.8479187488555908\n",
      "epoch: 57, batch: 44, loss: 1.7172801494598389\n",
      "epoch: 57, batch: 45, loss: 1.473446249961853\n",
      "epoch: 57, batch: 46, loss: 1.47422194480896\n",
      "epoch: 57, batch: 47, loss: 1.5478570461273193\n",
      "epoch: 57, batch: 48, loss: 1.6259791851043701\n",
      "epoch: 57, batch: 49, loss: 1.3827601671218872\n",
      "epoch: 57, batch: 50, loss: 1.5176026821136475\n",
      "epoch: 57, batch: 51, loss: 1.699721097946167\n",
      "epoch: 57, batch: 52, loss: 1.473192572593689\n",
      "epoch: 57, batch: 53, loss: 1.6685268878936768\n",
      "epoch: 57, batch: 54, loss: 1.4307559728622437\n",
      "epoch: 57, batch: 55, loss: 1.309006690979004\n",
      "epoch: 57, batch: 56, loss: 1.308990240097046\n",
      "epoch: 57, batch: 57, loss: 1.6722300052642822\n",
      "epoch: 57, batch: 58, loss: 1.5969773530960083\n",
      "epoch: 57, batch: 59, loss: 1.231483817100525\n",
      "epoch: 57, batch: 60, loss: 1.8475176095962524\n",
      "epoch: 57, batch: 61, loss: 1.7470734119415283\n",
      "epoch: 57, batch: 62, loss: 1.7166376113891602\n",
      "epoch: 57, batch: 63, loss: 1.3532822132110596\n",
      "epoch: 57, batch: 64, loss: 1.5959365367889404\n",
      "epoch: 57, batch: 65, loss: 1.3088397979736328\n",
      "epoch: 57, batch: 66, loss: 1.3088232278823853\n",
      "epoch: 57, batch: 67, loss: 1.1881005764007568\n",
      "epoch: 57, batch: 68, loss: 1.5528663396835327\n",
      "epoch: 57, batch: 69, loss: 1.517004370689392\n",
      "epoch: 57, batch: 70, loss: 1.5501537322998047\n",
      "epoch: 57, batch: 71, loss: 1.4268909692764282\n",
      "epoch: 57, batch: 72, loss: 1.6390342712402344\n",
      "epoch: 57, batch: 73, loss: 1.1880161762237549\n",
      "epoch: 57, batch: 74, loss: 1.5500659942626953\n",
      "epoch: 57, batch: 75, loss: 1.5030632019042969\n",
      "epoch: 57, batch: 76, loss: 1.867182970046997\n",
      "epoch: 57, batch: 77, loss: 1.5944921970367432\n",
      "epoch: 57, batch: 78, loss: 1.3086230754852295\n",
      "epoch: 57, batch: 79, loss: 1.5920064449310303\n",
      "epoch: 57, batch: 80, loss: 1.065625548362732\n",
      "epoch: 57, batch: 81, loss: 1.6714398860931396\n",
      "epoch: 57, batch: 82, loss: 1.6252790689468384\n",
      "epoch: 57, batch: 83, loss: 1.3513765335083008\n",
      "epoch: 57, batch: 84, loss: 1.5029093027114868\n",
      "epoch: 57, batch: 85, loss: 1.7476898431777954\n",
      "epoch: 57, batch: 86, loss: 1.5474262237548828\n",
      "epoch: 57, batch: 87, loss: 1.8500862121582031\n",
      "epoch: 57, batch: 88, loss: 1.3821921348571777\n",
      "epoch: 57, batch: 89, loss: 1.4754936695098877\n",
      "epoch: 57, batch: 90, loss: 1.671752691268921\n",
      "epoch: 57, batch: 91, loss: 1.5477912425994873\n",
      "epoch: 57, batch: 92, loss: 1.6388431787490845\n",
      "epoch: 57, batch: 93, loss: 1.187736988067627\n",
      "epoch: 57, batch: 94, loss: 1.4716260433197021\n",
      "epoch: 57, batch: 95, loss: 1.6679548025131226\n",
      "epoch: 57, batch: 96, loss: 1.6388075351715088\n",
      "epoch: 57, batch: 97, loss: 1.638798713684082\n",
      "epoch: 57, batch: 98, loss: 1.6387897729873657\n",
      "epoch: 57, batch: 99, loss: 1.1876534223556519\n",
      "epoch: 57, batch: 100, loss: 1.7571871280670166\n",
      "epoch: 57, batch: 101, loss: 1.4266564846038818\n",
      "epoch: 57, batch: 102, loss: 1.8331228494644165\n",
      "epoch: 57, batch: 103, loss: 1.428797960281372\n",
      "epoch: 57, batch: 104, loss: 1.5159249305725098\n",
      "epoch: 57, batch: 105, loss: 1.8979747295379639\n",
      "epoch: 57, batch: 106, loss: 1.4310026168823242\n",
      "epoch: 57, batch: 107, loss: 1.4310085773468018\n",
      "epoch: 57, batch: 108, loss: 1.7145702838897705\n",
      "epoch: 57, batch: 109, loss: 1.5939711332321167\n",
      "epoch: 57, batch: 110, loss: 1.5963220596313477\n",
      "epoch: 57, batch: 111, loss: 1.6410582065582275\n",
      "epoch: 57, batch: 112, loss: 1.5072393417358398\n",
      "epoch: 57, batch: 113, loss: 1.712426781654358\n",
      "epoch: 57, batch: 114, loss: 1.596358060836792\n",
      "epoch: 57, batch: 115, loss: 1.4265419244766235\n",
      "epoch: 57, batch: 116, loss: 1.5811599493026733\n",
      "epoch: 57, batch: 117, loss: 1.6254065036773682\n",
      "epoch: 57, batch: 118, loss: 1.748530387878418\n",
      "epoch: 57, batch: 119, loss: 1.6411807537078857\n",
      "epoch: 57, batch: 120, loss: 1.5075061321258545\n",
      "epoch: 57, batch: 121, loss: 1.4732334613800049\n",
      "epoch: 57, batch: 122, loss: 1.384365439414978\n",
      "epoch: 57, batch: 123, loss: 1.547006607055664\n",
      "epoch: 57, batch: 124, loss: 1.518004059791565\n",
      "epoch: 57, batch: 125, loss: 1.6749061346054077\n",
      "epoch: 57, batch: 126, loss: 1.803844690322876\n",
      "epoch: 57, batch: 127, loss: 1.3652675151824951\n",
      "epoch: 58, batch: 0, loss: 1.4311317205429077\n",
      "epoch: 58, batch: 1, loss: 1.719839096069336\n",
      "epoch: 58, batch: 2, loss: 1.5545194149017334\n",
      "epoch: 58, batch: 3, loss: 1.7151689529418945\n",
      "epoch: 58, batch: 4, loss: 1.5049738883972168\n",
      "epoch: 58, batch: 5, loss: 1.6703369617462158\n",
      "epoch: 58, batch: 6, loss: 1.838675856590271\n",
      "epoch: 58, batch: 7, loss: 1.7588905096054077\n",
      "epoch: 58, batch: 8, loss: 1.7152483463287354\n",
      "epoch: 58, batch: 9, loss: 1.1841375827789307\n",
      "epoch: 58, batch: 10, loss: 1.4760726690292358\n",
      "epoch: 58, batch: 11, loss: 1.5547436475753784\n",
      "epoch: 58, batch: 12, loss: 1.4280774593353271\n",
      "epoch: 58, batch: 13, loss: 1.3814486265182495\n",
      "epoch: 58, batch: 14, loss: 1.4311977624893188\n",
      "epoch: 58, batch: 15, loss: 1.7170318365097046\n",
      "epoch: 58, batch: 16, loss: 1.5548608303070068\n",
      "epoch: 58, batch: 17, loss: 1.673681616783142\n",
      "epoch: 58, batch: 18, loss: 1.3492063283920288\n",
      "epoch: 58, batch: 19, loss: 1.7443557977676392\n",
      "epoch: 58, batch: 20, loss: 1.758694052696228\n",
      "epoch: 58, batch: 21, loss: 1.550060749053955\n",
      "epoch: 58, batch: 22, loss: 1.7876031398773193\n",
      "epoch: 58, batch: 23, loss: 1.5932600498199463\n",
      "epoch: 58, batch: 24, loss: 1.6705257892608643\n",
      "epoch: 58, batch: 25, loss: 1.6705354452133179\n",
      "epoch: 58, batch: 26, loss: 1.7875407934188843\n",
      "epoch: 58, batch: 27, loss: 1.5516536235809326\n",
      "epoch: 58, batch: 28, loss: 1.5592708587646484\n",
      "epoch: 58, batch: 29, loss: 1.2284519672393799\n",
      "epoch: 58, batch: 30, loss: 1.3938229084014893\n",
      "epoch: 58, batch: 31, loss: 1.476254940032959\n",
      "epoch: 58, batch: 32, loss: 1.5791499614715576\n",
      "epoch: 58, batch: 33, loss: 1.4691200256347656\n",
      "epoch: 58, batch: 34, loss: 1.7771179676055908\n",
      "epoch: 58, batch: 35, loss: 1.546631097793579\n",
      "epoch: 58, batch: 36, loss: 1.6256145238876343\n",
      "epoch: 58, batch: 37, loss: 1.3486034870147705\n",
      "epoch: 58, batch: 38, loss: 1.4349658489227295\n",
      "epoch: 58, batch: 39, loss: 1.3485395908355713\n",
      "epoch: 58, batch: 40, loss: 1.6293659210205078\n",
      "epoch: 58, batch: 41, loss: 1.5465781688690186\n",
      "epoch: 58, batch: 42, loss: 1.3071644306182861\n",
      "epoch: 58, batch: 43, loss: 1.5138202905654907\n",
      "epoch: 58, batch: 44, loss: 1.589064598083496\n",
      "epoch: 58, batch: 45, loss: 1.509164571762085\n",
      "epoch: 58, batch: 46, loss: 1.3811216354370117\n",
      "epoch: 58, batch: 47, loss: 1.5053350925445557\n",
      "epoch: 58, batch: 48, loss: 1.5175788402557373\n",
      "epoch: 58, batch: 49, loss: 1.791088342666626\n",
      "epoch: 58, batch: 50, loss: 1.4234033823013306\n",
      "epoch: 58, batch: 51, loss: 1.3030483722686768\n",
      "epoch: 58, batch: 52, loss: 1.4313390254974365\n",
      "epoch: 58, batch: 53, loss: 1.3480862379074097\n",
      "epoch: 58, batch: 54, loss: 1.7118771076202393\n",
      "epoch: 58, batch: 55, loss: 1.423189640045166\n",
      "epoch: 58, batch: 56, loss: 1.3810184001922607\n",
      "epoch: 58, batch: 57, loss: 1.3069480657577515\n",
      "epoch: 58, batch: 58, loss: 1.6787678003311157\n",
      "epoch: 58, batch: 59, loss: 1.7491105794906616\n",
      "epoch: 58, batch: 60, loss: 1.4681249856948853\n",
      "epoch: 58, batch: 61, loss: 1.3809629678726196\n",
      "epoch: 58, batch: 62, loss: 1.6214708089828491\n",
      "epoch: 58, batch: 63, loss: 1.637717843055725\n",
      "epoch: 58, batch: 64, loss: 1.546358346939087\n",
      "epoch: 58, batch: 65, loss: 1.4261038303375244\n",
      "epoch: 58, batch: 66, loss: 1.352010726928711\n",
      "epoch: 58, batch: 67, loss: 1.4226510524749756\n",
      "epoch: 58, batch: 68, loss: 1.4722223281860352\n",
      "epoch: 58, batch: 69, loss: 1.476646065711975\n",
      "epoch: 58, batch: 70, loss: 1.5879478454589844\n",
      "epoch: 58, batch: 71, loss: 1.3067445755004883\n",
      "epoch: 58, batch: 72, loss: 1.6828407049179077\n",
      "epoch: 58, batch: 73, loss: 1.5055625438690186\n",
      "epoch: 58, batch: 74, loss: 1.5796760320663452\n",
      "epoch: 58, batch: 75, loss: 1.3066871166229248\n",
      "epoch: 58, batch: 76, loss: 1.3473060131072998\n",
      "epoch: 58, batch: 77, loss: 1.4314892292022705\n",
      "epoch: 58, batch: 78, loss: 1.3113261461257935\n",
      "epoch: 58, batch: 79, loss: 1.426794409751892\n",
      "epoch: 58, batch: 80, loss: 1.5914630889892578\n",
      "epoch: 58, batch: 81, loss: 1.3471299409866333\n",
      "epoch: 58, batch: 82, loss: 1.4267337322235107\n",
      "epoch: 58, batch: 83, loss: 1.4720218181610107\n",
      "epoch: 58, batch: 84, loss: 1.5056606531143188\n",
      "epoch: 58, batch: 85, loss: 1.5509917736053467\n",
      "epoch: 58, batch: 86, loss: 1.4719820022583008\n",
      "epoch: 58, batch: 87, loss: 1.467036485671997\n",
      "epoch: 58, batch: 88, loss: 1.6827560663223267\n",
      "epoch: 58, batch: 89, loss: 1.4315797090530396\n",
      "epoch: 58, batch: 90, loss: 1.7115142345428467\n",
      "epoch: 58, batch: 91, loss: 1.6717876195907593\n",
      "epoch: 58, batch: 92, loss: 1.791283369064331\n",
      "epoch: 58, batch: 93, loss: 1.5868732929229736\n",
      "epoch: 58, batch: 94, loss: 1.3466683626174927\n",
      "epoch: 58, batch: 95, loss: 1.3856853246688843\n",
      "epoch: 58, batch: 96, loss: 1.5109527111053467\n",
      "epoch: 58, batch: 97, loss: 1.4316432476043701\n",
      "epoch: 58, batch: 98, loss: 1.6712535619735718\n",
      "epoch: 58, batch: 99, loss: 1.4665383100509644\n",
      "epoch: 58, batch: 100, loss: 1.3010259866714478\n",
      "epoch: 58, batch: 101, loss: 1.471782922744751\n",
      "epoch: 58, batch: 102, loss: 1.4258768558502197\n",
      "epoch: 58, batch: 103, loss: 1.6426159143447876\n",
      "epoch: 58, batch: 104, loss: 1.3116817474365234\n",
      "epoch: 58, batch: 105, loss: 1.5572166442871094\n",
      "epoch: 58, batch: 106, loss: 1.7168067693710327\n",
      "epoch: 58, batch: 107, loss: 1.7113240957260132\n",
      "epoch: 58, batch: 108, loss: 1.3062102794647217\n",
      "epoch: 58, batch: 109, loss: 1.6371570825576782\n",
      "epoch: 58, batch: 110, loss: 1.3516812324523926\n",
      "epoch: 58, batch: 111, loss: 1.6769993305206299\n",
      "epoch: 58, batch: 112, loss: 1.3859426975250244\n",
      "epoch: 58, batch: 113, loss: 1.4261049032211304\n",
      "epoch: 58, batch: 114, loss: 1.4716094732284546\n",
      "epoch: 58, batch: 115, loss: 1.6771522760391235\n",
      "epoch: 58, batch: 116, loss: 1.4715826511383057\n",
      "epoch: 58, batch: 117, loss: 1.716986894607544\n",
      "epoch: 58, batch: 118, loss: 1.7800228595733643\n",
      "epoch: 58, batch: 119, loss: 1.4715423583984375\n",
      "epoch: 58, batch: 120, loss: 1.0603249073028564\n",
      "epoch: 58, batch: 121, loss: 1.4656225442886353\n",
      "epoch: 58, batch: 122, loss: 1.6825538873672485\n",
      "epoch: 58, batch: 123, loss: 1.665557622909546\n",
      "epoch: 58, batch: 124, loss: 1.5517804622650146\n",
      "epoch: 58, batch: 125, loss: 1.386139154434204\n",
      "epoch: 58, batch: 126, loss: 1.832066297531128\n",
      "epoch: 58, batch: 127, loss: 1.7102506160736084\n",
      "epoch: 59, batch: 0, loss: 1.3515334129333496\n",
      "epoch: 59, batch: 1, loss: 1.7172863483428955\n",
      "epoch: 59, batch: 2, loss: 1.3059113025665283\n",
      "epoch: 59, batch: 3, loss: 1.2997310161590576\n",
      "epoch: 59, batch: 4, loss: 1.4713672399520874\n",
      "epoch: 59, batch: 5, loss: 1.179785966873169\n",
      "epoch: 59, batch: 6, loss: 1.4650918245315552\n",
      "epoch: 59, batch: 7, loss: 1.3058446645736694\n",
      "epoch: 59, batch: 8, loss: 1.6304928064346313\n",
      "epoch: 59, batch: 9, loss: 1.3058178424835205\n",
      "epoch: 59, batch: 10, loss: 1.432003378868103\n",
      "epoch: 59, batch: 11, loss: 1.47127366065979\n",
      "epoch: 59, batch: 12, loss: 1.584676742553711\n",
      "epoch: 59, batch: 13, loss: 1.3449796438217163\n",
      "epoch: 59, batch: 14, loss: 1.5454057455062866\n",
      "epoch: 59, batch: 15, loss: 1.580394983291626\n",
      "epoch: 59, batch: 16, loss: 1.3057230710983276\n",
      "epoch: 59, batch: 17, loss: 1.4646391868591309\n",
      "epoch: 59, batch: 18, loss: 1.224997878074646\n",
      "epoch: 59, batch: 19, loss: 1.6915128231048584\n",
      "epoch: 59, batch: 20, loss: 1.4711542129516602\n",
      "epoch: 59, batch: 21, loss: 1.4187695980072021\n",
      "epoch: 59, batch: 22, loss: 1.5842087268829346\n",
      "epoch: 59, batch: 23, loss: 1.1858532428741455\n",
      "epoch: 59, batch: 24, loss: 1.6755545139312744\n",
      "epoch: 59, batch: 25, loss: 1.7175357341766357\n",
      "epoch: 59, batch: 26, loss: 1.5976455211639404\n",
      "epoch: 59, batch: 27, loss: 1.5805137157440186\n",
      "epoch: 59, batch: 28, loss: 1.3124291896820068\n",
      "epoch: 59, batch: 29, loss: 1.5521100759506226\n",
      "epoch: 59, batch: 30, loss: 1.8269431591033936\n",
      "epoch: 59, batch: 31, loss: 1.545182704925537\n",
      "epoch: 59, batch: 32, loss: 1.7530437707901\n",
      "epoch: 59, batch: 33, loss: 1.4322247505187988\n",
      "epoch: 59, batch: 34, loss: 1.470969319343567\n",
      "epoch: 59, batch: 35, loss: 1.544877052307129\n",
      "epoch: 59, batch: 36, loss: 1.5906455516815186\n",
      "epoch: 59, batch: 37, loss: 1.3054332733154297\n",
      "epoch: 59, batch: 38, loss: 1.4709166288375854\n",
      "epoch: 59, batch: 39, loss: 1.8446441888809204\n",
      "epoch: 59, batch: 40, loss: 1.640588402748108\n",
      "epoch: 59, batch: 41, loss: 1.4395520687103271\n",
      "epoch: 59, batch: 42, loss: 1.5064857006072998\n",
      "epoch: 59, batch: 43, loss: 1.7174861431121826\n",
      "epoch: 59, batch: 44, loss: 1.7003380060195923\n",
      "epoch: 59, batch: 45, loss: 1.425346851348877\n",
      "epoch: 59, batch: 46, loss: 1.7003475427627563\n",
      "epoch: 59, batch: 47, loss: 1.3437360525131226\n",
      "epoch: 59, batch: 48, loss: 1.7991338968276978\n",
      "epoch: 59, batch: 49, loss: 1.8349460363388062\n",
      "epoch: 59, batch: 50, loss: 1.5828802585601807\n",
      "epoch: 59, batch: 51, loss: 1.6645466089248657\n",
      "epoch: 59, batch: 52, loss: 1.424837350845337\n",
      "epoch: 59, batch: 53, loss: 1.9911201000213623\n",
      "epoch: 59, batch: 54, loss: 1.6745128631591797\n",
      "epoch: 59, batch: 55, loss: 1.4989606142044067\n",
      "epoch: 59, batch: 56, loss: 1.4706817865371704\n",
      "epoch: 59, batch: 57, loss: 1.5525572299957275\n",
      "epoch: 59, batch: 58, loss: 1.626213788986206\n",
      "epoch: 59, batch: 59, loss: 1.3793094158172607\n",
      "epoch: 59, batch: 60, loss: 1.5526065826416016\n",
      "epoch: 59, batch: 61, loss: 1.3871005773544312\n",
      "epoch: 59, batch: 62, loss: 1.8377189636230469\n",
      "epoch: 59, batch: 63, loss: 1.2971903085708618\n",
      "epoch: 59, batch: 64, loss: 1.8456474542617798\n",
      "epoch: 59, batch: 65, loss: 1.5679126977920532\n",
      "epoch: 59, batch: 66, loss: 1.6342004537582397\n",
      "epoch: 59, batch: 67, loss: 1.8377784490585327\n",
      "epoch: 59, batch: 68, loss: 1.5820268392562866\n",
      "epoch: 59, batch: 69, loss: 1.7464603185653687\n",
      "epoch: 59, batch: 70, loss: 1.3130433559417725\n",
      "epoch: 59, batch: 71, loss: 1.3872512578964233\n",
      "epoch: 59, batch: 72, loss: 1.388892412185669\n",
      "epoch: 59, batch: 73, loss: 1.1772735118865967\n",
      "epoch: 59, batch: 74, loss: 1.3049100637435913\n",
      "epoch: 59, batch: 75, loss: 1.4704387187957764\n",
      "epoch: 59, batch: 76, loss: 1.3873258829116821\n",
      "epoch: 59, batch: 77, loss: 1.5355231761932373\n",
      "epoch: 59, batch: 78, loss: 1.5164884328842163\n",
      "epoch: 59, batch: 79, loss: 1.5893551111221313\n",
      "epoch: 59, batch: 80, loss: 1.7931175231933594\n",
      "epoch: 59, batch: 81, loss: 1.5080854892730713\n",
      "epoch: 59, batch: 82, loss: 1.627488374710083\n",
      "epoch: 59, batch: 83, loss: 1.3885871171951294\n",
      "epoch: 59, batch: 84, loss: 1.18534255027771\n",
      "epoch: 59, batch: 85, loss: 1.6347980499267578\n",
      "epoch: 59, batch: 86, loss: 1.5154165029525757\n",
      "epoch: 59, batch: 87, loss: 1.6819995641708374\n",
      "epoch: 59, batch: 88, loss: 1.5982623100280762\n",
      "epoch: 59, batch: 89, loss: 1.3875181674957275\n",
      "epoch: 59, batch: 90, loss: 1.3508567810058594\n",
      "epoch: 59, batch: 91, loss: 1.4240567684173584\n",
      "epoch: 59, batch: 92, loss: 1.6888965368270874\n",
      "epoch: 59, batch: 93, loss: 1.7457044124603271\n",
      "epoch: 59, batch: 94, loss: 1.5895675420761108\n",
      "epoch: 59, batch: 95, loss: 1.3420369625091553\n",
      "epoch: 59, batch: 96, loss: 1.5443813800811768\n",
      "epoch: 59, batch: 97, loss: 1.5905849933624268\n",
      "epoch: 59, batch: 98, loss: 1.5158660411834717\n",
      "epoch: 59, batch: 99, loss: 1.635705590248108\n",
      "epoch: 59, batch: 100, loss: 1.4701204299926758\n",
      "epoch: 59, batch: 101, loss: 1.5901914834976196\n",
      "epoch: 59, batch: 102, loss: 1.5983988046646118\n",
      "epoch: 59, batch: 103, loss: 1.4700822830200195\n",
      "epoch: 59, batch: 104, loss: 1.5533162355422974\n",
      "epoch: 59, batch: 105, loss: 1.635632872581482\n",
      "epoch: 59, batch: 106, loss: 1.414690613746643\n",
      "epoch: 59, batch: 107, loss: 1.6172797679901123\n",
      "epoch: 59, batch: 108, loss: 1.479156494140625\n",
      "epoch: 59, batch: 109, loss: 1.672686219215393\n",
      "epoch: 59, batch: 110, loss: 1.5442159175872803\n",
      "epoch: 59, batch: 111, loss: 1.672706961631775\n",
      "epoch: 59, batch: 112, loss: 1.4699666500091553\n",
      "epoch: 59, batch: 113, loss: 1.4699537754058838\n",
      "epoch: 59, batch: 114, loss: 1.4249014854431152\n",
      "epoch: 59, batch: 115, loss: 1.423609733581543\n",
      "epoch: 59, batch: 116, loss: 1.6724647283554077\n",
      "epoch: 59, batch: 117, loss: 1.5904629230499268\n",
      "epoch: 59, batch: 118, loss: 1.4604796171188354\n",
      "epoch: 59, batch: 119, loss: 1.3875362873077393\n",
      "epoch: 59, batch: 120, loss: 1.516682505607605\n",
      "epoch: 59, batch: 121, loss: 1.341126561164856\n",
      "epoch: 59, batch: 122, loss: 1.5072391033172607\n",
      "epoch: 59, batch: 123, loss: 1.847344160079956\n",
      "epoch: 59, batch: 124, loss: 1.460237741470337\n",
      "epoch: 59, batch: 125, loss: 1.5065641403198242\n",
      "epoch: 59, batch: 126, loss: 1.294577717781067\n",
      "epoch: 59, batch: 127, loss: 1.412108063697815\n",
      "epoch: 60, batch: 0, loss: 1.4330655336380005\n",
      "epoch: 60, batch: 1, loss: 1.4600340127944946\n",
      "epoch: 60, batch: 2, loss: 1.616774320602417\n",
      "epoch: 60, batch: 3, loss: 1.542513370513916\n",
      "epoch: 60, batch: 4, loss: 1.3505293130874634\n",
      "epoch: 60, batch: 5, loss: 1.4331220388412476\n",
      "epoch: 60, batch: 6, loss: 1.479535698890686\n",
      "epoch: 60, batch: 7, loss: 1.7570736408233643\n",
      "epoch: 60, batch: 8, loss: 1.5438932180404663\n",
      "epoch: 60, batch: 9, loss: 1.469641923904419\n",
      "epoch: 60, batch: 10, loss: 1.4247193336486816\n",
      "epoch: 60, batch: 11, loss: 1.5174272060394287\n",
      "epoch: 60, batch: 12, loss: 1.4796291589736938\n",
      "epoch: 60, batch: 13, loss: 1.4695895910263062\n",
      "epoch: 60, batch: 14, loss: 1.5538965463638306\n",
      "epoch: 60, batch: 15, loss: 1.5074756145477295\n",
      "epoch: 60, batch: 16, loss: 1.4246692657470703\n",
      "epoch: 60, batch: 17, loss: 1.2938133478164673\n",
      "epoch: 60, batch: 18, loss: 1.4797216653823853\n",
      "epoch: 60, batch: 19, loss: 1.7557480335235596\n",
      "epoch: 60, batch: 20, loss: 1.4694980382919312\n",
      "epoch: 60, batch: 21, loss: 1.7921876907348633\n",
      "epoch: 60, batch: 22, loss: 1.912886381149292\n",
      "epoch: 60, batch: 23, loss: 1.4694589376449585\n",
      "epoch: 60, batch: 24, loss: 1.443713903427124\n",
      "epoch: 60, batch: 25, loss: 1.578113317489624\n",
      "epoch: 60, batch: 26, loss: 1.433371901512146\n",
      "epoch: 60, batch: 27, loss: 1.6349458694458008\n",
      "epoch: 60, batch: 28, loss: 1.5901004076004028\n",
      "epoch: 60, batch: 29, loss: 1.6266977787017822\n",
      "epoch: 60, batch: 30, loss: 1.660992980003357\n",
      "epoch: 60, batch: 31, loss: 1.6708056926727295\n",
      "epoch: 60, batch: 32, loss: 1.3861912488937378\n",
      "epoch: 60, batch: 33, loss: 1.7645132541656494\n",
      "epoch: 60, batch: 34, loss: 1.683872938156128\n",
      "epoch: 60, batch: 35, loss: 1.422809362411499\n",
      "epoch: 60, batch: 36, loss: 1.388700246810913\n",
      "epoch: 60, batch: 37, loss: 1.5775645971298218\n",
      "epoch: 60, batch: 38, loss: 1.58827805519104\n",
      "epoch: 60, batch: 39, loss: 1.3037364482879639\n",
      "epoch: 60, batch: 40, loss: 1.4969611167907715\n",
      "epoch: 60, batch: 41, loss: 1.552564263343811\n",
      "epoch: 60, batch: 42, loss: 1.3145771026611328\n",
      "epoch: 60, batch: 43, loss: 1.803208351135254\n",
      "epoch: 60, batch: 44, loss: 1.424420714378357\n",
      "epoch: 60, batch: 45, loss: 1.5047231912612915\n",
      "epoch: 60, batch: 46, loss: 1.6268304586410522\n",
      "epoch: 60, batch: 47, loss: 1.1736656427383423\n",
      "epoch: 60, batch: 48, loss: 1.3391185998916626\n",
      "epoch: 60, batch: 49, loss: 1.5078907012939453\n",
      "epoch: 60, batch: 50, loss: 1.7121860980987549\n",
      "epoch: 60, batch: 51, loss: 1.30361008644104\n",
      "epoch: 60, batch: 52, loss: 1.4967721700668335\n",
      "epoch: 60, batch: 53, loss: 1.610374093055725\n",
      "epoch: 60, batch: 54, loss: 1.5821784734725952\n",
      "epoch: 60, batch: 55, loss: 1.2923250198364258\n",
      "epoch: 60, batch: 56, loss: 1.4224827289581299\n",
      "epoch: 60, batch: 57, loss: 1.3500882387161255\n",
      "epoch: 60, batch: 58, loss: 1.5935542583465576\n",
      "epoch: 60, batch: 59, loss: 1.3387155532836914\n",
      "epoch: 60, batch: 60, loss: 1.706782341003418\n",
      "epoch: 60, batch: 61, loss: 1.3500570058822632\n",
      "epoch: 60, batch: 62, loss: 1.7067352533340454\n",
      "epoch: 60, batch: 63, loss: 1.543153166770935\n",
      "epoch: 60, batch: 64, loss: 1.5763019323349\n",
      "epoch: 60, batch: 65, loss: 1.519616723060608\n",
      "epoch: 60, batch: 66, loss: 1.338458776473999\n",
      "epoch: 60, batch: 67, loss: 1.6693178415298462\n",
      "epoch: 60, batch: 68, loss: 1.7647953033447266\n",
      "epoch: 60, batch: 69, loss: 1.3499969244003296\n",
      "epoch: 60, batch: 70, loss: 1.611048936843872\n",
      "epoch: 60, batch: 71, loss: 1.5081568956375122\n",
      "epoch: 60, batch: 72, loss: 1.5430309772491455\n",
      "epoch: 60, batch: 73, loss: 1.6387825012207031\n",
      "epoch: 60, batch: 74, loss: 1.6690365076065063\n",
      "epoch: 60, batch: 75, loss: 1.6342108249664307\n",
      "epoch: 60, batch: 76, loss: 1.2192720174789429\n",
      "epoch: 60, batch: 77, loss: 1.6270431280136108\n",
      "epoch: 60, batch: 78, loss: 1.4241414070129395\n",
      "epoch: 60, batch: 79, loss: 1.7083618640899658\n",
      "epoch: 60, batch: 80, loss: 1.6856616735458374\n",
      "epoch: 60, batch: 81, loss: 1.4686987400054932\n",
      "epoch: 60, batch: 82, loss: 1.673710823059082\n",
      "epoch: 60, batch: 83, loss: 1.468672513961792\n",
      "epoch: 60, batch: 84, loss: 1.5874353647232056\n",
      "epoch: 60, batch: 85, loss: 1.552893877029419\n",
      "epoch: 60, batch: 86, loss: 1.2910782098770142\n",
      "epoch: 60, batch: 87, loss: 1.5650863647460938\n",
      "epoch: 60, batch: 88, loss: 1.3032041788101196\n",
      "epoch: 60, batch: 89, loss: 1.633994460105896\n",
      "epoch: 60, batch: 90, loss: 1.5427840948104858\n",
      "epoch: 60, batch: 91, loss: 1.5529426336288452\n",
      "epoch: 60, batch: 92, loss: 1.4240238666534424\n",
      "epoch: 60, batch: 93, loss: 1.1720738410949707\n",
      "epoch: 60, batch: 94, loss: 1.5996309518814087\n",
      "epoch: 60, batch: 95, loss: 1.75261652469635\n",
      "epoch: 60, batch: 96, loss: 1.5826709270477295\n",
      "epoch: 60, batch: 97, loss: 1.4239832162857056\n",
      "epoch: 60, batch: 98, loss: 1.5529977083206177\n",
      "epoch: 60, batch: 99, loss: 1.7709500789642334\n",
      "epoch: 60, batch: 100, loss: 1.5210764408111572\n",
      "epoch: 60, batch: 101, loss: 1.1843851804733276\n",
      "epoch: 60, batch: 102, loss: 1.5024986267089844\n",
      "epoch: 60, batch: 103, loss: 1.3497378826141357\n",
      "epoch: 60, batch: 104, loss: 1.468398094177246\n",
      "epoch: 60, batch: 105, loss: 1.5743488073349\n",
      "epoch: 60, batch: 106, loss: 1.9197609424591064\n",
      "epoch: 60, batch: 107, loss: 1.7184178829193115\n",
      "epoch: 60, batch: 108, loss: 1.6336965560913086\n",
      "epoch: 60, batch: 109, loss: 1.7948882579803467\n",
      "epoch: 60, batch: 110, loss: 1.8521811962127686\n",
      "epoch: 60, batch: 111, loss: 1.3835198879241943\n",
      "epoch: 60, batch: 112, loss: 1.3771506547927856\n",
      "epoch: 60, batch: 113, loss: 1.6206916570663452\n",
      "epoch: 60, batch: 114, loss: 1.4553124904632568\n",
      "epoch: 60, batch: 115, loss: 1.839376449584961\n",
      "epoch: 60, batch: 116, loss: 1.6273291110992432\n",
      "epoch: 60, batch: 117, loss: 1.4812694787979126\n",
      "epoch: 60, batch: 118, loss: 1.2584969997406006\n",
      "epoch: 60, batch: 119, loss: 1.5829617977142334\n",
      "epoch: 60, batch: 120, loss: 1.390196442604065\n",
      "epoch: 60, batch: 121, loss: 1.5423723459243774\n",
      "epoch: 60, batch: 122, loss: 1.4082410335540771\n",
      "epoch: 60, batch: 123, loss: 1.542345643043518\n",
      "epoch: 60, batch: 124, loss: 1.50163733959198\n",
      "epoch: 60, batch: 125, loss: 1.377018928527832\n",
      "epoch: 60, batch: 126, loss: 1.3495604991912842\n",
      "epoch: 60, batch: 127, loss: 1.2172280550003052\n",
      "epoch: 61, batch: 0, loss: 1.3361871242523193\n",
      "epoch: 61, batch: 1, loss: 1.2893983125686646\n",
      "epoch: 61, batch: 2, loss: 1.4814751148223877\n",
      "epoch: 61, batch: 3, loss: 1.6742050647735596\n",
      "epoch: 61, batch: 4, loss: 1.862731695175171\n",
      "epoch: 61, batch: 5, loss: 1.5012750625610352\n",
      "epoch: 61, batch: 6, loss: 1.7324501276016235\n",
      "epoch: 61, batch: 7, loss: 1.4347944259643555\n",
      "epoch: 61, batch: 8, loss: 1.5421638488769531\n",
      "epoch: 61, batch: 9, loss: 1.613707184791565\n",
      "epoch: 61, batch: 10, loss: 1.0520670413970947\n",
      "epoch: 61, batch: 11, loss: 1.2890064716339111\n",
      "epoch: 61, batch: 12, loss: 1.4348682165145874\n",
      "epoch: 61, batch: 13, loss: 1.2172279357910156\n",
      "epoch: 61, batch: 14, loss: 1.5420781373977661\n",
      "epoch: 61, batch: 15, loss: 1.5558632612228394\n",
      "epoch: 61, batch: 16, loss: 1.7789685726165771\n",
      "epoch: 61, batch: 17, loss: 1.5007895231246948\n",
      "epoch: 61, batch: 18, loss: 1.481741189956665\n",
      "epoch: 61, batch: 19, loss: 1.3354771137237549\n",
      "epoch: 61, batch: 20, loss: 1.6275908946990967\n",
      "epoch: 61, batch: 21, loss: 1.2886111736297607\n",
      "epoch: 61, batch: 22, loss: 1.422285795211792\n",
      "epoch: 61, batch: 23, loss: 1.77411687374115\n",
      "epoch: 61, batch: 24, loss: 1.2582952976226807\n",
      "epoch: 61, batch: 25, loss: 1.386816382408142\n",
      "epoch: 61, batch: 26, loss: 1.7017936706542969\n",
      "epoch: 61, batch: 27, loss: 1.467738389968872\n",
      "epoch: 61, batch: 28, loss: 1.420919418334961\n",
      "epoch: 61, batch: 29, loss: 1.6717808246612549\n",
      "epoch: 61, batch: 30, loss: 1.6602357625961304\n",
      "epoch: 61, batch: 31, loss: 1.7902443408966064\n",
      "epoch: 61, batch: 32, loss: 1.660196304321289\n",
      "epoch: 61, batch: 33, loss: 1.4065021276474\n",
      "epoch: 61, batch: 34, loss: 1.4351972341537476\n",
      "epoch: 61, batch: 35, loss: 1.3492729663848877\n",
      "epoch: 61, batch: 36, loss: 1.349265694618225\n",
      "epoch: 61, batch: 37, loss: 1.4676134586334229\n",
      "epoch: 61, batch: 38, loss: 1.5825961828231812\n",
      "epoch: 61, batch: 39, loss: 1.7068977355957031\n",
      "epoch: 61, batch: 40, loss: 1.5416964292526245\n",
      "epoch: 61, batch: 41, loss: 1.4675642251968384\n",
      "epoch: 61, batch: 42, loss: 1.4561915397644043\n",
      "epoch: 61, batch: 43, loss: 1.3764712810516357\n",
      "epoch: 61, batch: 44, loss: 1.4675275087356567\n",
      "epoch: 61, batch: 45, loss: 1.3344910144805908\n",
      "epoch: 61, batch: 46, loss: 1.2875839471817017\n",
      "epoch: 61, batch: 47, loss: 1.5857932567596436\n",
      "epoch: 61, batch: 48, loss: 1.4232803583145142\n",
      "epoch: 61, batch: 49, loss: 1.6725457906723022\n",
      "epoch: 61, batch: 50, loss: 1.8078548908233643\n",
      "epoch: 61, batch: 51, loss: 1.6795015335083008\n",
      "epoch: 61, batch: 52, loss: 1.6006355285644531\n",
      "epoch: 61, batch: 53, loss: 1.311042070388794\n",
      "epoch: 61, batch: 54, loss: 1.287246823310852\n",
      "epoch: 61, batch: 55, loss: 1.6897790431976318\n",
      "epoch: 61, batch: 56, loss: 1.3762929439544678\n",
      "epoch: 61, batch: 57, loss: 1.509615182876587\n",
      "epoch: 61, batch: 58, loss: 1.4204410314559937\n",
      "epoch: 61, batch: 59, loss: 1.7640148401260376\n",
      "epoch: 61, batch: 60, loss: 1.6324924230575562\n",
      "epoch: 61, batch: 61, loss: 1.556591272354126\n",
      "epoch: 61, batch: 62, loss: 1.5538432598114014\n",
      "epoch: 61, batch: 63, loss: 1.317409873008728\n",
      "epoch: 61, batch: 64, loss: 1.8083736896514893\n",
      "epoch: 61, batch: 65, loss: 1.5837633609771729\n",
      "epoch: 61, batch: 66, loss: 1.6008294820785522\n",
      "epoch: 61, batch: 67, loss: 1.5141980648040771\n",
      "epoch: 61, batch: 68, loss: 1.6323878765106201\n",
      "epoch: 61, batch: 69, loss: 1.3761144876480103\n",
      "epoch: 61, batch: 70, loss: 1.6644150018692017\n",
      "epoch: 61, batch: 71, loss: 1.4827123880386353\n",
      "epoch: 61, batch: 72, loss: 1.6904869079589844\n",
      "epoch: 61, batch: 73, loss: 1.2864567041397095\n",
      "epoch: 61, batch: 74, loss: 1.6280384063720703\n",
      "epoch: 61, batch: 75, loss: 1.7660939693450928\n",
      "epoch: 61, batch: 76, loss: 1.6280066967010498\n",
      "epoch: 61, batch: 77, loss: 1.5568420886993408\n",
      "epoch: 61, batch: 78, loss: 1.8286653757095337\n",
      "epoch: 61, batch: 79, loss: 1.7062629461288452\n",
      "epoch: 61, batch: 80, loss: 1.59969162940979\n",
      "epoch: 61, batch: 81, loss: 1.4358901977539062\n",
      "epoch: 61, batch: 82, loss: 1.5698777437210083\n",
      "epoch: 61, batch: 83, loss: 1.0497918128967285\n",
      "epoch: 61, batch: 84, loss: 1.5569498538970947\n",
      "epoch: 61, batch: 85, loss: 1.4981129169464111\n",
      "epoch: 61, batch: 86, loss: 1.5569803714752197\n",
      "epoch: 61, batch: 87, loss: 1.750246286392212\n",
      "epoch: 61, batch: 88, loss: 1.4228885173797607\n",
      "epoch: 61, batch: 89, loss: 1.557025671005249\n",
      "epoch: 61, batch: 90, loss: 1.5850696563720703\n",
      "epoch: 61, batch: 91, loss: 1.2577394247055054\n",
      "epoch: 61, batch: 92, loss: 1.5541397333145142\n",
      "epoch: 61, batch: 93, loss: 1.6590166091918945\n",
      "epoch: 61, batch: 94, loss: 1.600250482559204\n",
      "epoch: 61, batch: 95, loss: 1.6320199966430664\n",
      "epoch: 61, batch: 96, loss: 1.4198620319366455\n",
      "epoch: 61, batch: 97, loss: 1.4506114721298218\n",
      "epoch: 61, batch: 98, loss: 1.5264222621917725\n",
      "epoch: 61, batch: 99, loss: 1.7807376384735107\n",
      "epoch: 61, batch: 100, loss: 1.7185500860214233\n",
      "epoch: 61, batch: 101, loss: 1.3487781286239624\n",
      "epoch: 61, batch: 102, loss: 1.3181402683258057\n",
      "epoch: 61, batch: 103, loss: 1.57069993019104\n",
      "epoch: 61, batch: 104, loss: 1.7805118560791016\n",
      "epoch: 61, batch: 105, loss: 1.301694631576538\n",
      "epoch: 61, batch: 106, loss: 1.5848101377487183\n",
      "epoch: 61, batch: 107, loss: 1.167083978652954\n",
      "epoch: 61, batch: 108, loss: 1.214105248451233\n",
      "epoch: 61, batch: 109, loss: 1.7629287242889404\n",
      "epoch: 61, batch: 110, loss: 1.7057733535766602\n",
      "epoch: 61, batch: 111, loss: 1.6317903995513916\n",
      "epoch: 61, batch: 112, loss: 1.70574152469635\n",
      "epoch: 61, batch: 113, loss: 1.6484901905059814\n",
      "epoch: 61, batch: 114, loss: 1.658643364906311\n",
      "epoch: 61, batch: 115, loss: 1.6723835468292236\n",
      "epoch: 61, batch: 116, loss: 1.2138433456420898\n",
      "epoch: 61, batch: 117, loss: 1.5103694200515747\n",
      "epoch: 61, batch: 118, loss: 1.6014865636825562\n",
      "epoch: 61, batch: 119, loss: 1.7194833755493164\n",
      "epoch: 61, batch: 120, loss: 1.348621129989624\n",
      "epoch: 61, batch: 121, loss: 1.4195138216018677\n",
      "epoch: 61, batch: 122, loss: 1.567564845085144\n",
      "epoch: 61, batch: 123, loss: 1.3315834999084473\n",
      "epoch: 61, batch: 124, loss: 1.4495108127593994\n",
      "epoch: 61, batch: 125, loss: 1.6145107746124268\n",
      "epoch: 61, batch: 126, loss: 1.6755285263061523\n",
      "epoch: 61, batch: 127, loss: 1.6673319339752197\n",
      "epoch: 62, batch: 0, loss: 1.4665031433105469\n",
      "epoch: 62, batch: 1, loss: 1.4194042682647705\n",
      "epoch: 62, batch: 2, loss: 1.348541021347046\n",
      "epoch: 62, batch: 3, loss: 1.5404052734375\n",
      "epoch: 62, batch: 4, loss: 1.557663917541504\n",
      "epoch: 62, batch: 5, loss: 1.6141541004180908\n",
      "epoch: 62, batch: 6, loss: 1.3311784267425537\n",
      "epoch: 62, batch: 7, loss: 1.5577093362808228\n",
      "epoch: 62, batch: 8, loss: 1.7495653629302979\n",
      "epoch: 62, batch: 9, loss: 1.7712833881378174\n",
      "epoch: 62, batch: 10, loss: 1.4018348455429077\n",
      "epoch: 62, batch: 11, loss: 1.7371145486831665\n",
      "epoch: 62, batch: 12, loss: 1.7227725982666016\n",
      "epoch: 62, batch: 13, loss: 1.4488036632537842\n",
      "epoch: 62, batch: 14, loss: 1.5546804666519165\n",
      "epoch: 62, batch: 15, loss: 1.4663162231445312\n",
      "epoch: 62, batch: 16, loss: 1.4957754611968994\n",
      "epoch: 62, batch: 17, loss: 1.6639420986175537\n",
      "epoch: 62, batch: 18, loss: 1.5401899814605713\n",
      "epoch: 62, batch: 19, loss: 1.542749047279358\n",
      "epoch: 62, batch: 20, loss: 1.720313310623169\n",
      "epoch: 62, batch: 21, loss: 1.7668135166168213\n",
      "epoch: 62, batch: 22, loss: 1.5401318073272705\n",
      "epoch: 62, batch: 23, loss: 1.242714524269104\n",
      "epoch: 62, batch: 24, loss: 1.5548022985458374\n",
      "epoch: 62, batch: 25, loss: 1.2126365900039673\n",
      "epoch: 62, batch: 26, loss: 1.301253318786621\n",
      "epoch: 62, batch: 27, loss: 1.3774902820587158\n",
      "epoch: 62, batch: 28, loss: 1.5109281539916992\n",
      "epoch: 62, batch: 29, loss: 1.257308006286621\n",
      "epoch: 62, batch: 30, loss: 1.4661271572113037\n",
      "epoch: 62, batch: 31, loss: 1.9037039279937744\n",
      "epoch: 62, batch: 32, loss: 1.1653085947036743\n",
      "epoch: 62, batch: 33, loss: 1.6309789419174194\n",
      "epoch: 62, batch: 34, loss: 1.6780561208724976\n",
      "epoch: 62, batch: 35, loss: 1.6106319427490234\n",
      "epoch: 62, batch: 36, loss: 1.319395899772644\n",
      "epoch: 62, batch: 37, loss: 1.4842870235443115\n",
      "epoch: 62, batch: 38, loss: 1.2122035026550293\n",
      "epoch: 62, batch: 39, loss: 1.5549991130828857\n",
      "epoch: 62, batch: 40, loss: 1.466001272201538\n",
      "epoch: 62, batch: 41, loss: 1.6124725341796875\n",
      "epoch: 62, batch: 42, loss: 1.5398370027542114\n",
      "epoch: 62, batch: 43, loss: 1.2827022075653076\n",
      "epoch: 62, batch: 44, loss: 1.164916753768921\n",
      "epoch: 62, batch: 45, loss: 1.5836832523345947\n",
      "epoch: 62, batch: 46, loss: 1.4003334045410156\n",
      "epoch: 62, batch: 47, loss: 1.6289596557617188\n",
      "epoch: 62, batch: 48, loss: 1.7670148611068726\n",
      "epoch: 62, batch: 49, loss: 1.319679617881775\n",
      "epoch: 62, batch: 50, loss: 1.447246789932251\n",
      "epoch: 62, batch: 51, loss: 1.7215042114257812\n",
      "epoch: 62, batch: 52, loss: 1.7515560388565063\n",
      "epoch: 62, batch: 53, loss: 1.5364750623703003\n",
      "epoch: 62, batch: 54, loss: 1.6477863788604736\n",
      "epoch: 62, batch: 55, loss: 1.53964364528656\n",
      "epoch: 62, batch: 56, loss: 1.3292944431304932\n",
      "epoch: 62, batch: 57, loss: 1.8126760721206665\n",
      "epoch: 62, batch: 58, loss: 1.611670732498169\n",
      "epoch: 62, batch: 59, loss: 1.25714111328125\n",
      "epoch: 62, batch: 60, loss: 1.300998330116272\n",
      "epoch: 62, batch: 61, loss: 1.3480603694915771\n",
      "epoch: 62, batch: 62, loss: 1.6585499048233032\n",
      "epoch: 62, batch: 63, loss: 1.5833806991577148\n",
      "epoch: 62, batch: 64, loss: 1.5305335521697998\n",
      "epoch: 62, batch: 65, loss: 1.5305793285369873\n",
      "epoch: 62, batch: 66, loss: 1.6571334600448608\n",
      "epoch: 62, batch: 67, loss: 1.3288757801055908\n",
      "epoch: 62, batch: 68, loss: 1.465650200843811\n",
      "epoch: 62, batch: 69, loss: 1.2817347049713135\n",
      "epoch: 62, batch: 70, loss: 1.558645248413086\n",
      "epoch: 62, batch: 71, loss: 1.1640268564224243\n",
      "epoch: 62, batch: 72, loss: 1.5747400522232056\n",
      "epoch: 62, batch: 73, loss: 1.28158438205719\n",
      "epoch: 62, batch: 74, loss: 1.446228265762329\n",
      "epoch: 62, batch: 75, loss: 1.4185017347335815\n",
      "epoch: 62, batch: 76, loss: 1.49320387840271\n",
      "epoch: 62, batch: 77, loss: 1.4184776544570923\n",
      "epoch: 62, batch: 78, loss: 1.437934160232544\n",
      "epoch: 62, batch: 79, loss: 1.5312201976776123\n",
      "epoch: 62, batch: 80, loss: 1.39414381980896\n",
      "epoch: 62, batch: 81, loss: 1.3988685607910156\n",
      "epoch: 62, batch: 82, loss: 1.6097596883773804\n",
      "epoch: 62, batch: 83, loss: 1.7501792907714844\n",
      "epoch: 62, batch: 84, loss: 1.5855519771575928\n",
      "epoch: 62, batch: 85, loss: 1.7424978017807007\n",
      "epoch: 62, batch: 86, loss: 1.6961673498153687\n",
      "epoch: 62, batch: 87, loss: 1.6224628686904907\n",
      "epoch: 62, batch: 88, loss: 1.6764767169952393\n",
      "epoch: 62, batch: 89, loss: 1.582942247390747\n",
      "epoch: 62, batch: 90, loss: 1.6027603149414062\n",
      "epoch: 62, batch: 91, loss: 1.5391027927398682\n",
      "epoch: 62, batch: 92, loss: 1.7502533197402954\n",
      "epoch: 62, batch: 93, loss: 1.7275497913360596\n",
      "epoch: 62, batch: 94, loss: 1.3744611740112305\n",
      "epoch: 62, batch: 95, loss: 1.5557565689086914\n",
      "epoch: 62, batch: 96, loss: 1.8607664108276367\n",
      "epoch: 62, batch: 97, loss: 1.3944765329360962\n",
      "epoch: 62, batch: 98, loss: 1.4922804832458496\n",
      "epoch: 62, batch: 99, loss: 1.438302755355835\n",
      "epoch: 62, batch: 100, loss: 1.7973997592926025\n",
      "epoch: 62, batch: 101, loss: 1.5120465755462646\n",
      "epoch: 62, batch: 102, loss: 1.327547311782837\n",
      "epoch: 62, batch: 103, loss: 1.696872353553772\n",
      "epoch: 62, batch: 104, loss: 1.7503551244735718\n",
      "epoch: 62, batch: 105, loss: 1.650050163269043\n",
      "epoch: 62, batch: 106, loss: 1.6061375141143799\n",
      "epoch: 62, batch: 107, loss: 1.3477122783660889\n",
      "epoch: 62, batch: 108, loss: 1.5122544765472412\n",
      "epoch: 62, batch: 109, loss: 1.7737499475479126\n",
      "epoch: 62, batch: 110, loss: 1.3005975484848022\n",
      "epoch: 62, batch: 111, loss: 1.5764216184616089\n",
      "epoch: 62, batch: 112, loss: 1.555962324142456\n",
      "epoch: 62, batch: 113, loss: 1.444573163986206\n",
      "epoch: 62, batch: 114, loss: 1.5121901035308838\n",
      "epoch: 62, batch: 115, loss: 1.656113862991333\n",
      "epoch: 62, batch: 116, loss: 1.4650686979293823\n",
      "epoch: 62, batch: 117, loss: 1.3476396799087524\n",
      "epoch: 62, batch: 118, loss: 1.6238157749176025\n",
      "epoch: 62, batch: 119, loss: 1.4386388063430786\n",
      "epoch: 62, batch: 120, loss: 1.70319402217865\n",
      "epoch: 62, batch: 121, loss: 1.5123364925384521\n",
      "epoch: 62, batch: 122, loss: 1.3004976511001587\n",
      "epoch: 62, batch: 123, loss: 1.6297557353973389\n",
      "epoch: 62, batch: 124, loss: 1.6557165384292603\n",
      "epoch: 62, batch: 125, loss: 1.8888050317764282\n",
      "epoch: 62, batch: 126, loss: 1.417841911315918\n",
      "epoch: 62, batch: 127, loss: 1.3970866203308105\n",
      "epoch: 63, batch: 0, loss: 1.3004486560821533\n",
      "epoch: 63, batch: 1, loss: 1.6559276580810547\n",
      "epoch: 63, batch: 2, loss: 1.2794049978256226\n",
      "epoch: 63, batch: 3, loss: 1.3967230319976807\n",
      "epoch: 63, batch: 4, loss: 1.6033128499984741\n",
      "epoch: 63, batch: 5, loss: 1.46486496925354\n",
      "epoch: 63, batch: 6, loss: 1.5596396923065186\n",
      "epoch: 63, batch: 7, loss: 1.3003923892974854\n",
      "epoch: 63, batch: 8, loss: 1.6551849842071533\n",
      "epoch: 63, batch: 9, loss: 1.746585488319397\n",
      "epoch: 63, batch: 10, loss: 1.4906589984893799\n",
      "epoch: 63, batch: 11, loss: 1.2790699005126953\n",
      "epoch: 63, batch: 12, loss: 1.1830391883850098\n",
      "epoch: 63, batch: 13, loss: 1.347461223602295\n",
      "epoch: 63, batch: 14, loss: 1.3260751962661743\n",
      "epoch: 63, batch: 15, loss: 1.1830297708511353\n",
      "epoch: 63, batch: 16, loss: 1.4647334814071655\n",
      "epoch: 63, batch: 17, loss: 1.439069390296936\n",
      "epoch: 63, batch: 18, loss: 1.701857328414917\n",
      "epoch: 63, batch: 19, loss: 1.3218237161636353\n",
      "epoch: 63, batch: 20, loss: 1.4911811351776123\n",
      "epoch: 63, batch: 21, loss: 1.5779941082000732\n",
      "epoch: 63, batch: 22, loss: 1.5127676725387573\n",
      "epoch: 63, batch: 23, loss: 1.8415504693984985\n",
      "epoch: 63, batch: 24, loss: 1.7243006229400635\n",
      "epoch: 63, batch: 25, loss: 1.7208340167999268\n",
      "epoch: 63, batch: 26, loss: 1.4174902439117432\n",
      "epoch: 63, batch: 27, loss: 1.3219960927963257\n",
      "epoch: 63, batch: 28, loss: 1.5117132663726807\n",
      "epoch: 63, batch: 29, loss: 1.347342848777771\n",
      "epoch: 63, batch: 30, loss: 1.4392902851104736\n",
      "epoch: 63, batch: 31, loss: 1.5599017143249512\n",
      "epoch: 63, batch: 32, loss: 1.4209158420562744\n",
      "epoch: 63, batch: 33, loss: 1.7476165294647217\n",
      "epoch: 63, batch: 34, loss: 1.489678144454956\n",
      "epoch: 63, batch: 35, loss: 1.3472967147827148\n",
      "epoch: 63, batch: 36, loss: 1.6958186626434326\n",
      "epoch: 63, batch: 37, loss: 1.3001563549041748\n",
      "epoch: 63, batch: 38, loss: 1.6302064657211304\n",
      "epoch: 63, batch: 39, loss: 1.1829488277435303\n",
      "epoch: 63, batch: 40, loss: 1.8416649103164673\n",
      "epoch: 63, batch: 41, loss: 1.7237627506256104\n",
      "epoch: 63, batch: 42, loss: 1.4893553256988525\n",
      "epoch: 63, batch: 43, loss: 1.6287082433700562\n",
      "epoch: 63, batch: 44, loss: 1.3249797821044922\n",
      "epoch: 63, batch: 45, loss: 1.6038252115249634\n",
      "epoch: 63, batch: 46, loss: 1.5114961862564087\n",
      "epoch: 63, batch: 47, loss: 1.3959884643554688\n",
      "epoch: 63, batch: 48, loss: 1.347193717956543\n",
      "epoch: 63, batch: 49, loss: 1.3960275650024414\n",
      "epoch: 63, batch: 50, loss: 1.3000552654266357\n",
      "epoch: 63, batch: 51, loss: 1.4207470417022705\n",
      "epoch: 63, batch: 52, loss: 1.277571439743042\n",
      "epoch: 63, batch: 53, loss: 1.4396635293960571\n",
      "epoch: 63, batch: 54, loss: 2.0114309787750244\n",
      "epoch: 63, batch: 55, loss: 1.488830804824829\n",
      "epoch: 63, batch: 56, loss: 1.7001063823699951\n",
      "epoch: 63, batch: 57, loss: 1.8908374309539795\n",
      "epoch: 63, batch: 58, loss: 1.5342258214950562\n",
      "epoch: 63, batch: 59, loss: 1.4206745624542236\n",
      "epoch: 63, batch: 60, loss: 1.513347864151001\n",
      "epoch: 63, batch: 61, loss: 1.5341813564300537\n",
      "epoch: 63, batch: 62, loss: 1.7211246490478516\n",
      "epoch: 63, batch: 63, loss: 1.7268317937850952\n",
      "epoch: 63, batch: 64, loss: 1.3735172748565674\n",
      "epoch: 63, batch: 65, loss: 1.3735086917877197\n",
      "epoch: 63, batch: 66, loss: 1.58121657371521\n",
      "epoch: 63, batch: 67, loss: 1.513455867767334\n",
      "epoch: 63, batch: 68, loss: 1.4905622005462646\n",
      "epoch: 63, batch: 69, loss: 1.8235619068145752\n",
      "epoch: 63, batch: 70, loss: 1.5111902952194214\n",
      "epoch: 63, batch: 71, loss: 1.6271213293075562\n",
      "epoch: 63, batch: 72, loss: 1.3469947576522827\n",
      "epoch: 63, batch: 73, loss: 1.3469865322113037\n",
      "epoch: 63, batch: 74, loss: 1.6522306203842163\n",
      "epoch: 63, batch: 75, loss: 1.323868751525879\n",
      "epoch: 63, batch: 76, loss: 1.7273279428482056\n",
      "epoch: 63, batch: 77, loss: 1.206752061843872\n",
      "epoch: 63, batch: 78, loss: 1.8683675527572632\n",
      "epoch: 63, batch: 79, loss: 1.4168707132339478\n",
      "epoch: 63, batch: 80, loss: 1.5577601194381714\n",
      "epoch: 63, batch: 81, loss: 1.6042487621307373\n",
      "epoch: 63, batch: 82, loss: 1.5136826038360596\n",
      "epoch: 63, batch: 83, loss: 1.6307220458984375\n",
      "epoch: 63, batch: 84, loss: 1.6046911478042603\n",
      "epoch: 63, batch: 85, loss: 1.55754554271698\n",
      "epoch: 63, batch: 86, loss: 1.6778579950332642\n",
      "epoch: 63, batch: 87, loss: 1.4402107000350952\n",
      "epoch: 63, batch: 88, loss: 1.654237985610962\n",
      "epoch: 63, batch: 89, loss: 1.8183799982070923\n",
      "epoch: 63, batch: 90, loss: 1.604358434677124\n",
      "epoch: 63, batch: 91, loss: 1.841992974281311\n",
      "epoch: 63, batch: 92, loss: 1.654354453086853\n",
      "epoch: 63, batch: 93, loss: 1.4167091846466064\n",
      "epoch: 63, batch: 94, loss: 1.7684894800186157\n",
      "epoch: 63, batch: 95, loss: 1.580764651298523\n",
      "epoch: 63, batch: 96, loss: 1.7250707149505615\n",
      "epoch: 63, batch: 97, loss: 1.7712411880493164\n",
      "epoch: 63, batch: 98, loss: 1.7044273614883423\n",
      "epoch: 63, batch: 99, loss: 1.5874764919281006\n",
      "epoch: 63, batch: 100, loss: 1.5610562562942505\n",
      "epoch: 63, batch: 101, loss: 1.5377874374389648\n",
      "epoch: 63, batch: 102, loss: 1.392755150794983\n",
      "epoch: 63, batch: 103, loss: 1.6283833980560303\n",
      "epoch: 63, batch: 104, loss: 1.322840929031372\n",
      "epoch: 63, batch: 105, loss: 1.587564468383789\n",
      "epoch: 63, batch: 106, loss: 1.627692461013794\n",
      "epoch: 63, batch: 107, loss: 1.561164379119873\n",
      "epoch: 63, batch: 108, loss: 1.634714126586914\n",
      "epoch: 63, batch: 109, loss: 1.322667121887207\n",
      "epoch: 63, batch: 110, loss: 1.2995916604995728\n",
      "epoch: 63, batch: 111, loss: 1.7950407266616821\n",
      "epoch: 63, batch: 112, loss: 1.5106956958770752\n",
      "epoch: 63, batch: 113, loss: 1.32252836227417\n",
      "epoch: 63, batch: 114, loss: 1.5804579257965088\n",
      "epoch: 63, batch: 115, loss: 1.463552474975586\n",
      "epoch: 63, batch: 116, loss: 1.6275355815887451\n",
      "epoch: 63, batch: 117, loss: 1.439269781112671\n",
      "epoch: 63, batch: 118, loss: 1.463517427444458\n",
      "epoch: 63, batch: 119, loss: 1.2752087116241455\n",
      "epoch: 63, batch: 120, loss: 1.4201558828353882\n",
      "epoch: 63, batch: 121, loss: 1.653873085975647\n",
      "epoch: 63, batch: 122, loss: 1.3238990306854248\n",
      "epoch: 63, batch: 123, loss: 1.346605658531189\n",
      "epoch: 63, batch: 124, loss: 1.5878407955169678\n",
      "epoch: 63, batch: 125, loss: 1.7762937545776367\n",
      "epoch: 63, batch: 126, loss: 1.4389145374298096\n",
      "epoch: 63, batch: 127, loss: 1.2179720401763916\n",
      "epoch: 64, batch: 0, loss: 1.3465698957443237\n",
      "epoch: 64, batch: 1, loss: 1.4200842380523682\n",
      "epoch: 64, batch: 2, loss: 1.1579983234405518\n",
      "epoch: 64, batch: 3, loss: 1.3218986988067627\n",
      "epoch: 64, batch: 4, loss: 1.3218634128570557\n",
      "epoch: 64, batch: 5, loss: 1.4200503826141357\n",
      "epoch: 64, batch: 6, loss: 1.6519845724105835\n",
      "epoch: 64, batch: 7, loss: 1.557770013809204\n",
      "epoch: 64, batch: 8, loss: 1.27460777759552\n",
      "epoch: 64, batch: 9, loss: 1.5368088483810425\n",
      "epoch: 64, batch: 10, loss: 1.5081158876419067\n",
      "epoch: 64, batch: 11, loss: 1.6271730661392212\n",
      "epoch: 64, batch: 12, loss: 1.5103759765625\n",
      "epoch: 64, batch: 13, loss: 1.5395054817199707\n",
      "epoch: 64, batch: 14, loss: 1.8423635959625244\n",
      "epoch: 64, batch: 15, loss: 1.8423707485198975\n",
      "epoch: 64, batch: 16, loss: 1.5617334842681885\n",
      "epoch: 64, batch: 17, loss: 1.79095458984375\n",
      "epoch: 64, batch: 18, loss: 1.5548789501190186\n",
      "epoch: 64, batch: 19, loss: 1.6521497964859009\n",
      "epoch: 64, batch: 20, loss: 1.5146827697753906\n",
      "epoch: 64, batch: 21, loss: 1.70050847530365\n",
      "epoch: 64, batch: 22, loss: 1.74374258518219\n",
      "epoch: 64, batch: 23, loss: 1.4160268306732178\n",
      "epoch: 64, batch: 24, loss: 1.3463902473449707\n",
      "epoch: 64, batch: 25, loss: 1.2740036249160767\n",
      "epoch: 64, batch: 26, loss: 1.5365849733352661\n",
      "epoch: 64, batch: 27, loss: 1.768968939781189\n",
      "epoch: 64, batch: 28, loss: 1.653276801109314\n",
      "epoch: 64, batch: 29, loss: 1.7182143926620483\n",
      "epoch: 64, batch: 30, loss: 1.5543521642684937\n",
      "epoch: 64, batch: 31, loss: 1.3208949565887451\n",
      "epoch: 64, batch: 32, loss: 1.5581047534942627\n",
      "epoch: 64, batch: 33, loss: 1.4893916845321655\n",
      "epoch: 64, batch: 34, loss: 1.3207862377166748\n",
      "epoch: 64, batch: 35, loss: 1.6060556173324585\n",
      "epoch: 64, batch: 36, loss: 1.3463008403778076\n",
      "epoch: 64, batch: 37, loss: 1.5364363193511963\n",
      "epoch: 64, batch: 38, loss: 1.6523786783218384\n",
      "epoch: 64, batch: 39, loss: 1.5149908065795898\n",
      "epoch: 64, batch: 40, loss: 1.6053107976913452\n",
      "epoch: 64, batch: 41, loss: 1.6787773370742798\n",
      "epoch: 64, batch: 42, loss: 1.484242558479309\n",
      "epoch: 64, batch: 43, loss: 1.3984050750732422\n",
      "epoch: 64, batch: 44, loss: 1.2037742137908936\n",
      "epoch: 64, batch: 45, loss: 1.0400111675262451\n",
      "epoch: 64, batch: 46, loss: 1.7310701608657837\n",
      "epoch: 64, batch: 47, loss: 1.7479512691497803\n",
      "epoch: 64, batch: 48, loss: 1.4628467559814453\n",
      "epoch: 64, batch: 49, loss: 1.4628350734710693\n",
      "epoch: 64, batch: 50, loss: 1.68379807472229\n",
      "epoch: 64, batch: 51, loss: 1.6229257583618164\n",
      "epoch: 64, batch: 52, loss: 1.7313060760498047\n",
      "epoch: 64, batch: 53, loss: 1.752335786819458\n",
      "epoch: 64, batch: 54, loss: 1.4366568326950073\n",
      "epoch: 64, batch: 55, loss: 1.6525928974151611\n",
      "epoch: 64, batch: 56, loss: 1.7523624897003174\n",
      "epoch: 64, batch: 57, loss: 1.7523715496063232\n",
      "epoch: 64, batch: 58, loss: 1.4627318382263184\n",
      "epoch: 64, batch: 59, loss: 1.7484118938446045\n",
      "epoch: 64, batch: 60, loss: 1.4890412092208862\n",
      "epoch: 64, batch: 61, loss: 1.8218955993652344\n",
      "epoch: 64, batch: 62, loss: 1.4890148639678955\n",
      "epoch: 64, batch: 63, loss: 1.4362876415252686\n",
      "epoch: 64, batch: 64, loss: 1.489079475402832\n",
      "epoch: 64, batch: 65, loss: 1.7582603693008423\n",
      "epoch: 64, batch: 66, loss: 1.4626401662826538\n",
      "epoch: 64, batch: 67, loss: 1.868999719619751\n",
      "epoch: 64, batch: 68, loss: 1.2990105152130127\n",
      "epoch: 64, batch: 69, loss: 1.3460769653320312\n",
      "epoch: 64, batch: 70, loss: 1.2029532194137573\n",
      "epoch: 64, batch: 71, loss: 1.4155113697052002\n",
      "epoch: 64, batch: 72, loss: 1.5626081228256226\n",
      "epoch: 64, batch: 73, loss: 1.6261413097381592\n",
      "epoch: 64, batch: 74, loss: 1.4154784679412842\n",
      "epoch: 64, batch: 75, loss: 1.3192980289459229\n",
      "epoch: 64, batch: 76, loss: 1.4625266790390015\n",
      "epoch: 64, batch: 77, loss: 1.488818883895874\n",
      "epoch: 64, batch: 78, loss: 1.5095738172531128\n",
      "epoch: 64, batch: 79, loss: 1.6792007684707642\n",
      "epoch: 64, batch: 80, loss: 1.4422991275787354\n",
      "epoch: 64, batch: 81, loss: 1.5789411067962646\n",
      "epoch: 64, batch: 82, loss: 1.5588018894195557\n",
      "epoch: 64, batch: 83, loss: 1.4624475240707397\n",
      "epoch: 64, batch: 84, loss: 1.6058967113494873\n",
      "epoch: 64, batch: 85, loss: 1.6321998834609985\n",
      "epoch: 64, batch: 86, loss: 1.4353514909744263\n",
      "epoch: 64, batch: 87, loss: 1.5157803297042847\n",
      "epoch: 64, batch: 88, loss: 1.442444086074829\n",
      "epoch: 64, batch: 89, loss: 1.255798578262329\n",
      "epoch: 64, batch: 90, loss: 1.6594349145889282\n",
      "epoch: 64, batch: 91, loss: 1.3722145557403564\n",
      "epoch: 64, batch: 92, loss: 1.558931827545166\n",
      "epoch: 64, batch: 93, loss: 1.6164872646331787\n",
      "epoch: 64, batch: 94, loss: 1.4192596673965454\n",
      "epoch: 64, batch: 95, loss: 1.1824378967285156\n",
      "epoch: 64, batch: 96, loss: 1.3656219244003296\n",
      "epoch: 64, batch: 97, loss: 1.2714449167251587\n",
      "epoch: 64, batch: 98, loss: 1.3655531406402588\n",
      "epoch: 64, batch: 99, loss: 1.4348289966583252\n",
      "epoch: 64, batch: 100, loss: 1.63236403465271\n",
      "epoch: 64, batch: 101, loss: 1.15492844581604\n",
      "epoch: 64, batch: 102, loss: 1.5630918741226196\n",
      "epoch: 64, batch: 103, loss: 1.27122962474823\n",
      "epoch: 64, batch: 104, loss: 1.5436314344406128\n",
      "epoch: 64, batch: 105, loss: 1.5436756610870361\n",
      "epoch: 64, batch: 106, loss: 1.653260588645935\n",
      "epoch: 64, batch: 107, loss: 1.5631731748580933\n",
      "epoch: 64, batch: 108, loss: 1.5785092115402222\n",
      "epoch: 64, batch: 109, loss: 1.625575304031372\n",
      "epoch: 64, batch: 110, loss: 1.1824016571044922\n",
      "epoch: 64, batch: 111, loss: 1.5632377862930298\n",
      "epoch: 64, batch: 112, loss: 1.201666235923767\n",
      "epoch: 64, batch: 113, loss: 1.2987110614776611\n",
      "epoch: 64, batch: 114, loss: 1.6062877178192139\n",
      "epoch: 64, batch: 115, loss: 1.6604238748550415\n",
      "epoch: 64, batch: 116, loss: 1.8238556385040283\n",
      "epoch: 64, batch: 117, loss: 1.5162451267242432\n",
      "epoch: 64, batch: 118, loss: 1.2986732721328735\n",
      "epoch: 64, batch: 119, loss: 1.726747751235962\n",
      "epoch: 64, batch: 120, loss: 1.7339226007461548\n",
      "epoch: 64, batch: 121, loss: 1.433959722518921\n",
      "epoch: 64, batch: 122, loss: 1.4620157480239868\n",
      "epoch: 64, batch: 123, loss: 1.8711971044540405\n",
      "epoch: 64, batch: 124, loss: 1.7811660766601562\n",
      "epoch: 64, batch: 125, loss: 1.5971695184707642\n",
      "epoch: 64, batch: 126, loss: 1.6515356302261353\n",
      "epoch: 64, batch: 127, loss: 1.6032788753509521\n",
      "epoch: 65, batch: 0, loss: 1.7960214614868164\n",
      "epoch: 65, batch: 1, loss: 1.443129301071167\n",
      "epoch: 65, batch: 2, loss: 1.1823307275772095\n",
      "epoch: 65, batch: 3, loss: 1.6797983646392822\n",
      "epoch: 65, batch: 4, loss: 1.2985658645629883\n",
      "epoch: 65, batch: 5, loss: 1.6536270380020142\n",
      "epoch: 65, batch: 6, loss: 1.6043134927749634\n",
      "epoch: 65, batch: 7, loss: 1.5165144205093384\n",
      "epoch: 65, batch: 8, loss: 1.635040044784546\n",
      "epoch: 65, batch: 9, loss: 1.4880427122116089\n",
      "epoch: 65, batch: 10, loss: 1.559489369392395\n",
      "epoch: 65, batch: 11, loss: 1.7084455490112305\n",
      "epoch: 65, batch: 12, loss: 1.5780317783355713\n",
      "epoch: 65, batch: 13, loss: 1.4188883304595947\n",
      "epoch: 65, batch: 14, loss: 1.6906747817993164\n",
      "epoch: 65, batch: 15, loss: 1.2984815835952759\n",
      "epoch: 65, batch: 16, loss: 1.8223339319229126\n",
      "epoch: 65, batch: 17, loss: 1.545374870300293\n",
      "epoch: 65, batch: 18, loss: 1.1535295248031616\n",
      "epoch: 65, batch: 19, loss: 1.487927794456482\n",
      "epoch: 65, batch: 20, loss: 1.672123908996582\n",
      "epoch: 65, batch: 21, loss: 1.5638270378112793\n",
      "epoch: 65, batch: 22, loss: 1.7490880489349365\n",
      "epoch: 65, batch: 23, loss: 1.5088047981262207\n",
      "epoch: 65, batch: 24, loss: 1.4145805835723877\n",
      "epoch: 65, batch: 25, loss: 1.5087839365005493\n",
      "epoch: 65, batch: 26, loss: 1.6329567432403564\n",
      "epoch: 65, batch: 27, loss: 1.3454973697662354\n",
      "epoch: 65, batch: 28, loss: 1.2003374099731445\n",
      "epoch: 65, batch: 29, loss: 1.1822240352630615\n",
      "epoch: 65, batch: 30, loss: 1.5639642477035522\n",
      "epoch: 65, batch: 31, loss: 1.3163914680480957\n",
      "epoch: 65, batch: 32, loss: 1.7092365026474\n",
      "epoch: 65, batch: 33, loss: 1.5597431659698486\n",
      "epoch: 65, batch: 34, loss: 1.3274900913238525\n",
      "epoch: 65, batch: 35, loss: 1.5169286727905273\n",
      "epoch: 65, batch: 36, loss: 1.9167087078094482\n",
      "epoch: 65, batch: 37, loss: 1.6068989038467407\n",
      "epoch: 65, batch: 38, loss: 1.5086462497711182\n",
      "epoch: 65, batch: 39, loss: 1.5462697744369507\n",
      "epoch: 65, batch: 40, loss: 1.5483176708221436\n",
      "epoch: 65, batch: 41, loss: 1.4876846075057983\n",
      "epoch: 65, batch: 42, loss: 1.3453853130340576\n",
      "epoch: 65, batch: 43, loss: 1.770180106163025\n",
      "epoch: 65, batch: 44, loss: 1.345369815826416\n",
      "epoch: 65, batch: 45, loss: 1.4143460988998413\n",
      "epoch: 65, batch: 46, loss: 1.75359308719635\n",
      "epoch: 65, batch: 47, loss: 1.5951553583145142\n",
      "epoch: 65, batch: 48, loss: 1.5642321109771729\n",
      "epoch: 65, batch: 49, loss: 1.6246099472045898\n",
      "epoch: 65, batch: 50, loss: 1.1821305751800537\n",
      "epoch: 65, batch: 51, loss: 1.47892165184021\n",
      "epoch: 65, batch: 52, loss: 1.327803373336792\n",
      "epoch: 65, batch: 53, loss: 1.4613707065582275\n",
      "epoch: 65, batch: 54, loss: 1.4142491817474365\n",
      "epoch: 65, batch: 55, loss: 1.5774143934249878\n",
      "epoch: 65, batch: 56, loss: 1.5774004459381104\n",
      "epoch: 65, batch: 57, loss: 1.487522840499878\n",
      "epoch: 65, batch: 58, loss: 1.6244815587997437\n",
      "epoch: 65, batch: 59, loss: 1.1523088216781616\n",
      "epoch: 65, batch: 60, loss: 1.4439913034439087\n",
      "epoch: 65, batch: 61, loss: 1.5644214153289795\n",
      "epoch: 65, batch: 62, loss: 1.4185367822647095\n",
      "epoch: 65, batch: 63, loss: 1.3624473810195923\n",
      "epoch: 65, batch: 64, loss: 1.2682006359100342\n",
      "epoch: 65, batch: 65, loss: 1.7276216745376587\n",
      "epoch: 65, batch: 66, loss: 1.663374662399292\n",
      "epoch: 65, batch: 67, loss: 1.783829927444458\n",
      "epoch: 65, batch: 68, loss: 1.5472297668457031\n",
      "epoch: 65, batch: 69, loss: 1.525376558303833\n",
      "epoch: 65, batch: 70, loss: 1.4140806198120117\n",
      "epoch: 65, batch: 71, loss: 1.3281300067901611\n",
      "epoch: 65, batch: 72, loss: 1.5601775646209717\n",
      "epoch: 65, batch: 73, loss: 1.5645958185195923\n",
      "epoch: 65, batch: 74, loss: 1.478090524673462\n",
      "epoch: 65, batch: 75, loss: 1.7277320623397827\n",
      "epoch: 65, batch: 76, loss: 1.6073238849639893\n",
      "epoch: 65, batch: 77, loss: 1.843754768371582\n",
      "epoch: 65, batch: 78, loss: 1.4610971212387085\n",
      "epoch: 65, batch: 79, loss: 1.6806776523590088\n",
      "epoch: 65, batch: 80, loss: 1.5770679712295532\n",
      "epoch: 65, batch: 81, loss: 1.7401397228240967\n",
      "epoch: 65, batch: 82, loss: 1.2676231861114502\n",
      "epoch: 65, batch: 83, loss: 1.6032577753067017\n",
      "epoch: 65, batch: 84, loss: 1.5647534132003784\n",
      "epoch: 65, batch: 85, loss: 1.6503251791000366\n",
      "epoch: 65, batch: 86, loss: 1.3450329303741455\n",
      "epoch: 65, batch: 87, loss: 1.5769709348678589\n",
      "epoch: 65, batch: 88, loss: 1.692927360534668\n",
      "epoch: 65, batch: 89, loss: 1.6807925701141357\n",
      "epoch: 65, batch: 90, loss: 1.5177545547485352\n",
      "epoch: 65, batch: 91, loss: 1.7705299854278564\n",
      "epoch: 65, batch: 92, loss: 1.59111750125885\n",
      "epoch: 65, batch: 93, loss: 1.1984031200408936\n",
      "epoch: 65, batch: 94, loss: 1.3712213039398193\n",
      "epoch: 65, batch: 95, loss: 1.5079898834228516\n",
      "epoch: 65, batch: 96, loss: 1.534233808517456\n",
      "epoch: 65, batch: 97, loss: 1.8538024425506592\n",
      "epoch: 65, batch: 98, loss: 1.477210521697998\n",
      "epoch: 65, batch: 99, loss: 1.413789987564087\n",
      "epoch: 65, batch: 100, loss: 1.5812675952911377\n",
      "epoch: 65, batch: 101, loss: 1.6338564157485962\n",
      "epoch: 65, batch: 102, loss: 1.5341715812683105\n",
      "epoch: 65, batch: 103, loss: 1.654685616493225\n",
      "epoch: 65, batch: 104, loss: 1.6708719730377197\n",
      "epoch: 65, batch: 105, loss: 1.3711541891098022\n",
      "epoch: 65, batch: 106, loss: 1.6455475091934204\n",
      "epoch: 65, batch: 107, loss: 1.5489917993545532\n",
      "epoch: 65, batch: 108, loss: 1.4137017726898193\n",
      "epoch: 65, batch: 109, loss: 1.534097671508789\n",
      "epoch: 65, batch: 110, loss: 1.3288280963897705\n",
      "epoch: 65, batch: 111, loss: 1.4918060302734375\n",
      "epoch: 65, batch: 112, loss: 1.765852928161621\n",
      "epoch: 65, batch: 113, loss: 1.6547847986221313\n",
      "epoch: 65, batch: 114, loss: 1.712226152420044\n",
      "epoch: 65, batch: 115, loss: 1.2552095651626587\n",
      "epoch: 65, batch: 116, loss: 1.2665458917617798\n",
      "epoch: 65, batch: 117, loss: 1.41361403465271\n",
      "epoch: 65, batch: 118, loss: 1.6653188467025757\n",
      "epoch: 65, batch: 119, loss: 1.42937433719635\n",
      "epoch: 65, batch: 120, loss: 1.5182287693023682\n",
      "epoch: 65, batch: 121, loss: 1.4606255292892456\n",
      "epoch: 65, batch: 122, loss: 1.4869118928909302\n",
      "epoch: 65, batch: 123, loss: 1.4180940389633179\n",
      "epoch: 65, batch: 124, loss: 1.4135451316833496\n",
      "epoch: 65, batch: 125, loss: 1.591655969619751\n",
      "epoch: 65, batch: 126, loss: 1.6497676372528076\n",
      "epoch: 65, batch: 127, loss: 1.0345085859298706\n",
      "epoch: 66, batch: 0, loss: 1.7815351486206055\n",
      "epoch: 66, batch: 1, loss: 1.4978065490722656\n",
      "epoch: 66, batch: 2, loss: 1.445034384727478\n",
      "epoch: 66, batch: 3, loss: 1.7075815200805664\n",
      "epoch: 66, batch: 4, loss: 1.181809425354004\n",
      "epoch: 66, batch: 5, loss: 1.6079471111297607\n",
      "epoch: 66, batch: 6, loss: 1.6967016458511353\n",
      "epoch: 66, batch: 7, loss: 1.9492130279541016\n",
      "epoch: 66, batch: 8, loss: 1.5808614492416382\n",
      "epoch: 66, batch: 9, loss: 1.3446452617645264\n",
      "epoch: 66, batch: 10, loss: 1.518506646156311\n",
      "epoch: 66, batch: 11, loss: 1.6025416851043701\n",
      "epoch: 66, batch: 12, loss: 1.6080281734466553\n",
      "epoch: 66, batch: 13, loss: 1.5808111429214478\n",
      "epoch: 66, batch: 14, loss: 1.5761924982070923\n",
      "epoch: 66, batch: 15, loss: 1.4179565906524658\n",
      "epoch: 66, batch: 16, loss: 1.6965644359588623\n",
      "epoch: 66, batch: 17, loss: 1.3126531839370728\n",
      "epoch: 66, batch: 18, loss: 1.5441734790802002\n",
      "epoch: 66, batch: 19, loss: 1.5610629320144653\n",
      "epoch: 66, batch: 20, loss: 1.2975212335586548\n",
      "epoch: 66, batch: 21, loss: 1.6081348657608032\n",
      "epoch: 66, batch: 22, loss: 1.6551955938339233\n",
      "epoch: 66, batch: 23, loss: 1.3445490598678589\n",
      "epoch: 66, batch: 24, loss: 1.0338644981384277\n",
      "epoch: 66, batch: 25, loss: 1.4751904010772705\n",
      "epoch: 66, batch: 26, loss: 1.7388200759887695\n",
      "epoch: 66, batch: 27, loss: 1.561156988143921\n",
      "epoch: 66, batch: 28, loss: 1.1817249059677124\n",
      "epoch: 66, batch: 29, loss: 1.3122581243515015\n",
      "epoch: 66, batch: 30, loss: 1.681593656539917\n",
      "epoch: 66, batch: 31, loss: 1.6345551013946533\n",
      "epoch: 66, batch: 32, loss: 1.4865132570266724\n",
      "epoch: 66, batch: 33, loss: 1.6816291809082031\n",
      "epoch: 66, batch: 34, loss: 1.149324655532837\n",
      "epoch: 66, batch: 35, loss: 1.6553537845611572\n",
      "epoch: 66, batch: 36, loss: 1.7649433612823486\n",
      "epoch: 66, batch: 37, loss: 1.6229194402694702\n",
      "epoch: 66, batch: 38, loss: 1.696255087852478\n",
      "epoch: 66, batch: 39, loss: 1.2973883152008057\n",
      "epoch: 66, batch: 40, loss: 1.3444331884384155\n",
      "epoch: 66, batch: 41, loss: 1.5287599563598633\n",
      "epoch: 66, batch: 42, loss: 1.3444199562072754\n",
      "epoch: 66, batch: 43, loss: 1.6817480325698853\n",
      "epoch: 66, batch: 44, loss: 1.7877506017684937\n",
      "epoch: 66, batch: 45, loss: 1.1490001678466797\n",
      "epoch: 66, batch: 46, loss: 1.8141210079193115\n",
      "epoch: 66, batch: 47, loss: 1.4273349046707153\n",
      "epoch: 66, batch: 48, loss: 1.5614078044891357\n",
      "epoch: 66, batch: 49, loss: 1.4129825830459595\n",
      "epoch: 66, batch: 50, loss: 1.2973127365112305\n",
      "epoch: 66, batch: 51, loss: 1.6227245330810547\n",
      "epoch: 66, batch: 52, loss: 1.507059097290039\n",
      "epoch: 66, batch: 53, loss: 1.3706367015838623\n",
      "epoch: 66, batch: 54, loss: 1.478743076324463\n",
      "epoch: 66, batch: 55, loss: 1.4599738121032715\n",
      "epoch: 66, batch: 56, loss: 1.6348474025726318\n",
      "epoch: 66, batch: 57, loss: 1.4599531888961792\n",
      "epoch: 66, batch: 58, loss: 1.1956722736358643\n",
      "epoch: 66, batch: 59, loss: 1.6696723699569702\n",
      "epoch: 66, batch: 60, loss: 1.3705849647521973\n",
      "epoch: 66, batch: 61, loss: 1.5663455724716187\n",
      "epoch: 66, batch: 62, loss: 1.3111695051193237\n",
      "epoch: 66, batch: 63, loss: 1.3705631494522095\n",
      "epoch: 66, batch: 64, loss: 1.6225471496582031\n",
      "epoch: 66, batch: 65, loss: 1.3581303358078003\n",
      "epoch: 66, batch: 66, loss: 1.4128004312515259\n",
      "epoch: 66, batch: 67, loss: 1.8446922302246094\n",
      "epoch: 66, batch: 68, loss: 1.3304624557495117\n",
      "epoch: 66, batch: 69, loss: 1.8620399236679077\n",
      "epoch: 66, batch: 70, loss: 1.3442332744598389\n",
      "epoch: 66, batch: 71, loss: 1.6558067798614502\n",
      "epoch: 66, batch: 72, loss: 1.3108386993408203\n",
      "epoch: 66, batch: 73, loss: 1.3305588960647583\n",
      "epoch: 66, batch: 74, loss: 1.3792848587036133\n",
      "epoch: 66, batch: 75, loss: 1.3305972814559937\n",
      "epoch: 66, batch: 76, loss: 1.506817102432251\n",
      "epoch: 66, batch: 77, loss: 1.5952727794647217\n",
      "epoch: 66, batch: 78, loss: 1.4126778841018677\n",
      "epoch: 66, batch: 79, loss: 1.4261640310287476\n",
      "epoch: 66, batch: 80, loss: 1.6015465259552002\n",
      "epoch: 66, batch: 81, loss: 1.7977561950683594\n",
      "epoch: 66, batch: 82, loss: 1.6822172403335571\n",
      "epoch: 66, batch: 83, loss: 1.575230360031128\n",
      "epoch: 66, batch: 84, loss: 1.8026320934295654\n",
      "epoch: 66, batch: 85, loss: 1.575203537940979\n",
      "epoch: 66, batch: 86, loss: 1.7422840595245361\n",
      "epoch: 66, batch: 87, loss: 1.4125874042510986\n",
      "epoch: 66, batch: 88, loss: 1.6955530643463135\n",
      "epoch: 66, batch: 89, loss: 1.2970407009124756\n",
      "epoch: 66, batch: 90, loss: 1.5800024271011353\n",
      "epoch: 66, batch: 91, loss: 1.3703595399856567\n",
      "epoch: 66, batch: 92, loss: 1.789567232131958\n",
      "epoch: 66, batch: 93, loss: 1.7978637218475342\n",
      "epoch: 66, batch: 94, loss: 1.5750823020935059\n",
      "epoch: 66, batch: 95, loss: 1.6905758380889893\n",
      "epoch: 66, batch: 96, loss: 1.566882848739624\n",
      "epoch: 66, batch: 97, loss: 1.7426999807357788\n",
      "epoch: 66, batch: 98, loss: 1.802797555923462\n",
      "epoch: 66, batch: 99, loss: 1.789831519126892\n",
      "epoch: 66, batch: 100, loss: 1.4936144351959229\n",
      "epoch: 66, batch: 101, loss: 1.3569395542144775\n",
      "epoch: 66, batch: 102, loss: 1.4807460308074951\n",
      "epoch: 66, batch: 103, loss: 1.0318024158477783\n",
      "epoch: 66, batch: 104, loss: 1.7716865539550781\n",
      "epoch: 66, batch: 105, loss: 1.5541996955871582\n",
      "epoch: 66, batch: 106, loss: 1.4466538429260254\n",
      "epoch: 66, batch: 107, loss: 1.4466694593429565\n",
      "epoch: 66, batch: 108, loss: 1.459429144859314\n",
      "epoch: 66, batch: 109, loss: 1.4123659133911133\n",
      "epoch: 66, batch: 110, loss: 1.4123562574386597\n",
      "epoch: 66, batch: 111, loss: 1.9527889490127563\n",
      "epoch: 66, batch: 112, loss: 1.2968839406967163\n",
      "epoch: 66, batch: 113, loss: 1.1940439939498901\n",
      "epoch: 66, batch: 114, loss: 1.3439210653305054\n",
      "epoch: 66, batch: 115, loss: 1.5934511423110962\n",
      "epoch: 66, batch: 116, loss: 1.4248384237289429\n",
      "epoch: 66, batch: 117, loss: 1.5934826135635376\n",
      "epoch: 66, batch: 118, loss: 1.4247686862945557\n",
      "epoch: 66, batch: 119, loss: 1.6777162551879883\n",
      "epoch: 66, batch: 120, loss: 1.2622205018997192\n",
      "epoch: 66, batch: 121, loss: 1.6093559265136719\n",
      "epoch: 66, batch: 122, loss: 1.7718400955200195\n",
      "epoch: 66, batch: 123, loss: 1.6564273834228516\n",
      "epoch: 66, batch: 124, loss: 1.6480176448822021\n",
      "epoch: 66, batch: 125, loss: 1.6356887817382812\n",
      "epoch: 66, batch: 126, loss: 1.2967854738235474\n",
      "epoch: 66, batch: 127, loss: 1.437321424484253\n",
      "epoch: 67, batch: 0, loss: 1.7786712646484375\n",
      "epoch: 67, batch: 1, loss: 1.4592154026031494\n",
      "epoch: 67, batch: 2, loss: 1.4243505001068115\n",
      "epoch: 67, batch: 3, loss: 1.6216402053833008\n",
      "epoch: 67, batch: 4, loss: 1.5973402261734009\n",
      "epoch: 67, batch: 5, loss: 1.370073676109314\n",
      "epoch: 67, batch: 6, loss: 1.609506607055664\n",
      "epoch: 67, batch: 7, loss: 1.1813353300094604\n",
      "epoch: 67, batch: 8, loss: 1.4854376316070557\n",
      "epoch: 67, batch: 9, loss: 1.4241080284118652\n",
      "epoch: 67, batch: 10, loss: 1.759979248046875\n",
      "epoch: 67, batch: 11, loss: 1.4170823097229004\n",
      "epoch: 67, batch: 12, loss: 1.5744749307632446\n",
      "epoch: 67, batch: 13, loss: 1.5324379205703735\n",
      "epoch: 67, batch: 14, loss: 1.4120362997055054\n",
      "epoch: 67, batch: 15, loss: 1.5744354724884033\n",
      "epoch: 67, batch: 16, loss: 1.2546446323394775\n",
      "epoch: 67, batch: 17, loss: 1.308476209640503\n",
      "epoch: 67, batch: 18, loss: 1.6096408367156982\n",
      "epoch: 67, batch: 19, loss: 1.8034007549285889\n",
      "epoch: 67, batch: 20, loss: 1.6830145120620728\n",
      "epoch: 67, batch: 21, loss: 1.2966200113296509\n",
      "epoch: 67, batch: 22, loss: 1.355367660522461\n",
      "epoch: 67, batch: 23, loss: 1.5677127838134766\n",
      "epoch: 67, batch: 24, loss: 1.1458722352981567\n",
      "epoch: 67, batch: 25, loss: 1.3699465990066528\n",
      "epoch: 67, batch: 26, loss: 1.4119127988815308\n",
      "epoch: 67, batch: 27, loss: 1.5858633518218994\n",
      "epoch: 67, batch: 28, loss: 1.5858261585235596\n",
      "epoch: 67, batch: 29, loss: 1.3436113595962524\n",
      "epoch: 67, batch: 30, loss: 1.5174837112426758\n",
      "epoch: 67, batch: 31, loss: 1.7920706272125244\n",
      "epoch: 67, batch: 32, loss: 1.707284688949585\n",
      "epoch: 67, batch: 33, loss: 1.6412250995635986\n",
      "epoch: 67, batch: 34, loss: 1.6831852197647095\n",
      "epoch: 67, batch: 35, loss: 1.5678942203521729\n",
      "epoch: 67, batch: 36, loss: 1.5855259895324707\n",
      "epoch: 67, batch: 37, loss: 1.5059001445770264\n",
      "epoch: 67, batch: 38, loss: 1.3698610067367554\n",
      "epoch: 67, batch: 39, loss: 1.713886022567749\n",
      "epoch: 67, batch: 40, loss: 1.6300684213638306\n",
      "epoch: 67, batch: 41, loss: 1.4168881177902222\n",
      "epoch: 67, batch: 42, loss: 1.4117549657821655\n",
      "epoch: 67, batch: 43, loss: 1.192370057106018\n",
      "epoch: 67, batch: 44, loss: 1.7191879749298096\n",
      "epoch: 67, batch: 45, loss: 1.4850929975509644\n",
      "epoch: 67, batch: 46, loss: 1.3075567483901978\n",
      "epoch: 67, batch: 47, loss: 1.594395399093628\n",
      "epoch: 67, batch: 48, loss: 1.5269694328308105\n",
      "epoch: 67, batch: 49, loss: 1.9329694509506226\n",
      "epoch: 67, batch: 50, loss: 1.2603886127471924\n",
      "epoch: 67, batch: 51, loss: 1.8597691059112549\n",
      "epoch: 67, batch: 52, loss: 1.254508376121521\n",
      "epoch: 67, batch: 53, loss: 1.4116525650024414\n",
      "epoch: 67, batch: 54, loss: 1.2963868379592896\n",
      "epoch: 67, batch: 55, loss: 1.4225237369537354\n",
      "epoch: 67, batch: 56, loss: 1.191986322402954\n",
      "epoch: 67, batch: 57, loss: 1.8349119424819946\n",
      "epoch: 67, batch: 58, loss: 1.6571452617645264\n",
      "epoch: 67, batch: 59, loss: 1.2963504791259766\n",
      "epoch: 67, batch: 60, loss: 1.521224856376648\n",
      "epoch: 67, batch: 61, loss: 1.6472246646881104\n",
      "epoch: 67, batch: 62, loss: 1.6364927291870117\n",
      "epoch: 67, batch: 63, loss: 1.494931936264038\n",
      "epoch: 67, batch: 64, loss: 1.5319565534591675\n",
      "epoch: 67, batch: 65, loss: 1.6365301609039307\n",
      "epoch: 67, batch: 66, loss: 1.8197036981582642\n",
      "epoch: 67, batch: 67, loss: 1.4115171432495117\n",
      "epoch: 67, batch: 68, loss: 1.3068602085113525\n",
      "epoch: 67, batch: 69, loss: 1.7099565267562866\n",
      "epoch: 67, batch: 70, loss: 1.5684077739715576\n",
      "epoch: 67, batch: 71, loss: 1.7830240726470947\n",
      "epoch: 67, batch: 72, loss: 1.7254892587661743\n",
      "epoch: 67, batch: 73, loss: 1.4219169616699219\n",
      "epoch: 67, batch: 74, loss: 1.5214288234710693\n",
      "epoch: 67, batch: 75, loss: 1.6470649242401123\n",
      "epoch: 67, batch: 76, loss: 1.6207107305526733\n",
      "epoch: 67, batch: 77, loss: 1.1443352699279785\n",
      "epoch: 67, batch: 78, loss: 1.4847886562347412\n",
      "epoch: 67, batch: 79, loss: 1.620676040649414\n",
      "epoch: 67, batch: 80, loss: 1.667705774307251\n",
      "epoch: 67, batch: 81, loss: 1.515730619430542\n",
      "epoch: 67, batch: 82, loss: 1.2961739301681519\n",
      "epoch: 67, batch: 83, loss: 1.5215586423873901\n",
      "epoch: 67, batch: 84, loss: 1.4797719717025757\n",
      "epoch: 67, batch: 85, loss: 1.7726571559906006\n",
      "epoch: 67, batch: 86, loss: 1.673675537109375\n",
      "epoch: 67, batch: 87, loss: 1.3431806564331055\n",
      "epoch: 67, batch: 88, loss: 1.4846913814544678\n",
      "epoch: 67, batch: 89, loss: 1.4583401679992676\n",
      "epoch: 67, batch: 90, loss: 1.6939337253570557\n",
      "epoch: 67, batch: 91, loss: 1.6105031967163086\n",
      "epoch: 67, batch: 92, loss: 1.4853410720825195\n",
      "epoch: 67, batch: 93, loss: 1.3694794178009033\n",
      "epoch: 67, batch: 94, loss: 1.45829176902771\n",
      "epoch: 67, batch: 95, loss: 1.4112361669540405\n",
      "epoch: 67, batch: 96, loss: 1.5053188800811768\n",
      "epoch: 67, batch: 97, loss: 1.4846055507659912\n",
      "epoch: 67, batch: 98, loss: 1.4483894109725952\n",
      "epoch: 67, batch: 99, loss: 1.3430941104888916\n",
      "epoch: 67, batch: 100, loss: 1.373956322669983\n",
      "epoch: 67, batch: 101, loss: 1.4582242965698242\n",
      "epoch: 67, batch: 102, loss: 1.4582147598266602\n",
      "epoch: 67, batch: 103, loss: 1.63699209690094\n",
      "epoch: 67, batch: 104, loss: 1.4484739303588867\n",
      "epoch: 67, batch: 105, loss: 1.1808669567108154\n",
      "epoch: 67, batch: 106, loss: 1.7354907989501953\n",
      "epoch: 67, batch: 107, loss: 1.819923758506775\n",
      "epoch: 67, batch: 108, loss: 1.6509910821914673\n",
      "epoch: 67, batch: 109, loss: 1.420701503753662\n",
      "epoch: 67, batch: 110, loss: 1.25849449634552\n",
      "epoch: 67, batch: 111, loss: 1.4206345081329346\n",
      "epoch: 67, batch: 112, loss: 1.8295085430145264\n",
      "epoch: 67, batch: 113, loss: 1.45810866355896\n",
      "epoch: 67, batch: 114, loss: 1.6388553380966187\n",
      "epoch: 67, batch: 115, loss: 1.3053898811340332\n",
      "epoch: 67, batch: 116, loss: 1.535573959350586\n",
      "epoch: 67, batch: 117, loss: 1.5691099166870117\n",
      "epoch: 67, batch: 118, loss: 1.9668413400650024\n",
      "epoch: 67, batch: 119, loss: 1.2958965301513672\n",
      "epoch: 67, batch: 120, loss: 1.5050957202911377\n",
      "epoch: 67, batch: 121, loss: 1.8277695178985596\n",
      "epoch: 67, batch: 122, loss: 1.8371250629425049\n",
      "epoch: 67, batch: 123, loss: 1.4843642711639404\n",
      "epoch: 67, batch: 124, loss: 1.3051115274429321\n",
      "epoch: 67, batch: 125, loss: 1.837225317955017\n",
      "epoch: 67, batch: 126, loss: 1.4579823017120361\n",
      "epoch: 67, batch: 127, loss: 1.3743219375610352\n",
      "epoch: 68, batch: 0, loss: 1.4579627513885498\n",
      "epoch: 68, batch: 1, loss: 1.3337481021881104\n",
      "epoch: 68, batch: 2, loss: 1.180748701095581\n",
      "epoch: 68, batch: 3, loss: 1.5693213939666748\n",
      "epoch: 68, batch: 4, loss: 1.1807398796081543\n",
      "epoch: 68, batch: 5, loss: 1.1427180767059326\n",
      "epoch: 68, batch: 6, loss: 1.504960298538208\n",
      "epoch: 68, batch: 7, loss: 1.295782208442688\n",
      "epoch: 68, batch: 8, loss: 1.7135683298110962\n",
      "epoch: 68, batch: 9, loss: 1.4108186960220337\n",
      "epoch: 68, batch: 10, loss: 1.7143335342407227\n",
      "epoch: 68, batch: 11, loss: 1.4667539596557617\n",
      "epoch: 68, batch: 12, loss: 1.4578449726104736\n",
      "epoch: 68, batch: 13, loss: 1.4578354358673096\n",
      "epoch: 68, batch: 14, loss: 1.4578256607055664\n",
      "epoch: 68, batch: 15, loss: 1.8259522914886475\n",
      "epoch: 68, batch: 16, loss: 1.8466379642486572\n",
      "epoch: 68, batch: 17, loss: 1.522473931312561\n",
      "epoch: 68, batch: 18, loss: 1.6375173330307007\n",
      "epoch: 68, batch: 19, loss: 1.7109720706939697\n",
      "epoch: 68, batch: 20, loss: 1.6285393238067627\n",
      "epoch: 68, batch: 21, loss: 1.3514056205749512\n",
      "epoch: 68, batch: 22, loss: 1.4106905460357666\n",
      "epoch: 68, batch: 23, loss: 1.254097580909729\n",
      "epoch: 68, batch: 24, loss: 1.846718192100525\n",
      "epoch: 68, batch: 25, loss: 1.4192384481430054\n",
      "epoch: 68, batch: 26, loss: 1.3041975498199463\n",
      "epoch: 68, batch: 27, loss: 1.3690903186798096\n",
      "epoch: 68, batch: 28, loss: 1.846758246421814\n",
      "epoch: 68, batch: 29, loss: 1.7581675052642822\n",
      "epoch: 68, batch: 30, loss: 1.4962663650512695\n",
      "epoch: 68, batch: 31, loss: 1.602836012840271\n",
      "epoch: 68, batch: 32, loss: 1.3426584005355835\n",
      "epoch: 68, batch: 33, loss: 1.410581111907959\n",
      "epoch: 68, batch: 34, loss: 1.6764039993286133\n",
      "epoch: 68, batch: 35, loss: 1.4492762088775635\n",
      "epoch: 68, batch: 36, loss: 1.596221923828125\n",
      "epoch: 68, batch: 37, loss: 1.8302513360977173\n",
      "epoch: 68, batch: 38, loss: 1.6113535165786743\n",
      "epoch: 68, batch: 39, loss: 1.418773889541626\n",
      "epoch: 68, batch: 40, loss: 1.6584367752075195\n",
      "epoch: 68, batch: 41, loss: 1.3037405014038086\n",
      "epoch: 68, batch: 42, loss: 1.5725185871124268\n",
      "epoch: 68, batch: 43, loss: 1.2566174268722534\n",
      "epoch: 68, batch: 44, loss: 1.68489670753479\n",
      "epoch: 68, batch: 45, loss: 1.3036201000213623\n",
      "epoch: 68, batch: 46, loss: 1.2954967021942139\n",
      "epoch: 68, batch: 47, loss: 1.4185116291046143\n",
      "epoch: 68, batch: 48, loss: 1.572444200515747\n",
      "epoch: 68, batch: 49, loss: 1.522947072982788\n",
      "epoch: 68, batch: 50, loss: 1.8079179525375366\n",
      "epoch: 68, batch: 51, loss: 1.4574668407440186\n",
      "epoch: 68, batch: 52, loss: 1.6115039587020874\n",
      "epoch: 68, batch: 53, loss: 1.4103788137435913\n",
      "epoch: 68, batch: 54, loss: 1.3504197597503662\n",
      "epoch: 68, batch: 55, loss: 1.4081062078475952\n",
      "epoch: 68, batch: 56, loss: 1.465287208557129\n",
      "epoch: 68, batch: 57, loss: 1.577981948852539\n",
      "epoch: 68, batch: 58, loss: 1.4966480731964111\n",
      "epoch: 68, batch: 59, loss: 1.4181177616119385\n",
      "epoch: 68, batch: 60, loss: 1.4838134050369263\n",
      "epoch: 68, batch: 61, loss: 1.2560666799545288\n",
      "epoch: 68, batch: 62, loss: 1.685120940208435\n",
      "epoch: 68, batch: 63, loss: 1.5044267177581787\n",
      "epoch: 68, batch: 64, loss: 1.4179531335830688\n",
      "epoch: 68, batch: 65, loss: 1.4496674537658691\n",
      "epoch: 68, batch: 66, loss: 1.6192963123321533\n",
      "epoch: 68, batch: 67, loss: 1.871678352355957\n",
      "epoch: 68, batch: 68, loss: 1.6192724704742432\n",
      "epoch: 68, batch: 69, loss: 1.4572935104370117\n",
      "epoch: 68, batch: 70, loss: 1.530806303024292\n",
      "epoch: 68, batch: 71, loss: 1.4177210330963135\n",
      "epoch: 68, batch: 72, loss: 1.3028086423873901\n",
      "epoch: 68, batch: 73, loss: 1.5703821182250977\n",
      "epoch: 68, batch: 74, loss: 1.2952913045883179\n",
      "epoch: 68, batch: 75, loss: 1.6514027118682861\n",
      "epoch: 68, batch: 76, loss: 1.3026869297027588\n",
      "epoch: 68, batch: 77, loss: 1.2952704429626465\n",
      "epoch: 68, batch: 78, loss: 1.5778217315673828\n",
      "epoch: 68, batch: 79, loss: 1.598509430885315\n",
      "epoch: 68, batch: 80, loss: 1.4571882486343384\n",
      "epoch: 68, batch: 81, loss: 1.4100964069366455\n",
      "epoch: 68, batch: 82, loss: 1.7251856327056885\n",
      "epoch: 68, batch: 83, loss: 1.4836158752441406\n",
      "epoch: 68, batch: 84, loss: 1.8473243713378906\n",
      "epoch: 68, batch: 85, loss: 1.6854093074798584\n",
      "epoch: 68, batch: 86, loss: 1.8002617359161377\n",
      "epoch: 68, batch: 87, loss: 1.791761040687561\n",
      "epoch: 68, batch: 88, loss: 1.6383640766143799\n",
      "epoch: 68, batch: 89, loss: 1.5706281661987305\n",
      "epoch: 68, batch: 90, loss: 1.7325553894042969\n",
      "epoch: 68, batch: 91, loss: 1.6119365692138672\n",
      "epoch: 68, batch: 92, loss: 1.295167088508606\n",
      "epoch: 68, batch: 93, loss: 1.4087860584259033\n",
      "epoch: 68, batch: 94, loss: 1.7267875671386719\n",
      "epoch: 68, batch: 95, loss: 1.4971669912338257\n",
      "epoch: 68, batch: 96, loss: 1.7120182514190674\n",
      "epoch: 68, batch: 97, loss: 1.564919352531433\n",
      "epoch: 68, batch: 98, loss: 1.8543552160263062\n",
      "epoch: 68, batch: 99, loss: 1.5305662155151367\n",
      "epoch: 68, batch: 100, loss: 1.3019580841064453\n",
      "epoch: 68, batch: 101, loss: 1.578608512878418\n",
      "epoch: 68, batch: 102, loss: 1.1400182247161865\n",
      "epoch: 68, batch: 103, loss: 1.1870763301849365\n",
      "epoch: 68, batch: 104, loss: 1.5305266380310059\n",
      "epoch: 68, batch: 105, loss: 1.5708731412887573\n",
      "epoch: 68, batch: 106, loss: 1.6524789333343506\n",
      "epoch: 68, batch: 107, loss: 1.6254808902740479\n",
      "epoch: 68, batch: 108, loss: 1.3354816436767578\n",
      "epoch: 68, batch: 109, loss: 1.5974239110946655\n",
      "epoch: 68, batch: 110, loss: 1.3693455457687378\n",
      "epoch: 68, batch: 111, loss: 1.5709645748138428\n",
      "epoch: 68, batch: 112, loss: 1.368610143661499\n",
      "epoch: 68, batch: 113, loss: 1.8734623193740845\n",
      "epoch: 68, batch: 114, loss: 1.6393028497695923\n",
      "epoch: 68, batch: 115, loss: 1.4097696542739868\n",
      "epoch: 68, batch: 116, loss: 1.8882482051849365\n",
      "epoch: 68, batch: 117, loss: 1.5239710807800293\n",
      "epoch: 68, batch: 118, loss: 1.4097415208816528\n",
      "epoch: 68, batch: 119, loss: 1.4097321033477783\n",
      "epoch: 68, batch: 120, loss: 1.5711010694503784\n",
      "epoch: 68, batch: 121, loss: 1.6186249256134033\n",
      "epoch: 68, batch: 122, loss: 1.1802226305007935\n",
      "epoch: 68, batch: 123, loss: 1.4975522756576538\n",
      "epoch: 68, batch: 124, loss: 1.5038540363311768\n",
      "epoch: 68, batch: 125, loss: 1.6060479879379272\n",
      "epoch: 68, batch: 126, loss: 1.6859222650527954\n",
      "epoch: 68, batch: 127, loss: 1.7670388221740723\n",
      "epoch: 69, batch: 0, loss: 1.1863933801651\n",
      "epoch: 69, batch: 1, loss: 1.6388747692108154\n",
      "epoch: 69, batch: 2, loss: 1.3010581731796265\n",
      "epoch: 69, batch: 3, loss: 1.4567029476165771\n",
      "epoch: 69, batch: 4, loss: 1.5775116682052612\n",
      "epoch: 69, batch: 5, loss: 1.7802766561508179\n",
      "epoch: 69, batch: 6, loss: 1.571313500404358\n",
      "epoch: 69, batch: 7, loss: 1.6184558868408203\n",
      "epoch: 69, batch: 8, loss: 1.2948675155639648\n",
      "epoch: 69, batch: 9, loss: 1.5302531719207764\n",
      "epoch: 69, batch: 10, loss: 1.1390389204025269\n",
      "epoch: 69, batch: 11, loss: 1.6184062957763672\n",
      "epoch: 69, batch: 12, loss: 1.6860969066619873\n",
      "epoch: 69, batch: 13, loss: 1.5243360996246338\n",
      "epoch: 69, batch: 14, loss: 1.524204969406128\n",
      "epoch: 69, batch: 15, loss: 1.4096813201904297\n",
      "epoch: 69, batch: 16, loss: 1.3360685110092163\n",
      "epoch: 69, batch: 17, loss: 1.3418893814086914\n",
      "epoch: 69, batch: 18, loss: 1.368436574935913\n",
      "epoch: 69, batch: 19, loss: 1.341875672340393\n",
      "epoch: 69, batch: 20, loss: 1.680151343345642\n",
      "epoch: 69, batch: 21, loss: 1.4508193731307983\n",
      "epoch: 69, batch: 22, loss: 1.6805317401885986\n",
      "epoch: 69, batch: 23, loss: 1.5711778402328491\n",
      "epoch: 69, batch: 24, loss: 1.8423423767089844\n",
      "epoch: 69, batch: 25, loss: 1.842374563217163\n",
      "epoch: 69, batch: 26, loss: 1.3003480434417725\n",
      "epoch: 69, batch: 27, loss: 1.6918599605560303\n",
      "epoch: 69, batch: 28, loss: 1.612640142440796\n",
      "epoch: 69, batch: 29, loss: 1.3418056964874268\n",
      "epoch: 69, batch: 30, loss: 1.6181700229644775\n",
      "epoch: 69, batch: 31, loss: 1.4099671840667725\n",
      "epoch: 69, batch: 32, loss: 1.6808795928955078\n",
      "epoch: 69, batch: 33, loss: 1.5710564851760864\n",
      "epoch: 69, batch: 34, loss: 1.461829423904419\n",
      "epoch: 69, batch: 35, loss: 1.8017327785491943\n",
      "epoch: 69, batch: 36, loss: 1.639318823814392\n",
      "epoch: 69, batch: 37, loss: 1.6454579830169678\n",
      "epoch: 69, batch: 38, loss: 1.3000017404556274\n",
      "epoch: 69, batch: 39, loss: 1.686431884765625\n",
      "epoch: 69, batch: 40, loss: 1.1800320148468018\n",
      "epoch: 69, batch: 41, loss: 1.4829577207565308\n",
      "epoch: 69, batch: 42, loss: 2.032127857208252\n",
      "epoch: 69, batch: 43, loss: 1.6917064189910889\n",
      "epoch: 69, batch: 44, loss: 1.691697120666504\n",
      "epoch: 69, batch: 45, loss: 1.5248234272003174\n",
      "epoch: 69, batch: 46, loss: 1.2997729778289795\n",
      "epoch: 69, batch: 47, loss: 1.341678500175476\n",
      "epoch: 69, batch: 48, loss: 1.612830400466919\n",
      "epoch: 69, batch: 49, loss: 1.6865546703338623\n",
      "epoch: 69, batch: 50, loss: 1.2996585369110107\n",
      "epoch: 69, batch: 51, loss: 1.6815290451049805\n",
      "epoch: 69, batch: 52, loss: 1.8432252407073975\n",
      "epoch: 69, batch: 53, loss: 1.3466415405273438\n",
      "epoch: 69, batch: 54, loss: 1.2995439767837524\n",
      "epoch: 69, batch: 55, loss: 1.456206202507019\n",
      "epoch: 69, batch: 56, loss: 1.5671172142028809\n",
      "epoch: 69, batch: 57, loss: 1.179958701133728\n",
      "epoch: 69, batch: 58, loss: 1.7288347482681274\n",
      "epoch: 69, batch: 59, loss: 1.4561681747436523\n",
      "epoch: 69, batch: 60, loss: 1.4090945720672607\n",
      "epoch: 69, batch: 61, loss: 1.6444698572158813\n",
      "epoch: 69, batch: 62, loss: 1.4609482288360596\n",
      "epoch: 69, batch: 63, loss: 1.5706982612609863\n",
      "epoch: 69, batch: 64, loss: 1.4153110980987549\n",
      "epoch: 69, batch: 65, loss: 1.4090512990951538\n",
      "epoch: 69, batch: 66, loss: 1.4090425968170166\n",
      "epoch: 69, batch: 67, loss: 1.1846126317977905\n",
      "epoch: 69, batch: 68, loss: 1.5753180980682373\n",
      "epoch: 69, batch: 69, loss: 1.3461717367172241\n",
      "epoch: 69, batch: 70, loss: 1.3415086269378662\n",
      "epoch: 69, batch: 71, loss: 1.2520049810409546\n",
      "epoch: 69, batch: 72, loss: 1.253670334815979\n",
      "epoch: 69, batch: 73, loss: 1.5298172235488892\n",
      "epoch: 69, batch: 74, loss: 1.3369333744049072\n",
      "epoch: 69, batch: 75, loss: 1.7701585292816162\n",
      "epoch: 69, batch: 76, loss: 1.4089571237564087\n",
      "epoch: 69, batch: 77, loss: 1.0227566957473755\n",
      "epoch: 69, batch: 78, loss: 1.408940076828003\n",
      "epoch: 69, batch: 79, loss: 1.686924934387207\n",
      "epoch: 69, batch: 80, loss: 1.1372194290161133\n",
      "epoch: 69, batch: 81, loss: 1.337035059928894\n",
      "epoch: 69, batch: 82, loss: 1.8058620691299438\n",
      "epoch: 69, batch: 83, loss: 1.826127290725708\n",
      "epoch: 69, batch: 84, loss: 1.5254175662994385\n",
      "epoch: 69, batch: 85, loss: 1.6379331350326538\n",
      "epoch: 69, batch: 86, loss: 1.7747608423233032\n",
      "epoch: 69, batch: 87, loss: 1.5254632234573364\n",
      "epoch: 69, batch: 88, loss: 1.7566235065460205\n",
      "epoch: 69, batch: 89, loss: 1.3371496200561523\n",
      "epoch: 69, batch: 90, loss: 1.75873601436615\n",
      "epoch: 69, batch: 91, loss: 1.597174048423767\n",
      "epoch: 69, batch: 92, loss: 1.3371922969818115\n",
      "epoch: 69, batch: 93, loss: 1.4558453559875488\n",
      "epoch: 69, batch: 94, loss: 1.6871130466461182\n",
      "epoch: 69, batch: 95, loss: 1.4599100351333618\n",
      "epoch: 69, batch: 96, loss: 1.613294243812561\n",
      "epoch: 69, batch: 97, loss: 1.4087753295898438\n",
      "epoch: 69, batch: 98, loss: 1.2942640781402588\n",
      "epoch: 69, batch: 99, loss: 1.1367226839065552\n",
      "epoch: 69, batch: 100, loss: 1.7546558380126953\n",
      "epoch: 69, batch: 101, loss: 1.7342315912246704\n",
      "epoch: 69, batch: 102, loss: 1.3451995849609375\n",
      "epoch: 69, batch: 103, loss: 1.7571780681610107\n",
      "epoch: 69, batch: 104, loss: 1.2535988092422485\n",
      "epoch: 69, batch: 105, loss: 1.687251091003418\n",
      "epoch: 69, batch: 106, loss: 1.4799985885620117\n",
      "epoch: 69, batch: 107, loss: 1.5027430057525635\n",
      "epoch: 69, batch: 108, loss: 1.7749208211898804\n",
      "epoch: 69, batch: 109, loss: 1.6402723789215088\n",
      "epoch: 69, batch: 110, loss: 1.4113295078277588\n",
      "epoch: 69, batch: 111, loss: 1.8746473789215088\n",
      "epoch: 69, batch: 112, loss: 1.4123591184616089\n",
      "epoch: 69, batch: 113, loss: 1.6171576976776123\n",
      "epoch: 69, batch: 114, loss: 1.5728952884674072\n",
      "epoch: 69, batch: 115, loss: 1.8018442392349243\n",
      "epoch: 69, batch: 116, loss: 1.714263916015625\n",
      "epoch: 69, batch: 117, loss: 1.4520050287246704\n",
      "epoch: 69, batch: 118, loss: 1.617100477218628\n",
      "epoch: 69, batch: 119, loss: 1.7351207733154297\n",
      "epoch: 69, batch: 120, loss: 1.455592393875122\n",
      "epoch: 69, batch: 121, loss: 1.4591140747070312\n",
      "epoch: 69, batch: 122, loss: 1.525984764099121\n",
      "epoch: 69, batch: 123, loss: 1.52948796749115\n",
      "epoch: 69, batch: 124, loss: 1.34457528591156\n",
      "epoch: 69, batch: 125, loss: 1.6101276874542236\n",
      "epoch: 69, batch: 126, loss: 1.6840909719467163\n",
      "epoch: 69, batch: 127, loss: 1.6521859169006348\n",
      "epoch: 70, batch: 0, loss: 1.135967493057251\n",
      "epoch: 70, batch: 1, loss: 1.684192419052124\n",
      "epoch: 70, batch: 2, loss: 1.613621711730957\n",
      "epoch: 70, batch: 3, loss: 1.297349214553833\n",
      "epoch: 70, batch: 4, loss: 1.754992127418518\n",
      "epoch: 70, batch: 5, loss: 1.8155797719955444\n",
      "epoch: 70, batch: 6, loss: 1.7378984689712524\n",
      "epoch: 70, batch: 7, loss: 1.6843936443328857\n",
      "epoch: 70, batch: 8, loss: 1.3410240411758423\n",
      "epoch: 70, batch: 9, loss: 1.3410167694091797\n",
      "epoch: 70, batch: 10, loss: 1.6472200155258179\n",
      "epoch: 70, batch: 11, loss: 1.643802285194397\n",
      "epoch: 70, batch: 12, loss: 1.775151252746582\n",
      "epoch: 70, batch: 13, loss: 1.4993135929107666\n",
      "epoch: 70, batch: 14, loss: 1.6442029476165771\n",
      "epoch: 70, batch: 15, loss: 1.5262967348098755\n",
      "epoch: 70, batch: 16, loss: 1.340966820716858\n",
      "epoch: 70, batch: 17, loss: 1.408333659172058\n",
      "epoch: 70, batch: 18, loss: 1.3409521579742432\n",
      "epoch: 70, batch: 19, loss: 1.5733810663223267\n",
      "epoch: 70, batch: 20, loss: 1.8521759510040283\n",
      "epoch: 70, batch: 21, loss: 1.179513692855835\n",
      "epoch: 70, batch: 22, loss: 1.1824369430541992\n",
      "epoch: 70, batch: 23, loss: 1.5264148712158203\n",
      "epoch: 70, batch: 24, loss: 1.3437951803207397\n",
      "epoch: 70, batch: 25, loss: 1.4149277210235596\n",
      "epoch: 70, batch: 26, loss: 1.2938709259033203\n",
      "epoch: 70, batch: 27, loss: 1.249671220779419\n",
      "epoch: 70, batch: 28, loss: 1.4149166345596313\n",
      "epoch: 70, batch: 29, loss: 1.296636939048767\n",
      "epoch: 70, batch: 30, loss: 1.6138641834259033\n",
      "epoch: 70, batch: 31, loss: 1.457973837852478\n",
      "epoch: 70, batch: 32, loss: 1.5668652057647705\n",
      "epoch: 70, batch: 33, loss: 1.3435426950454712\n",
      "epoch: 70, batch: 34, loss: 1.4578847885131836\n",
      "epoch: 70, batch: 35, loss: 1.5292574167251587\n",
      "epoch: 70, batch: 36, loss: 1.293801188468933\n",
      "epoch: 70, batch: 37, loss: 1.2937942743301392\n",
      "epoch: 70, batch: 38, loss: 1.4525588750839233\n",
      "epoch: 70, batch: 39, loss: 1.5762426853179932\n",
      "epoch: 70, batch: 40, loss: 1.7350432872772217\n",
      "epoch: 70, batch: 41, loss: 1.8024104833602905\n",
      "epoch: 70, batch: 42, loss: 1.6905819177627563\n",
      "epoch: 70, batch: 43, loss: 1.6139774322509766\n",
      "epoch: 70, batch: 44, loss: 1.5719417333602905\n",
      "epoch: 70, batch: 45, loss: 1.4123857021331787\n",
      "epoch: 70, batch: 46, loss: 1.2961692810058594\n",
      "epoch: 70, batch: 47, loss: 1.33830726146698\n",
      "epoch: 70, batch: 48, loss: 1.5718156099319458\n",
      "epoch: 70, batch: 49, loss: 1.1793606281280518\n",
      "epoch: 70, batch: 50, loss: 1.6435216665267944\n",
      "epoch: 70, batch: 51, loss: 1.3406894207000732\n",
      "epoch: 70, batch: 52, loss: 1.179343819618225\n",
      "epoch: 70, batch: 53, loss: 1.5223827362060547\n",
      "epoch: 70, batch: 54, loss: 1.57162606716156\n",
      "epoch: 70, batch: 55, loss: 1.6610682010650635\n",
      "epoch: 70, batch: 56, loss: 1.497524619102478\n",
      "epoch: 70, batch: 57, loss: 1.5245519876480103\n",
      "epoch: 70, batch: 58, loss: 1.5019513368606567\n",
      "epoch: 70, batch: 59, loss: 1.3384612798690796\n",
      "epoch: 70, batch: 60, loss: 1.3384740352630615\n",
      "epoch: 70, batch: 61, loss: 1.5269869565963745\n",
      "epoch: 70, batch: 62, loss: 1.5019060373306274\n",
      "epoch: 70, batch: 63, loss: 1.5964648723602295\n",
      "epoch: 70, batch: 64, loss: 1.5270328521728516\n",
      "epoch: 70, batch: 65, loss: 1.6631723642349243\n",
      "epoch: 70, batch: 66, loss: 1.688361406326294\n",
      "epoch: 70, batch: 67, loss: 1.4099218845367432\n",
      "epoch: 70, batch: 68, loss: 1.6434004306793213\n",
      "epoch: 70, batch: 69, loss: 1.8749721050262451\n",
      "epoch: 70, batch: 70, loss: 1.4821009635925293\n",
      "epoch: 70, batch: 71, loss: 1.688423752784729\n",
      "epoch: 70, batch: 72, loss: 1.8046554327011108\n",
      "epoch: 70, batch: 73, loss: 1.4128406047821045\n",
      "epoch: 70, batch: 74, loss: 1.3677572011947632\n",
      "epoch: 70, batch: 75, loss: 1.6902987957000732\n",
      "epoch: 70, batch: 76, loss: 1.4548038244247437\n",
      "epoch: 70, batch: 77, loss: 1.5673378705978394\n",
      "epoch: 70, batch: 78, loss: 1.5290095806121826\n",
      "epoch: 70, batch: 79, loss: 1.3421919345855713\n",
      "epoch: 70, batch: 80, loss: 1.8045791387557983\n",
      "epoch: 70, batch: 81, loss: 1.4129716157913208\n",
      "epoch: 70, batch: 82, loss: 1.7354917526245117\n",
      "epoch: 70, batch: 83, loss: 1.7286516427993774\n",
      "epoch: 70, batch: 84, loss: 1.293482780456543\n",
      "epoch: 70, batch: 85, loss: 1.1791582107543945\n",
      "epoch: 70, batch: 86, loss: 1.7612724304199219\n",
      "epoch: 70, batch: 87, loss: 1.5000512599945068\n",
      "epoch: 70, batch: 88, loss: 1.4562400579452515\n",
      "epoch: 70, batch: 89, loss: 1.715995192527771\n",
      "epoch: 70, batch: 90, loss: 1.5909757614135742\n",
      "epoch: 70, batch: 91, loss: 1.1791261434555054\n",
      "epoch: 70, batch: 92, loss: 1.4561175107955933\n",
      "epoch: 70, batch: 93, loss: 1.294867753982544\n",
      "epoch: 70, batch: 94, loss: 1.5963176488876343\n",
      "epoch: 70, batch: 95, loss: 1.6432178020477295\n",
      "epoch: 70, batch: 96, loss: 1.180477499961853\n",
      "epoch: 70, batch: 97, loss: 1.7274086475372314\n",
      "epoch: 70, batch: 98, loss: 1.407698392868042\n",
      "epoch: 70, batch: 99, loss: 1.362105131149292\n",
      "epoch: 70, batch: 100, loss: 1.3620774745941162\n",
      "epoch: 70, batch: 101, loss: 1.8087276220321655\n",
      "epoch: 70, batch: 102, loss: 1.6145069599151611\n",
      "epoch: 70, batch: 103, loss: 1.4088863134384155\n",
      "epoch: 70, batch: 104, loss: 1.6876325607299805\n",
      "epoch: 70, batch: 105, loss: 1.5700126886367798\n",
      "epoch: 70, batch: 106, loss: 1.8019940853118896\n",
      "epoch: 70, batch: 107, loss: 1.6489042043685913\n",
      "epoch: 70, batch: 108, loss: 1.5687971115112305\n",
      "epoch: 70, batch: 109, loss: 1.6145751476287842\n",
      "epoch: 70, batch: 110, loss: 1.5735511779785156\n",
      "epoch: 70, batch: 111, loss: 1.4075946807861328\n",
      "epoch: 70, batch: 112, loss: 1.5013549327850342\n",
      "epoch: 70, batch: 113, loss: 1.7358356714248657\n",
      "epoch: 70, batch: 114, loss: 1.6430730819702148\n",
      "epoch: 70, batch: 115, loss: 1.643065094947815\n",
      "epoch: 70, batch: 116, loss: 1.40755295753479\n",
      "epoch: 70, batch: 117, loss: 1.7349560260772705\n",
      "epoch: 70, batch: 118, loss: 1.6421353816986084\n",
      "epoch: 70, batch: 119, loss: 1.4535220861434937\n",
      "epoch: 70, batch: 120, loss: 1.5739089250564575\n",
      "epoch: 70, batch: 121, loss: 1.6890532970428467\n",
      "epoch: 70, batch: 122, loss: 1.763432502746582\n",
      "epoch: 70, batch: 123, loss: 1.4535692930221558\n",
      "epoch: 70, batch: 124, loss: 1.3401020765304565\n",
      "epoch: 70, batch: 125, loss: 1.6147266626358032\n",
      "epoch: 70, batch: 126, loss: 1.688376784324646\n",
      "epoch: 70, batch: 127, loss: 1.4469923973083496\n",
      "epoch: 71, batch: 0, loss: 1.293897271156311\n",
      "epoch: 71, batch: 1, loss: 1.2931914329528809\n",
      "epoch: 71, batch: 2, loss: 1.4074320793151855\n",
      "epoch: 71, batch: 3, loss: 1.4542982578277588\n",
      "epoch: 71, batch: 4, loss: 1.6147884130477905\n",
      "epoch: 71, batch: 5, loss: 2.044811964035034\n",
      "epoch: 71, batch: 6, loss: 1.246861219406128\n",
      "epoch: 71, batch: 7, loss: 1.4542617797851562\n",
      "epoch: 71, batch: 8, loss: 1.5684871673583984\n",
      "epoch: 71, batch: 9, loss: 1.293656349182129\n",
      "epoch: 71, batch: 10, loss: 1.8105703592300415\n",
      "epoch: 71, batch: 11, loss: 1.5286575555801392\n",
      "epoch: 71, batch: 12, loss: 1.689292311668396\n",
      "epoch: 71, batch: 13, loss: 1.689305067062378\n",
      "epoch: 71, batch: 14, loss: 1.6152957677841187\n",
      "epoch: 71, batch: 15, loss: 1.500259518623352\n",
      "epoch: 71, batch: 16, loss: 1.6148900985717773\n",
      "epoch: 71, batch: 17, loss: 1.5010477304458618\n",
      "epoch: 71, batch: 18, loss: 1.2533214092254639\n",
      "epoch: 71, batch: 19, loss: 1.924607515335083\n",
      "epoch: 71, batch: 20, loss: 1.6428216695785522\n",
      "epoch: 71, batch: 21, loss: 1.5749132633209229\n",
      "epoch: 71, batch: 22, loss: 1.7239036560058594\n",
      "epoch: 71, batch: 23, loss: 1.6896775960922241\n",
      "epoch: 71, batch: 24, loss: 1.5685350894927979\n",
      "epoch: 71, batch: 25, loss: 1.132158875465393\n",
      "epoch: 71, batch: 26, loss: 1.2932065725326538\n",
      "epoch: 71, batch: 27, loss: 1.6355903148651123\n",
      "epoch: 71, batch: 28, loss: 1.501097559928894\n",
      "epoch: 71, batch: 29, loss: 1.4540621042251587\n",
      "epoch: 71, batch: 30, loss: 1.83843195438385\n",
      "epoch: 71, batch: 31, loss: 1.5009198188781738\n",
      "epoch: 71, batch: 32, loss: 1.4539635181427002\n",
      "epoch: 71, batch: 33, loss: 1.3397934436798096\n",
      "epoch: 71, batch: 34, loss: 1.2929948568344116\n",
      "epoch: 71, batch: 35, loss: 1.5958425998687744\n",
      "epoch: 71, batch: 36, loss: 1.5681657791137695\n",
      "epoch: 71, batch: 37, loss: 1.4070879220962524\n",
      "epoch: 71, batch: 38, loss: 1.454028606414795\n",
      "epoch: 71, batch: 39, loss: 1.4143520593643188\n",
      "epoch: 71, batch: 40, loss: 1.367475152015686\n",
      "epoch: 71, batch: 41, loss: 1.6149917840957642\n",
      "epoch: 71, batch: 42, loss: 1.5212339162826538\n",
      "epoch: 71, batch: 43, loss: 1.5756683349609375\n",
      "epoch: 71, batch: 44, loss: 1.8113014698028564\n",
      "epoch: 71, batch: 45, loss: 1.575362205505371\n",
      "epoch: 71, batch: 46, loss: 1.3674577474594116\n",
      "epoch: 71, batch: 47, loss: 1.5287069082260132\n",
      "epoch: 71, batch: 48, loss: 1.4538915157318115\n",
      "epoch: 71, batch: 49, loss: 1.454150915145874\n",
      "epoch: 71, batch: 50, loss: 1.1784225702285767\n",
      "epoch: 71, batch: 51, loss: 1.6502315998077393\n",
      "epoch: 71, batch: 52, loss: 1.6897945404052734\n",
      "epoch: 71, batch: 53, loss: 1.5284473896026611\n",
      "epoch: 71, batch: 54, loss: 1.6628258228302002\n",
      "epoch: 71, batch: 55, loss: 1.4143015146255493\n",
      "epoch: 71, batch: 56, loss: 1.575711727142334\n",
      "epoch: 71, batch: 57, loss: 1.2923800945281982\n",
      "epoch: 71, batch: 58, loss: 1.8122574090957642\n",
      "epoch: 71, batch: 59, loss: 1.6621322631835938\n",
      "epoch: 71, batch: 60, loss: 1.3401479721069336\n",
      "epoch: 71, batch: 61, loss: 1.8040330410003662\n",
      "epoch: 71, batch: 62, loss: 1.359499454498291\n",
      "epoch: 71, batch: 63, loss: 1.5678799152374268\n",
      "epoch: 71, batch: 64, loss: 1.8515031337738037\n",
      "epoch: 71, batch: 65, loss: 1.4068723917007446\n",
      "epoch: 71, batch: 66, loss: 1.5758554935455322\n",
      "epoch: 71, batch: 67, loss: 1.6153323650360107\n",
      "epoch: 71, batch: 68, loss: 1.2920875549316406\n",
      "epoch: 71, batch: 69, loss: 1.3673866987228394\n",
      "epoch: 71, batch: 70, loss: 1.4814939498901367\n",
      "epoch: 71, batch: 71, loss: 1.4061172008514404\n",
      "epoch: 71, batch: 72, loss: 1.452955961227417\n",
      "epoch: 71, batch: 73, loss: 1.681138038635254\n",
      "epoch: 71, batch: 74, loss: 1.6146304607391357\n",
      "epoch: 71, batch: 75, loss: 1.4067862033843994\n",
      "epoch: 71, batch: 76, loss: 1.4544435739517212\n",
      "epoch: 71, batch: 77, loss: 1.4059494733810425\n",
      "epoch: 71, batch: 78, loss: 1.614587426185608\n",
      "epoch: 71, batch: 79, loss: 1.7648369073867798\n",
      "epoch: 71, batch: 80, loss: 1.4536080360412598\n",
      "epoch: 71, batch: 81, loss: 1.4527019262313843\n",
      "epoch: 71, batch: 82, loss: 1.8033466339111328\n",
      "epoch: 71, batch: 83, loss: 1.5292346477508545\n",
      "epoch: 71, batch: 84, loss: 1.4067081212997437\n",
      "epoch: 71, batch: 85, loss: 1.5676475763320923\n",
      "epoch: 71, batch: 86, loss: 1.6892290115356445\n",
      "epoch: 71, batch: 87, loss: 1.6144899129867554\n",
      "epoch: 71, batch: 88, loss: 1.5676147937774658\n",
      "epoch: 71, batch: 89, loss: 1.6144676208496094\n",
      "epoch: 71, batch: 90, loss: 1.130570650100708\n",
      "epoch: 71, batch: 91, loss: 1.899181604385376\n",
      "epoch: 71, batch: 92, loss: 1.2925701141357422\n",
      "epoch: 71, batch: 93, loss: 1.6347640752792358\n",
      "epoch: 71, batch: 94, loss: 1.3394200801849365\n",
      "epoch: 71, batch: 95, loss: 1.2913744449615479\n",
      "epoch: 71, batch: 96, loss: 1.5294288396835327\n",
      "epoch: 71, batch: 97, loss: 1.5027544498443604\n",
      "epoch: 71, batch: 98, loss: 1.8525310754776\n",
      "epoch: 71, batch: 99, loss: 1.4053213596343994\n",
      "epoch: 71, batch: 100, loss: 1.3393806219100952\n",
      "epoch: 71, batch: 101, loss: 1.5295042991638184\n",
      "epoch: 71, batch: 102, loss: 1.4547317028045654\n",
      "epoch: 71, batch: 103, loss: 1.757622480392456\n",
      "epoch: 71, batch: 104, loss: 1.6525636911392212\n",
      "epoch: 71, batch: 105, loss: 1.3379707336425781\n",
      "epoch: 71, batch: 106, loss: 1.453378677368164\n",
      "epoch: 71, batch: 107, loss: 1.4141432046890259\n",
      "epoch: 71, batch: 108, loss: 1.3672800064086914\n",
      "epoch: 71, batch: 109, loss: 1.8528616428375244\n",
      "epoch: 71, batch: 110, loss: 1.4813027381896973\n",
      "epoch: 71, batch: 111, loss: 1.8544166088104248\n",
      "epoch: 71, batch: 112, loss: 1.4064648151397705\n",
      "epoch: 71, batch: 113, loss: 1.2909008264541626\n",
      "epoch: 71, batch: 114, loss: 1.5281449556350708\n",
      "epoch: 71, batch: 115, loss: 1.662610411643982\n",
      "epoch: 71, batch: 116, loss: 1.3408687114715576\n",
      "epoch: 71, batch: 117, loss: 1.3408814668655396\n",
      "epoch: 71, batch: 118, loss: 1.4532725811004639\n",
      "epoch: 71, batch: 119, loss: 1.617431640625\n",
      "epoch: 71, batch: 120, loss: 1.1767150163650513\n",
      "epoch: 71, batch: 121, loss: 1.178378701210022\n",
      "epoch: 71, batch: 122, loss: 1.5952476263046265\n",
      "epoch: 71, batch: 123, loss: 1.5672262907028198\n",
      "epoch: 71, batch: 124, loss: 1.6907037496566772\n",
      "epoch: 71, batch: 125, loss: 1.8253114223480225\n",
      "epoch: 71, batch: 126, loss: 1.2923461198806763\n",
      "epoch: 71, batch: 127, loss: 1.560319185256958\n",
      "epoch: 72, batch: 0, loss: 1.4140981435775757\n",
      "epoch: 72, batch: 1, loss: 1.6907665729522705\n",
      "epoch: 72, batch: 2, loss: 1.5749431848526\n",
      "epoch: 72, batch: 3, loss: 1.5280746221542358\n",
      "epoch: 72, batch: 4, loss: 1.406282901763916\n",
      "epoch: 72, batch: 5, loss: 1.6608473062515259\n",
      "epoch: 72, batch: 6, loss: 1.2903642654418945\n",
      "epoch: 72, batch: 7, loss: 1.2532470226287842\n",
      "epoch: 72, batch: 8, loss: 1.5651227235794067\n",
      "epoch: 72, batch: 9, loss: 1.5182281732559204\n",
      "epoch: 72, batch: 10, loss: 1.567062258720398\n",
      "epoch: 72, batch: 11, loss: 1.6607818603515625\n",
      "epoch: 72, batch: 12, loss: 1.3411476612091064\n",
      "epoch: 72, batch: 13, loss: 1.680991530418396\n",
      "epoch: 72, batch: 14, loss: 1.6138836145401\n",
      "epoch: 72, batch: 15, loss: 1.7257392406463623\n",
      "epoch: 72, batch: 16, loss: 1.6117511987686157\n",
      "epoch: 72, batch: 17, loss: 1.6809403896331787\n",
      "epoch: 72, batch: 18, loss: 1.455170750617981\n",
      "epoch: 72, batch: 19, loss: 1.566964030265808\n",
      "epoch: 72, batch: 20, loss: 1.4039565324783325\n",
      "epoch: 72, batch: 21, loss: 1.3412554264068604\n",
      "epoch: 72, batch: 22, loss: 1.5770857334136963\n",
      "epoch: 72, batch: 23, loss: 1.641937494277954\n",
      "epoch: 72, batch: 24, loss: 1.28990638256073\n",
      "epoch: 72, batch: 25, loss: 1.4163329601287842\n",
      "epoch: 72, batch: 26, loss: 1.3413177728652954\n",
      "epoch: 72, batch: 27, loss: 1.3390088081359863\n",
      "epoch: 72, batch: 28, loss: 1.2921448945999146\n",
      "epoch: 72, batch: 29, loss: 1.406067132949829\n",
      "epoch: 72, batch: 30, loss: 1.4914734363555908\n",
      "epoch: 72, batch: 31, loss: 1.2921262979507446\n",
      "epoch: 72, batch: 32, loss: 1.0150039196014404\n",
      "epoch: 72, batch: 33, loss: 1.8050858974456787\n",
      "epoch: 72, batch: 34, loss: 1.517486572265625\n",
      "epoch: 72, batch: 35, loss: 1.6443383693695068\n",
      "epoch: 72, batch: 36, loss: 1.4528604745864868\n",
      "epoch: 72, batch: 37, loss: 1.644364595413208\n",
      "epoch: 72, batch: 38, loss: 1.5692847967147827\n",
      "epoch: 72, batch: 39, loss: 1.5773341655731201\n",
      "epoch: 72, batch: 40, loss: 1.1287424564361572\n",
      "epoch: 72, batch: 41, loss: 1.128719449043274\n",
      "epoch: 72, batch: 42, loss: 1.5305283069610596\n",
      "epoch: 72, batch: 43, loss: 1.9324133396148682\n",
      "epoch: 72, batch: 44, loss: 1.6444542407989502\n",
      "epoch: 72, batch: 45, loss: 1.2425203323364258\n",
      "epoch: 72, batch: 46, loss: 1.694013237953186\n",
      "epoch: 72, batch: 47, loss: 1.3671596050262451\n",
      "epoch: 72, batch: 48, loss: 1.4554742574691772\n",
      "epoch: 72, batch: 49, loss: 1.4527446031570435\n",
      "epoch: 72, batch: 50, loss: 1.5774977207183838\n",
      "epoch: 72, batch: 51, loss: 1.6913928985595703\n",
      "epoch: 72, batch: 52, loss: 1.4058650732040405\n",
      "epoch: 72, batch: 53, loss: 1.730126142501831\n",
      "epoch: 72, batch: 54, loss: 1.4058459997177124\n",
      "epoch: 72, batch: 55, loss: 1.5307174921035767\n",
      "epoch: 72, batch: 56, loss: 1.6914567947387695\n",
      "epoch: 72, batch: 57, loss: 1.2919502258300781\n",
      "epoch: 72, batch: 58, loss: 1.4058068990707397\n",
      "epoch: 72, batch: 59, loss: 1.5665181875228882\n",
      "epoch: 72, batch: 60, loss: 1.8165085315704346\n",
      "epoch: 72, batch: 61, loss: 1.6885690689086914\n",
      "epoch: 72, batch: 62, loss: 1.3387755155563354\n",
      "epoch: 72, batch: 63, loss: 1.0143508911132812\n",
      "epoch: 72, batch: 64, loss: 1.5694644451141357\n",
      "epoch: 72, batch: 65, loss: 1.680298089981079\n",
      "epoch: 72, batch: 66, loss: 1.6915842294692993\n",
      "epoch: 72, batch: 67, loss: 1.4170422554016113\n",
      "epoch: 72, batch: 68, loss: 1.5308983325958252\n",
      "epoch: 72, batch: 69, loss: 1.5777852535247803\n",
      "epoch: 72, batch: 70, loss: 1.3387253284454346\n",
      "epoch: 72, batch: 71, loss: 1.9023529291152954\n",
      "epoch: 72, batch: 72, loss: 1.341848611831665\n",
      "epoch: 72, batch: 73, loss: 1.4525336027145386\n",
      "epoch: 72, batch: 74, loss: 1.6163997650146484\n",
      "epoch: 72, batch: 75, loss: 1.7270444631576538\n",
      "epoch: 72, batch: 76, loss: 1.566327691078186\n",
      "epoch: 72, batch: 77, loss: 1.4172022342681885\n",
      "epoch: 72, batch: 78, loss: 1.4023693799972534\n",
      "epoch: 72, batch: 79, loss: 1.6164315938949585\n",
      "epoch: 72, batch: 80, loss: 1.8023090362548828\n",
      "epoch: 72, batch: 81, loss: 1.1277815103530884\n",
      "epoch: 72, batch: 82, loss: 1.3353520631790161\n",
      "epoch: 72, batch: 83, loss: 1.7671442031860352\n",
      "epoch: 72, batch: 84, loss: 1.4055395126342773\n",
      "epoch: 72, batch: 85, loss: 1.3419710397720337\n",
      "epoch: 72, batch: 86, loss: 1.527782678604126\n",
      "epoch: 72, batch: 87, loss: 1.6600062847137451\n",
      "epoch: 72, batch: 88, loss: 1.5661931037902832\n",
      "epoch: 72, batch: 89, loss: 1.7302815914154053\n",
      "epoch: 72, batch: 90, loss: 1.40547776222229\n",
      "epoch: 72, batch: 91, loss: 1.7737579345703125\n",
      "epoch: 72, batch: 92, loss: 1.6165094375610352\n",
      "epoch: 72, batch: 93, loss: 1.6199842691421509\n",
      "epoch: 72, batch: 94, loss: 1.767356514930725\n",
      "epoch: 72, batch: 95, loss: 1.7388627529144287\n",
      "epoch: 72, batch: 96, loss: 1.8463432788848877\n",
      "epoch: 72, batch: 97, loss: 1.5782047510147095\n",
      "epoch: 72, batch: 98, loss: 2.0106589794158936\n",
      "epoch: 72, batch: 99, loss: 1.6067708730697632\n",
      "epoch: 72, batch: 100, loss: 1.6165530681610107\n",
      "epoch: 72, batch: 101, loss: 1.5277515649795532\n",
      "epoch: 72, batch: 102, loss: 1.9859386682510376\n",
      "epoch: 72, batch: 103, loss: 1.5277483463287354\n",
      "epoch: 72, batch: 104, loss: 1.5660120248794556\n",
      "epoch: 72, batch: 105, loss: 1.4991711378097534\n",
      "epoch: 72, batch: 106, loss: 1.4522392749786377\n",
      "epoch: 72, batch: 107, loss: 1.3384822607040405\n",
      "epoch: 72, batch: 108, loss: 1.413994312286377\n",
      "epoch: 72, batch: 109, loss: 1.2878261804580688\n",
      "epoch: 72, batch: 110, loss: 1.7589495182037354\n",
      "epoch: 72, batch: 111, loss: 1.6070225238800049\n",
      "epoch: 72, batch: 112, loss: 1.7428518533706665\n",
      "epoch: 72, batch: 113, loss: 1.499106526374817\n",
      "epoch: 72, batch: 114, loss: 1.2915030717849731\n",
      "epoch: 72, batch: 115, loss: 1.6166263818740845\n",
      "epoch: 72, batch: 116, loss: 1.4013941287994385\n",
      "epoch: 72, batch: 117, loss: 1.0132582187652588\n",
      "epoch: 72, batch: 118, loss: 1.177748680114746\n",
      "epoch: 72, batch: 119, loss: 1.7353156805038452\n",
      "epoch: 72, batch: 120, loss: 1.805978536605835\n",
      "epoch: 72, batch: 121, loss: 1.4482066631317139\n",
      "epoch: 72, batch: 122, loss: 1.4520944356918335\n",
      "epoch: 72, batch: 123, loss: 1.4560123682022095\n",
      "epoch: 72, batch: 124, loss: 1.4140043258666992\n",
      "epoch: 72, batch: 125, loss: 1.4990066289901733\n",
      "epoch: 72, batch: 126, loss: 1.4807729721069336\n",
      "epoch: 72, batch: 127, loss: 1.3560035228729248\n",
      "epoch: 73, batch: 0, loss: 1.806060791015625\n",
      "epoch: 73, batch: 1, loss: 1.8060710430145264\n",
      "epoch: 73, batch: 2, loss: 1.342355489730835\n",
      "epoch: 73, batch: 3, loss: 1.4140068292617798\n",
      "epoch: 73, batch: 4, loss: 1.3342431783676147\n",
      "epoch: 73, batch: 5, loss: 1.5030171871185303\n",
      "epoch: 73, batch: 6, loss: 1.5944499969482422\n",
      "epoch: 73, batch: 7, loss: 1.3423969745635986\n",
      "epoch: 73, batch: 8, loss: 1.4181339740753174\n",
      "epoch: 73, batch: 9, loss: 1.4008736610412598\n",
      "epoch: 73, batch: 10, loss: 1.7394230365753174\n",
      "epoch: 73, batch: 11, loss: 1.342429518699646\n",
      "epoch: 73, batch: 12, loss: 1.561425805091858\n",
      "epoch: 73, batch: 13, loss: 1.53189218044281\n",
      "epoch: 73, batch: 14, loss: 1.4140079021453857\n",
      "epoch: 73, batch: 15, loss: 1.7858505249023438\n",
      "epoch: 73, batch: 16, loss: 1.641359567642212\n",
      "epoch: 73, batch: 17, loss: 1.404942274093628\n",
      "epoch: 73, batch: 18, loss: 1.7261669635772705\n",
      "epoch: 73, batch: 19, loss: 1.6124837398529053\n",
      "epoch: 73, batch: 20, loss: 1.616783857345581\n",
      "epoch: 73, batch: 21, loss: 1.353635549545288\n",
      "epoch: 73, batch: 22, loss: 1.5746195316314697\n",
      "epoch: 73, batch: 23, loss: 1.9334462881088257\n",
      "epoch: 73, batch: 24, loss: 1.5075098276138306\n",
      "epoch: 73, batch: 25, loss: 1.7439943552017212\n",
      "epoch: 73, batch: 26, loss: 1.4048599004745483\n",
      "epoch: 73, batch: 27, loss: 1.2911924123764038\n",
      "epoch: 73, batch: 28, loss: 1.5943796634674072\n",
      "epoch: 73, batch: 29, loss: 1.4184573888778687\n",
      "epoch: 73, batch: 30, loss: 1.4562280178070068\n",
      "epoch: 73, batch: 31, loss: 1.3533940315246582\n",
      "epoch: 73, batch: 32, loss: 1.2866652011871338\n",
      "epoch: 73, batch: 33, loss: 1.7352025508880615\n",
      "epoch: 73, batch: 34, loss: 1.4002676010131836\n",
      "epoch: 73, batch: 35, loss: 1.6168451309204102\n",
      "epoch: 73, batch: 36, loss: 1.806439995765686\n",
      "epoch: 73, batch: 37, loss: 1.4940897226333618\n",
      "epoch: 73, batch: 38, loss: 1.678973913192749\n",
      "epoch: 73, batch: 39, loss: 1.7774451971054077\n",
      "epoch: 73, batch: 40, loss: 1.612260103225708\n",
      "epoch: 73, batch: 41, loss: 1.7549254894256592\n",
      "epoch: 73, batch: 42, loss: 1.5322891473770142\n",
      "epoch: 73, batch: 43, loss: 1.5606318712234497\n",
      "epoch: 73, batch: 44, loss: 1.4516395330429077\n",
      "epoch: 73, batch: 45, loss: 1.3380038738250732\n",
      "epoch: 73, batch: 46, loss: 1.6976085901260376\n",
      "epoch: 73, batch: 47, loss: 1.8487956523895264\n",
      "epoch: 73, batch: 48, loss: 1.6976619958877563\n",
      "epoch: 73, batch: 49, loss: 1.2393414974212646\n",
      "epoch: 73, batch: 50, loss: 1.4937843084335327\n",
      "epoch: 73, batch: 51, loss: 1.17739737033844\n",
      "epoch: 73, batch: 52, loss: 1.678789496421814\n",
      "epoch: 73, batch: 53, loss: 1.5841853618621826\n",
      "epoch: 73, batch: 54, loss: 1.3670902252197266\n",
      "epoch: 73, batch: 55, loss: 1.4515380859375\n",
      "epoch: 73, batch: 56, loss: 1.2861378192901611\n",
      "epoch: 73, batch: 57, loss: 1.446666955947876\n",
      "epoch: 73, batch: 58, loss: 1.1773622035980225\n",
      "epoch: 73, batch: 59, loss: 1.2909542322158813\n",
      "epoch: 73, batch: 60, loss: 1.2391005754470825\n",
      "epoch: 73, batch: 61, loss: 1.2390787601470947\n",
      "epoch: 73, batch: 62, loss: 1.5181138515472412\n",
      "epoch: 73, batch: 63, loss: 1.5276415348052979\n",
      "epoch: 73, batch: 64, loss: 1.451454758644104\n",
      "epoch: 73, batch: 65, loss: 1.446474552154541\n",
      "epoch: 73, batch: 66, loss: 1.4983865022659302\n",
      "epoch: 73, batch: 67, loss: 1.574588418006897\n",
      "epoch: 73, batch: 68, loss: 1.5649958848953247\n",
      "epoch: 73, batch: 69, loss: 1.3671098947525024\n",
      "epoch: 73, batch: 70, loss: 1.5700194835662842\n",
      "epoch: 73, batch: 71, loss: 1.8164122104644775\n",
      "epoch: 73, batch: 72, loss: 1.4513814449310303\n",
      "epoch: 73, batch: 73, loss: 1.693242073059082\n",
      "epoch: 73, batch: 74, loss: 1.5179768800735474\n",
      "epoch: 73, batch: 75, loss: 1.4806784391403198\n",
      "epoch: 73, batch: 76, loss: 1.6690845489501953\n",
      "epoch: 73, batch: 77, loss: 1.7254035472869873\n",
      "epoch: 73, batch: 78, loss: 1.7357282638549805\n",
      "epoch: 73, batch: 79, loss: 1.1772583723068237\n",
      "epoch: 73, batch: 80, loss: 1.735762357711792\n",
      "epoch: 73, batch: 81, loss: 1.6933391094207764\n",
      "epoch: 73, batch: 82, loss: 1.693351149559021\n",
      "epoch: 73, batch: 83, loss: 1.5034782886505127\n",
      "epoch: 73, batch: 84, loss: 1.451271653175354\n",
      "epoch: 73, batch: 85, loss: 1.2854951620101929\n",
      "epoch: 73, batch: 86, loss: 1.8833030462265015\n",
      "epoch: 73, batch: 87, loss: 1.5178251266479492\n",
      "epoch: 73, batch: 88, loss: 1.7751251459121704\n",
      "epoch: 73, batch: 89, loss: 1.4512255191802979\n",
      "epoch: 73, batch: 90, loss: 1.2853871583938599\n",
      "epoch: 73, batch: 91, loss: 2.012122392654419\n",
      "epoch: 73, batch: 92, loss: 1.503528356552124\n",
      "epoch: 73, batch: 93, loss: 1.177191138267517\n",
      "epoch: 73, batch: 94, loss: 1.7404521703720093\n",
      "epoch: 73, batch: 95, loss: 1.5330339670181274\n",
      "epoch: 73, batch: 96, loss: 1.4195349216461182\n",
      "epoch: 73, batch: 97, loss: 1.5854655504226685\n",
      "epoch: 73, batch: 98, loss: 1.5276165008544922\n",
      "epoch: 73, batch: 99, loss: 1.5646384954452515\n",
      "epoch: 73, batch: 100, loss: 1.4511237144470215\n",
      "epoch: 73, batch: 101, loss: 1.498075008392334\n",
      "epoch: 73, batch: 102, loss: 1.865078330039978\n",
      "epoch: 73, batch: 103, loss: 1.4455678462982178\n",
      "epoch: 73, batch: 104, loss: 1.6880725622177124\n",
      "epoch: 73, batch: 105, loss: 1.4510772228240967\n",
      "epoch: 73, batch: 106, loss: 1.4566364288330078\n",
      "epoch: 73, batch: 107, loss: 1.4197123050689697\n",
      "epoch: 73, batch: 108, loss: 1.39849054813385\n",
      "epoch: 73, batch: 109, loss: 1.2905935049057007\n",
      "epoch: 73, batch: 110, loss: 1.0110392570495605\n",
      "epoch: 73, batch: 111, loss: 1.9420076608657837\n",
      "epoch: 73, batch: 112, loss: 1.9051854610443115\n",
      "epoch: 73, batch: 113, loss: 1.4510024785995483\n",
      "epoch: 73, batch: 114, loss: 1.4040275812149048\n",
      "epoch: 73, batch: 115, loss: 1.1244251728057861\n",
      "epoch: 73, batch: 116, loss: 1.8484890460968018\n",
      "epoch: 73, batch: 117, loss: 1.5276219844818115\n",
      "epoch: 73, batch: 118, loss: 1.3374923467636108\n",
      "epoch: 73, batch: 119, loss: 1.4509464502334595\n",
      "epoch: 73, batch: 120, loss: 1.4141641855239868\n",
      "epoch: 73, batch: 121, loss: 1.936702013015747\n",
      "epoch: 73, batch: 122, loss: 1.3432414531707764\n",
      "epoch: 73, batch: 123, loss: 1.367202877998352\n",
      "epoch: 73, batch: 124, loss: 1.1712234020233154\n",
      "epoch: 73, batch: 125, loss: 1.6101787090301514\n",
      "epoch: 73, batch: 126, loss: 1.4450510740280151\n",
      "epoch: 73, batch: 127, loss: 1.3145184516906738\n",
      "epoch: 74, batch: 0, loss: 1.4806569814682007\n",
      "epoch: 74, batch: 1, loss: 1.6053969860076904\n",
      "epoch: 74, batch: 2, loss: 1.8014799356460571\n",
      "epoch: 74, batch: 3, loss: 1.420084834098816\n",
      "epoch: 74, batch: 4, loss: 1.564263105392456\n",
      "epoch: 74, batch: 5, loss: 1.4567394256591797\n",
      "epoch: 74, batch: 6, loss: 1.8245261907577515\n",
      "epoch: 74, batch: 7, loss: 1.517259120941162\n",
      "epoch: 74, batch: 8, loss: 1.1240208148956299\n",
      "epoch: 74, batch: 9, loss: 1.5037288665771484\n",
      "epoch: 74, batch: 10, loss: 1.517226219177246\n",
      "epoch: 74, batch: 11, loss: 1.4977339506149292\n",
      "epoch: 74, batch: 12, loss: 1.5581591129302979\n",
      "epoch: 74, batch: 13, loss: 2.0612215995788574\n",
      "epoch: 74, batch: 14, loss: 1.509796142578125\n",
      "epoch: 74, batch: 15, loss: 1.6880197525024414\n",
      "epoch: 74, batch: 16, loss: 1.6471176147460938\n",
      "epoch: 74, batch: 17, loss: 1.497679352760315\n",
      "epoch: 74, batch: 18, loss: 1.7410862445831299\n",
      "epoch: 74, batch: 19, loss: 1.7407058477401733\n",
      "epoch: 74, batch: 20, loss: 1.6233155727386475\n",
      "epoch: 74, batch: 21, loss: 1.5579392910003662\n",
      "epoch: 74, batch: 22, loss: 1.8242549896240234\n",
      "epoch: 74, batch: 23, loss: 1.5337963104248047\n",
      "epoch: 74, batch: 24, loss: 1.6518081426620483\n",
      "epoch: 74, batch: 25, loss: 1.4506332874298096\n",
      "epoch: 74, batch: 26, loss: 1.2902520895004272\n",
      "epoch: 74, batch: 27, loss: 1.6047624349594116\n",
      "epoch: 74, batch: 28, loss: 1.6879971027374268\n",
      "epoch: 74, batch: 29, loss: 1.403627872467041\n",
      "epoch: 74, batch: 30, loss: 1.6711151599884033\n",
      "epoch: 74, batch: 31, loss: 1.6109387874603271\n",
      "epoch: 74, batch: 32, loss: 1.5808874368667603\n",
      "epoch: 74, batch: 33, loss: 1.337167739868164\n",
      "epoch: 74, batch: 34, loss: 1.683650255203247\n",
      "epoch: 74, batch: 35, loss: 1.5038381814956665\n",
      "epoch: 74, batch: 36, loss: 1.3307955265045166\n",
      "epoch: 74, batch: 37, loss: 1.5105836391448975\n",
      "epoch: 74, batch: 38, loss: 1.2837915420532227\n",
      "epoch: 74, batch: 39, loss: 1.5276296138763428\n",
      "epoch: 74, batch: 40, loss: 1.6943743228912354\n",
      "epoch: 74, batch: 41, loss: 1.534050703048706\n",
      "epoch: 74, batch: 42, loss: 1.337097406387329\n",
      "epoch: 74, batch: 43, loss: 1.3306396007537842\n",
      "epoch: 74, batch: 44, loss: 1.5276294946670532\n",
      "epoch: 74, batch: 45, loss: 1.6410030126571655\n",
      "epoch: 74, batch: 46, loss: 1.7781299352645874\n",
      "epoch: 74, batch: 47, loss: 1.6237653493881226\n",
      "epoch: 74, batch: 48, loss: 1.8078372478485107\n",
      "epoch: 74, batch: 49, loss: 1.5637779235839844\n",
      "epoch: 74, batch: 50, loss: 1.4034578800201416\n",
      "epoch: 74, batch: 51, loss: 1.6409928798675537\n",
      "epoch: 74, batch: 52, loss: 1.2900789976119995\n",
      "epoch: 74, batch: 53, loss: 1.403434157371521\n",
      "epoch: 74, batch: 54, loss: 1.5811785459518433\n",
      "epoch: 74, batch: 55, loss: 1.3303682804107666\n",
      "epoch: 74, batch: 56, loss: 1.7547820806503296\n",
      "epoch: 74, batch: 57, loss: 1.3436381816864014\n",
      "epoch: 74, batch: 58, loss: 1.2364364862442017\n",
      "epoch: 74, batch: 59, loss: 1.1230647563934326\n",
      "epoch: 74, batch: 60, loss: 1.6879075765609741\n",
      "epoch: 74, batch: 61, loss: 1.2833056449890137\n",
      "epoch: 74, batch: 62, loss: 1.2900149822235107\n",
      "epoch: 74, batch: 63, loss: 1.4210264682769775\n",
      "epoch: 74, batch: 64, loss: 1.3965867757797241\n",
      "epoch: 74, batch: 65, loss: 1.6946674585342407\n",
      "epoch: 74, batch: 66, loss: 1.8080205917358398\n",
      "epoch: 74, batch: 67, loss: 1.4210903644561768\n",
      "epoch: 74, batch: 68, loss: 1.4570506811141968\n",
      "epoch: 74, batch: 69, loss: 1.3368881940841675\n",
      "epoch: 74, batch: 70, loss: 1.694726586341858\n",
      "epoch: 74, batch: 71, loss: 1.4570661783218384\n",
      "epoch: 74, batch: 72, loss: 1.6173194646835327\n",
      "epoch: 74, batch: 73, loss: 1.457075834274292\n",
      "epoch: 74, batch: 74, loss: 1.4501783847808838\n",
      "epoch: 74, batch: 75, loss: 1.6409565210342407\n",
      "epoch: 74, batch: 76, loss: 1.6947975158691406\n",
      "epoch: 74, batch: 77, loss: 1.8387010097503662\n",
      "epoch: 74, batch: 78, loss: 1.588454008102417\n",
      "epoch: 74, batch: 79, loss: 1.7915071249008179\n",
      "epoch: 74, batch: 80, loss: 1.8262455463409424\n",
      "epoch: 74, batch: 81, loss: 1.6103410720825195\n",
      "epoch: 74, batch: 82, loss: 1.3437960147857666\n",
      "epoch: 74, batch: 83, loss: 1.36741042137146\n",
      "epoch: 74, batch: 84, loss: 1.4807285070419312\n",
      "epoch: 74, batch: 85, loss: 1.5704371929168701\n",
      "epoch: 74, batch: 86, loss: 1.235884428024292\n",
      "epoch: 74, batch: 87, loss: 1.3960800170898438\n",
      "epoch: 74, batch: 88, loss: 1.6949374675750732\n",
      "epoch: 74, batch: 89, loss: 1.235826849937439\n",
      "epoch: 74, batch: 90, loss: 1.4969322681427002\n",
      "epoch: 74, batch: 91, loss: 1.367436408996582\n",
      "epoch: 74, batch: 92, loss: 1.6030824184417725\n",
      "epoch: 74, batch: 93, loss: 1.7021408081054688\n",
      "epoch: 74, batch: 94, loss: 1.5348052978515625\n",
      "epoch: 74, batch: 95, loss: 1.176483392715454\n",
      "epoch: 74, batch: 96, loss: 1.5348321199417114\n",
      "epoch: 74, batch: 97, loss: 1.403062105178833\n",
      "epoch: 74, batch: 98, loss: 1.3366601467132568\n",
      "epoch: 74, batch: 99, loss: 1.6950654983520508\n",
      "epoch: 74, batch: 100, loss: 1.4499353170394897\n",
      "epoch: 74, batch: 101, loss: 1.1692068576812744\n",
      "epoch: 74, batch: 102, loss: 1.581812858581543\n",
      "epoch: 74, batch: 103, loss: 1.289719820022583\n",
      "epoch: 74, batch: 104, loss: 1.6409488916397095\n",
      "epoch: 74, batch: 105, loss: 1.734750747680664\n",
      "epoch: 74, batch: 106, loss: 1.2896959781646729\n",
      "epoch: 74, batch: 107, loss: 1.6173595190048218\n",
      "epoch: 74, batch: 108, loss: 1.1221802234649658\n",
      "epoch: 74, batch: 109, loss: 1.5162277221679688\n",
      "epoch: 74, batch: 110, loss: 1.1221444606781006\n",
      "epoch: 74, batch: 111, loss: 1.4571864604949951\n",
      "epoch: 74, batch: 112, loss: 1.5350428819656372\n",
      "epoch: 74, batch: 113, loss: 1.3365412950515747\n",
      "epoch: 74, batch: 114, loss: 1.594050645828247\n",
      "epoch: 74, batch: 115, loss: 1.7495534420013428\n",
      "epoch: 74, batch: 116, loss: 1.8628448247909546\n",
      "epoch: 74, batch: 117, loss: 1.1763404607772827\n",
      "epoch: 74, batch: 118, loss: 1.2895995378494263\n",
      "epoch: 74, batch: 119, loss: 1.2821475267410278\n",
      "epoch: 74, batch: 120, loss: 1.4807889461517334\n",
      "epoch: 74, batch: 121, loss: 1.5555355548858643\n",
      "epoch: 74, batch: 122, loss: 1.734762191772461\n",
      "epoch: 74, batch: 123, loss: 1.948460340499878\n",
      "epoch: 74, batch: 124, loss: 1.4497110843658447\n",
      "epoch: 74, batch: 125, loss: 1.8630402088165283\n",
      "epoch: 74, batch: 126, loss: 1.1687580347061157\n",
      "epoch: 74, batch: 127, loss: 1.6574864387512207\n",
      "epoch: 75, batch: 0, loss: 1.5746128559112549\n",
      "epoch: 75, batch: 1, loss: 1.582166314125061\n",
      "epoch: 75, batch: 2, loss: 1.609811782836914\n",
      "epoch: 75, batch: 3, loss: 1.3363969326019287\n",
      "epoch: 75, batch: 4, loss: 1.3951460123062134\n",
      "epoch: 75, batch: 5, loss: 1.6718772649765015\n",
      "epoch: 75, batch: 6, loss: 1.4027111530303955\n",
      "epoch: 75, batch: 7, loss: 1.3675693273544312\n",
      "epoch: 75, batch: 8, loss: 1.3363560438156128\n",
      "epoch: 75, batch: 9, loss: 1.7229812145233154\n",
      "epoch: 75, batch: 10, loss: 1.4964878559112549\n",
      "epoch: 75, batch: 11, loss: 1.5353888273239136\n",
      "epoch: 75, batch: 12, loss: 1.2817449569702148\n",
      "epoch: 75, batch: 13, loss: 1.2348194122314453\n",
      "epoch: 75, batch: 14, loss: 1.1215633153915405\n",
      "epoch: 75, batch: 15, loss: 1.781489372253418\n",
      "epoch: 75, batch: 16, loss: 1.5823595523834229\n",
      "epoch: 75, batch: 17, loss: 1.5823723077774048\n",
      "epoch: 75, batch: 18, loss: 1.5080957412719727\n",
      "epoch: 75, batch: 19, loss: 1.5277440547943115\n",
      "epoch: 75, batch: 20, loss: 1.2815914154052734\n",
      "epoch: 75, batch: 21, loss: 1.7739232778549194\n",
      "epoch: 75, batch: 22, loss: 1.3676151037216187\n",
      "epoch: 75, batch: 23, loss: 1.5277513265609741\n",
      "epoch: 75, batch: 24, loss: 1.2815135717391968\n",
      "epoch: 75, batch: 25, loss: 1.4416253566741943\n",
      "epoch: 75, batch: 26, loss: 1.6173889636993408\n",
      "epoch: 75, batch: 27, loss: 1.5903393030166626\n",
      "epoch: 75, batch: 28, loss: 1.6095412969589233\n",
      "epoch: 75, batch: 29, loss: 1.664297342300415\n",
      "epoch: 75, batch: 30, loss: 1.6642986536026\n",
      "epoch: 75, batch: 31, loss: 1.648869514465332\n",
      "epoch: 75, batch: 32, loss: 1.4962794780731201\n",
      "epoch: 75, batch: 33, loss: 1.4493672847747803\n",
      "epoch: 75, batch: 34, loss: 1.5625767707824707\n",
      "epoch: 75, batch: 35, loss: 1.422483205795288\n",
      "epoch: 75, batch: 36, loss: 1.6174026727676392\n",
      "epoch: 75, batch: 37, loss: 1.628862738609314\n",
      "epoch: 75, batch: 38, loss: 1.7274080514907837\n",
      "epoch: 75, batch: 39, loss: 1.3360971212387085\n",
      "epoch: 75, batch: 40, loss: 1.441311240196228\n",
      "epoch: 75, batch: 41, loss: 1.5277776718139648\n",
      "epoch: 75, batch: 42, loss: 1.2811596393585205\n",
      "epoch: 75, batch: 43, loss: 1.8720760345458984\n",
      "epoch: 75, batch: 44, loss: 1.5827162265777588\n",
      "epoch: 75, batch: 45, loss: 1.167891263961792\n",
      "epoch: 75, batch: 46, loss: 1.582741618156433\n",
      "epoch: 75, batch: 47, loss: 1.2341680526733398\n",
      "epoch: 75, batch: 48, loss: 1.2341493368148804\n",
      "epoch: 75, batch: 49, loss: 1.5358864068984985\n",
      "epoch: 75, batch: 50, loss: 1.6409991979599\n",
      "epoch: 75, batch: 51, loss: 1.3941901922225952\n",
      "epoch: 75, batch: 52, loss: 1.8878319263458252\n",
      "epoch: 75, batch: 53, loss: 1.4809072017669678\n",
      "epoch: 75, batch: 54, loss: 1.6174190044403076\n",
      "epoch: 75, batch: 55, loss: 1.4491633176803589\n",
      "epoch: 75, batch: 56, loss: 1.3359525203704834\n",
      "epoch: 75, batch: 57, loss: 1.5623456239700317\n",
      "epoch: 75, batch: 58, loss: 1.504224181175232\n",
      "epoch: 75, batch: 59, loss: 1.5154374837875366\n",
      "epoch: 75, batch: 60, loss: 1.7939592599868774\n",
      "epoch: 75, batch: 61, loss: 1.6174278259277344\n",
      "epoch: 75, batch: 62, loss: 1.3441493511199951\n",
      "epoch: 75, batch: 63, loss: 1.449089765548706\n",
      "epoch: 75, batch: 64, loss: 1.5071192979812622\n",
      "epoch: 75, batch: 65, loss: 1.5016498565673828\n",
      "epoch: 75, batch: 66, loss: 1.2338066101074219\n",
      "epoch: 75, batch: 67, loss: 1.3677442073822021\n",
      "epoch: 75, batch: 68, loss: 1.6174319982528687\n",
      "epoch: 75, batch: 69, loss: 1.2337497472763062\n",
      "epoch: 75, batch: 70, loss: 1.2889506816864014\n",
      "epoch: 75, batch: 71, loss: 1.4573599100112915\n",
      "epoch: 75, batch: 72, loss: 1.5153064727783203\n",
      "epoch: 75, batch: 73, loss: 1.8647081851959229\n",
      "epoch: 75, batch: 74, loss: 1.7542051076889038\n",
      "epoch: 75, batch: 75, loss: 1.7691179513931274\n",
      "epoch: 75, batch: 76, loss: 1.5362379550933838\n",
      "epoch: 75, batch: 77, loss: 1.6090278625488281\n",
      "epoch: 75, batch: 78, loss: 1.4489500522613525\n",
      "epoch: 75, batch: 79, loss: 1.2804481983184814\n",
      "epoch: 75, batch: 80, loss: 1.5278522968292236\n",
      "epoch: 75, batch: 81, loss: 1.414679765701294\n",
      "epoch: 75, batch: 82, loss: 1.5536315441131592\n",
      "epoch: 75, batch: 83, loss: 1.527860403060913\n",
      "epoch: 75, batch: 84, loss: 1.3935273885726929\n",
      "epoch: 75, batch: 85, loss: 1.515162706375122\n",
      "epoch: 75, batch: 86, loss: 1.4019827842712402\n",
      "epoch: 75, batch: 87, loss: 1.5917751789093018\n",
      "epoch: 75, batch: 88, loss: 1.2802814245224\n",
      "epoch: 75, batch: 89, loss: 1.3934271335601807\n",
      "epoch: 75, batch: 90, loss: 1.8650668859481812\n",
      "epoch: 75, batch: 91, loss: 1.839524507522583\n",
      "epoch: 75, batch: 92, loss: 1.536439299583435\n",
      "epoch: 75, batch: 93, loss: 1.3678290843963623\n",
      "epoch: 75, batch: 94, loss: 1.4957003593444824\n",
      "epoch: 75, batch: 95, loss: 1.1669957637786865\n",
      "epoch: 75, batch: 96, loss: 1.335629940032959\n",
      "epoch: 75, batch: 97, loss: 1.4018728733062744\n",
      "epoch: 75, batch: 98, loss: 1.448764681816101\n",
      "epoch: 75, batch: 99, loss: 1.5025229454040527\n",
      "epoch: 75, batch: 100, loss: 1.5042879581451416\n",
      "epoch: 75, batch: 101, loss: 1.809748888015747\n",
      "epoch: 75, batch: 102, loss: 1.5705299377441406\n",
      "epoch: 75, batch: 103, loss: 1.7435334920883179\n",
      "epoch: 75, batch: 104, loss: 1.6497313976287842\n",
      "epoch: 75, batch: 105, loss: 1.6730314493179321\n",
      "epoch: 75, batch: 106, loss: 1.4486911296844482\n",
      "epoch: 75, batch: 107, loss: 1.5279207229614258\n",
      "epoch: 75, batch: 108, loss: 1.840280294418335\n",
      "epoch: 75, batch: 109, loss: 1.7523353099822998\n",
      "epoch: 75, batch: 110, loss: 1.5835769176483154\n",
      "epoch: 75, batch: 111, loss: 1.6174335479736328\n",
      "epoch: 75, batch: 112, loss: 1.4486349821090698\n",
      "epoch: 75, batch: 113, loss: 1.6967451572418213\n",
      "epoch: 75, batch: 114, loss: 1.3929297924041748\n",
      "epoch: 75, batch: 115, loss: 1.6086502075195312\n",
      "epoch: 75, batch: 116, loss: 1.4398095607757568\n",
      "epoch: 75, batch: 117, loss: 1.7629972696304321\n",
      "epoch: 75, batch: 118, loss: 1.2885370254516602\n",
      "epoch: 75, batch: 119, loss: 1.8949617147445679\n",
      "epoch: 75, batch: 120, loss: 1.5279641151428223\n",
      "epoch: 75, batch: 121, loss: 1.2548094987869263\n",
      "epoch: 75, batch: 122, loss: 1.6499278545379639\n",
      "epoch: 75, batch: 123, loss: 1.335415005683899\n",
      "epoch: 75, batch: 124, loss: 1.448521614074707\n",
      "epoch: 75, batch: 125, loss: 1.7057489156723022\n",
      "epoch: 75, batch: 126, loss: 1.9216020107269287\n",
      "epoch: 75, batch: 127, loss: 1.6274645328521729\n",
      "epoch: 76, batch: 0, loss: 1.4148837327957153\n",
      "epoch: 76, batch: 1, loss: 1.3264663219451904\n",
      "epoch: 76, batch: 2, loss: 1.4953895807266235\n",
      "epoch: 76, batch: 3, loss: 1.5033645629882812\n",
      "epoch: 76, batch: 4, loss: 1.5927832126617432\n",
      "epoch: 76, batch: 5, loss: 1.2884056568145752\n",
      "epoch: 76, batch: 6, loss: 1.786367654800415\n",
      "epoch: 76, batch: 7, loss: 1.7304975986480713\n",
      "epoch: 76, batch: 8, loss: 1.439449667930603\n",
      "epoch: 76, batch: 9, loss: 1.7766525745391846\n",
      "epoch: 76, batch: 10, loss: 1.8480854034423828\n",
      "epoch: 76, batch: 11, loss: 1.4863227605819702\n",
      "epoch: 76, batch: 12, loss: 1.561466932296753\n",
      "epoch: 76, batch: 13, loss: 1.4393608570098877\n",
      "epoch: 76, batch: 14, loss: 1.8571007251739502\n",
      "epoch: 76, batch: 15, loss: 1.3262288570404053\n",
      "epoch: 76, batch: 16, loss: 1.6971004009246826\n",
      "epoch: 76, batch: 17, loss: 1.4483202695846558\n",
      "epoch: 76, batch: 18, loss: 1.1192348003387451\n",
      "epoch: 76, batch: 19, loss: 1.706183671951294\n",
      "epoch: 76, batch: 20, loss: 1.4482910633087158\n",
      "epoch: 76, batch: 21, loss: 1.6082948446273804\n",
      "epoch: 76, batch: 22, loss: 1.5144418478012085\n",
      "epoch: 76, batch: 23, loss: 1.4951823949813843\n",
      "epoch: 76, batch: 24, loss: 1.697190523147583\n",
      "epoch: 76, batch: 25, loss: 1.575010895729065\n",
      "epoch: 76, batch: 26, loss: 1.4013158082962036\n",
      "epoch: 76, batch: 27, loss: 1.688104271888733\n",
      "epoch: 76, batch: 28, loss: 1.5704290866851807\n",
      "epoch: 76, batch: 29, loss: 1.3351175785064697\n",
      "epoch: 76, batch: 30, loss: 1.7634302377700806\n",
      "epoch: 76, batch: 31, loss: 1.6081806421279907\n",
      "epoch: 76, batch: 32, loss: 1.697279691696167\n",
      "epoch: 76, batch: 33, loss: 1.344254732131958\n",
      "epoch: 76, batch: 34, loss: 1.6265103816986084\n",
      "epoch: 76, batch: 35, loss: 1.345139980316162\n",
      "epoch: 76, batch: 36, loss: 1.5704162120819092\n",
      "epoch: 76, batch: 37, loss: 1.5373501777648926\n",
      "epoch: 76, batch: 38, loss: 1.3350356817245483\n",
      "epoch: 76, batch: 39, loss: 1.3919780254364014\n",
      "epoch: 76, batch: 40, loss: 1.6412334442138672\n",
      "epoch: 76, batch: 41, loss: 1.617305040359497\n",
      "epoch: 76, batch: 42, loss: 1.5042245388031006\n",
      "epoch: 76, batch: 43, loss: 1.504221796989441\n",
      "epoch: 76, batch: 44, loss: 1.231920838356018\n",
      "epoch: 76, batch: 45, loss: 1.8666661977767944\n",
      "epoch: 76, batch: 46, loss: 1.4480358362197876\n",
      "epoch: 76, batch: 47, loss: 1.3918426036834717\n",
      "epoch: 76, batch: 48, loss: 1.7772413492202759\n",
      "epoch: 76, batch: 49, loss: 1.4151297807693481\n",
      "epoch: 76, batch: 50, loss: 1.7543436288833618\n",
      "epoch: 76, batch: 51, loss: 1.5844194889068604\n",
      "epoch: 76, batch: 52, loss: 1.947021484375\n",
      "epoch: 76, batch: 53, loss: 1.6172535419464111\n",
      "epoch: 76, batch: 54, loss: 1.5135231018066406\n",
      "epoch: 76, batch: 55, loss: 1.4948323965072632\n",
      "epoch: 76, batch: 56, loss: 1.537591576576233\n",
      "epoch: 76, batch: 57, loss: 1.391678810119629\n",
      "epoch: 76, batch: 58, loss: 1.5938684940338135\n",
      "epoch: 76, batch: 59, loss: 1.6506898403167725\n",
      "epoch: 76, batch: 60, loss: 1.4478942155838013\n",
      "epoch: 76, batch: 61, loss: 1.1748850345611572\n",
      "epoch: 76, batch: 62, loss: 1.4384803771972656\n",
      "epoch: 76, batch: 63, loss: 1.5703195333480835\n",
      "epoch: 76, batch: 64, loss: 1.5282858610153198\n",
      "epoch: 76, batch: 65, loss: 1.5135438442230225\n",
      "epoch: 76, batch: 66, loss: 1.4009557962417603\n",
      "epoch: 76, batch: 67, loss: 1.2315937280654907\n",
      "epoch: 76, batch: 68, loss: 1.4478117227554321\n",
      "epoch: 76, batch: 69, loss: 1.1653896570205688\n",
      "epoch: 76, batch: 70, loss: 1.2784268856048584\n",
      "epoch: 76, batch: 71, loss: 1.4946531057357788\n",
      "epoch: 76, batch: 72, loss: 1.7071609497070312\n",
      "epoch: 76, batch: 73, loss: 1.641392707824707\n",
      "epoch: 76, batch: 74, loss: 1.2783702611923218\n",
      "epoch: 76, batch: 75, loss: 1.3346928358078003\n",
      "epoch: 76, batch: 76, loss: 1.4477285146713257\n",
      "epoch: 76, batch: 77, loss: 1.3346736431121826\n",
      "epoch: 76, batch: 78, loss: 1.1652703285217285\n",
      "epoch: 76, batch: 79, loss: 1.174745798110962\n",
      "epoch: 76, batch: 80, loss: 1.400821566581726\n",
      "epoch: 76, batch: 81, loss: 1.8447039127349854\n",
      "epoch: 76, batch: 82, loss: 1.3346256017684937\n",
      "epoch: 76, batch: 83, loss: 1.5135377645492554\n",
      "epoch: 76, batch: 84, loss: 1.481557846069336\n",
      "epoch: 76, batch: 85, loss: 1.3685266971588135\n",
      "epoch: 76, batch: 86, loss: 1.4476244449615479\n",
      "epoch: 76, batch: 87, loss: 1.4815807342529297\n",
      "epoch: 76, batch: 88, loss: 1.7074127197265625\n",
      "epoch: 76, batch: 89, loss: 1.7882890701293945\n",
      "epoch: 76, batch: 90, loss: 1.6074764728546143\n",
      "epoch: 76, batch: 91, loss: 1.6753199100494385\n",
      "epoch: 76, batch: 92, loss: 1.6170072555541992\n",
      "epoch: 76, batch: 93, loss: 1.528482437133789\n",
      "epoch: 76, batch: 94, loss: 1.3249452114105225\n",
      "epoch: 76, batch: 95, loss: 1.6510965824127197\n",
      "epoch: 76, batch: 96, loss: 1.2876336574554443\n",
      "epoch: 76, batch: 97, loss: 1.8014284372329712\n",
      "epoch: 76, batch: 98, loss: 1.3344690799713135\n",
      "epoch: 76, batch: 99, loss: 1.3910505771636963\n",
      "epoch: 76, batch: 100, loss: 1.5285323858261108\n",
      "epoch: 76, batch: 101, loss: 1.6511627435684204\n",
      "epoch: 76, batch: 102, loss: 1.2779786586761475\n",
      "epoch: 76, batch: 103, loss: 1.2779648303985596\n",
      "epoch: 76, batch: 104, loss: 1.3909785747528076\n",
      "epoch: 76, batch: 105, loss: 1.7545217275619507\n",
      "epoch: 76, batch: 106, loss: 1.4251649379730225\n",
      "epoch: 76, batch: 107, loss: 1.6980794668197632\n",
      "epoch: 76, batch: 108, loss: 1.3440696001052856\n",
      "epoch: 76, batch: 109, loss: 1.9488155841827393\n",
      "epoch: 76, batch: 110, loss: 1.9488471746444702\n",
      "epoch: 76, batch: 111, loss: 1.7546123266220093\n",
      "epoch: 76, batch: 112, loss: 1.651283621788025\n",
      "epoch: 76, batch: 113, loss: 1.616891622543335\n",
      "epoch: 76, batch: 114, loss: 1.503838300704956\n",
      "epoch: 76, batch: 115, loss: 1.230947494506836\n",
      "epoch: 76, batch: 116, loss: 2.096541166305542\n",
      "epoch: 76, batch: 117, loss: 1.4818017482757568\n",
      "epoch: 76, batch: 118, loss: 1.6416763067245483\n",
      "epoch: 76, batch: 119, loss: 1.1647224426269531\n",
      "epoch: 76, batch: 120, loss: 1.513485074043274\n",
      "epoch: 76, batch: 121, loss: 1.569913625717163\n",
      "epoch: 76, batch: 122, loss: 1.5755343437194824\n",
      "epoch: 76, batch: 123, loss: 1.3342283964157104\n",
      "epoch: 76, batch: 124, loss: 1.506850004196167\n",
      "epoch: 76, batch: 125, loss: 1.5505332946777344\n",
      "epoch: 76, batch: 126, loss: 1.3688509464263916\n",
      "epoch: 76, batch: 127, loss: 1.6285167932510376\n",
      "epoch: 77, batch: 0, loss: 1.415716528892517\n",
      "epoch: 77, batch: 1, loss: 1.8429158926010132\n",
      "epoch: 77, batch: 2, loss: 1.5070253610610962\n",
      "epoch: 77, batch: 3, loss: 1.6070219278335571\n",
      "epoch: 77, batch: 4, loss: 1.5601592063903809\n",
      "epoch: 77, batch: 5, loss: 1.4002901315689087\n",
      "epoch: 77, batch: 6, loss: 1.5287688970565796\n",
      "epoch: 77, batch: 7, loss: 1.4842283725738525\n",
      "epoch: 77, batch: 8, loss: 1.5287830829620361\n",
      "epoch: 77, batch: 9, loss: 1.616712212562561\n",
      "epoch: 77, batch: 10, loss: 1.5287971496582031\n",
      "epoch: 77, batch: 11, loss: 1.868039846420288\n",
      "epoch: 77, batch: 12, loss: 1.6733133792877197\n",
      "epoch: 77, batch: 13, loss: 1.7082149982452393\n",
      "epoch: 77, batch: 14, loss: 1.1741987466812134\n",
      "epoch: 77, batch: 15, loss: 1.7082443237304688\n",
      "epoch: 77, batch: 16, loss: 1.6068817377090454\n",
      "epoch: 77, batch: 17, loss: 1.456522822380066\n",
      "epoch: 77, batch: 18, loss: 1.1741634607315063\n",
      "epoch: 77, batch: 19, loss: 1.5036476850509644\n",
      "epoch: 77, batch: 20, loss: 1.3339941501617432\n",
      "epoch: 77, batch: 21, loss: 1.425685167312622\n",
      "epoch: 77, batch: 22, loss: 1.7083427906036377\n",
      "epoch: 77, batch: 23, loss: 1.446958303451538\n",
      "epoch: 77, batch: 24, loss: 1.3902833461761475\n",
      "epoch: 77, batch: 25, loss: 1.4469373226165771\n",
      "epoch: 77, batch: 26, loss: 1.5387346744537354\n",
      "epoch: 77, batch: 27, loss: 1.5032345056533813\n",
      "epoch: 77, batch: 28, loss: 1.3437378406524658\n",
      "epoch: 77, batch: 29, loss: 1.5133981704711914\n",
      "epoch: 77, batch: 30, loss: 1.2303615808486938\n",
      "epoch: 77, batch: 31, loss: 1.6419483423233032\n",
      "epoch: 77, batch: 32, loss: 1.446864366531372\n",
      "epoch: 77, batch: 33, loss: 1.5951099395751953\n",
      "epoch: 77, batch: 34, loss: 1.776361107826233\n",
      "epoch: 77, batch: 35, loss: 1.3338466882705688\n",
      "epoch: 77, batch: 36, loss: 1.5856974124908447\n",
      "epoch: 77, batch: 37, loss: 1.6419970989227295\n",
      "epoch: 77, batch: 38, loss: 1.3999450206756592\n",
      "epoch: 77, batch: 39, loss: 1.7554136514663696\n",
      "epoch: 77, batch: 40, loss: 1.776304841041565\n",
      "epoch: 77, batch: 41, loss: 1.286929726600647\n",
      "epoch: 77, batch: 42, loss: 1.3692177534103394\n",
      "epoch: 77, batch: 43, loss: 1.44674813747406\n",
      "epoch: 77, batch: 44, loss: 1.5857841968536377\n",
      "epoch: 77, batch: 45, loss: 1.436875581741333\n",
      "epoch: 77, batch: 46, loss: 1.7456443309783936\n",
      "epoch: 77, batch: 47, loss: 1.164035439491272\n",
      "epoch: 77, batch: 48, loss: 1.5596729516983032\n",
      "epoch: 77, batch: 49, loss: 1.7653872966766357\n",
      "epoch: 77, batch: 50, loss: 1.436816930770874\n",
      "epoch: 77, batch: 51, loss: 1.3435444831848145\n",
      "epoch: 77, batch: 52, loss: 1.512766718864441\n",
      "epoch: 77, batch: 53, loss: 1.576021432876587\n",
      "epoch: 77, batch: 54, loss: 1.7194417715072632\n",
      "epoch: 77, batch: 55, loss: 1.4162050485610962\n",
      "epoch: 77, batch: 56, loss: 1.389885663986206\n",
      "epoch: 77, batch: 57, loss: 1.286763310432434\n",
      "epoch: 77, batch: 58, loss: 1.436724305152893\n",
      "epoch: 77, batch: 59, loss: 1.3997150659561157\n",
      "epoch: 77, batch: 60, loss: 1.662644624710083\n",
      "epoch: 77, batch: 61, loss: 1.689070463180542\n",
      "epoch: 77, batch: 62, loss: 1.8477569818496704\n",
      "epoch: 77, batch: 63, loss: 1.4564024209976196\n",
      "epoch: 77, batch: 64, loss: 1.286691427230835\n",
      "epoch: 77, batch: 65, loss: 1.6219078302383423\n",
      "epoch: 77, batch: 66, loss: 1.3694534301757812\n",
      "epoch: 77, batch: 67, loss: 1.173693060874939\n",
      "epoch: 77, batch: 68, loss: 1.3694732189178467\n",
      "epoch: 77, batch: 69, loss: 1.549560546875\n",
      "epoch: 77, batch: 70, loss: 1.4824583530426025\n",
      "epoch: 77, batch: 71, loss: 1.4163658618927002\n",
      "epoch: 77, batch: 72, loss: 1.2767295837402344\n",
      "epoch: 77, batch: 73, loss: 1.6990585327148438\n",
      "epoch: 77, batch: 74, loss: 1.5031646490097046\n",
      "epoch: 77, batch: 75, loss: 1.6728684902191162\n",
      "epoch: 77, batch: 76, loss: 1.4263006448745728\n",
      "epoch: 77, batch: 77, loss: 1.616100549697876\n",
      "epoch: 77, batch: 78, loss: 1.3695675134658813\n",
      "epoch: 77, batch: 79, loss: 1.6892287731170654\n",
      "epoch: 77, batch: 80, loss: 1.7361046075820923\n",
      "epoch: 77, batch: 81, loss: 1.7553379535675049\n",
      "epoch: 77, batch: 82, loss: 1.4932000637054443\n",
      "epoch: 77, batch: 83, loss: 1.6530145406723022\n",
      "epoch: 77, batch: 84, loss: 1.1636302471160889\n",
      "epoch: 77, batch: 85, loss: 1.8391069173812866\n",
      "epoch: 77, batch: 86, loss: 1.4462907314300537\n",
      "epoch: 77, batch: 87, loss: 1.3696507215499878\n",
      "epoch: 77, batch: 88, loss: 1.9990243911743164\n",
      "epoch: 77, batch: 89, loss: 1.5024360418319702\n",
      "epoch: 77, batch: 90, loss: 1.5763673782348633\n",
      "epoch: 77, batch: 91, loss: 1.4165550470352173\n",
      "epoch: 77, batch: 92, loss: 1.333274245262146\n",
      "epoch: 77, batch: 93, loss: 1.6893447637557983\n",
      "epoch: 77, batch: 94, loss: 1.7560632228851318\n",
      "epoch: 77, batch: 95, loss: 1.6727294921875\n",
      "epoch: 77, batch: 96, loss: 1.7757469415664673\n",
      "epoch: 77, batch: 97, loss: 1.6820104122161865\n",
      "epoch: 77, batch: 98, loss: 1.4461647272109985\n",
      "epoch: 77, batch: 99, loss: 1.569042444229126\n",
      "epoch: 77, batch: 100, loss: 1.5395243167877197\n",
      "epoch: 77, batch: 101, loss: 1.699346899986267\n",
      "epoch: 77, batch: 102, loss: 1.5029257535934448\n",
      "epoch: 77, batch: 103, loss: 1.389306664466858\n",
      "epoch: 77, batch: 104, loss: 1.3424376249313354\n",
      "epoch: 77, batch: 105, loss: 1.3331397771835327\n",
      "epoch: 77, batch: 106, loss: 1.1165152788162231\n",
      "epoch: 77, batch: 107, loss: 1.5296472311019897\n",
      "epoch: 77, batch: 108, loss: 2.0095877647399902\n",
      "epoch: 77, batch: 109, loss: 1.1164836883544922\n",
      "epoch: 77, batch: 110, loss: 1.7087223529815674\n",
      "epoch: 77, batch: 111, loss: 1.5864982604980469\n",
      "epoch: 77, batch: 112, loss: 1.568930983543396\n",
      "epoch: 77, batch: 113, loss: 1.9260501861572266\n",
      "epoch: 77, batch: 114, loss: 1.2861979007720947\n",
      "epoch: 77, batch: 115, loss: 1.8592827320098877\n",
      "epoch: 77, batch: 116, loss: 1.6234607696533203\n",
      "epoch: 77, batch: 117, loss: 1.3991167545318604\n",
      "epoch: 77, batch: 118, loss: 1.2861590385437012\n",
      "epoch: 77, batch: 119, loss: 1.426789402961731\n",
      "epoch: 77, batch: 120, loss: 1.3422638177871704\n",
      "epoch: 77, batch: 121, loss: 1.4459187984466553\n",
      "epoch: 77, batch: 122, loss: 1.4459081888198853\n",
      "epoch: 77, batch: 123, loss: 1.5959012508392334\n",
      "epoch: 77, batch: 124, loss: 1.5119916200637817\n",
      "epoch: 77, batch: 125, loss: 1.642760992050171\n",
      "epoch: 77, batch: 126, loss: 1.2760931253433228\n",
      "epoch: 77, batch: 127, loss: 1.461725115776062\n",
      "epoch: 78, batch: 0, loss: 1.5687768459320068\n",
      "epoch: 78, batch: 1, loss: 1.6996272802352905\n",
      "epoch: 78, batch: 2, loss: 1.5119259357452393\n",
      "epoch: 78, batch: 3, loss: 1.2760378122329712\n",
      "epoch: 78, batch: 4, loss: 1.3889684677124023\n",
      "epoch: 78, batch: 5, loss: 1.849871039390564\n",
      "epoch: 78, batch: 6, loss: 1.370121717453003\n",
      "epoch: 78, batch: 7, loss: 1.3228342533111572\n",
      "epoch: 78, batch: 8, loss: 1.3989174365997314\n",
      "epoch: 78, batch: 9, loss: 1.7284634113311768\n",
      "epoch: 78, batch: 10, loss: 1.3328008651733398\n",
      "epoch: 78, batch: 11, loss: 1.3327913284301758\n",
      "epoch: 78, batch: 12, loss: 1.6897380352020264\n",
      "epoch: 78, batch: 13, loss: 1.9351218938827515\n",
      "epoch: 78, batch: 14, loss: 1.7097622156143188\n",
      "epoch: 78, batch: 15, loss: 1.7466157674789429\n",
      "epoch: 78, batch: 16, loss: 1.8695627450942993\n",
      "epoch: 78, batch: 17, loss: 1.0031728744506836\n",
      "epoch: 78, batch: 18, loss: 1.7852230072021484\n",
      "epoch: 78, batch: 19, loss: 1.3887864351272583\n",
      "epoch: 78, batch: 20, loss: 1.4271161556243896\n",
      "epoch: 78, batch: 21, loss: 1.4556318521499634\n",
      "epoch: 78, batch: 22, loss: 1.6998409032821655\n",
      "epoch: 78, batch: 23, loss: 1.2573628425598145\n",
      "epoch: 78, batch: 24, loss: 1.5300735235214233\n",
      "epoch: 78, batch: 25, loss: 1.6430057287216187\n",
      "epoch: 78, batch: 26, loss: 1.4832381010055542\n",
      "epoch: 78, batch: 27, loss: 1.6815265417099\n",
      "epoch: 78, batch: 28, loss: 1.6253505945205688\n",
      "epoch: 78, batch: 29, loss: 1.285762071609497\n",
      "epoch: 78, batch: 30, loss: 1.5769909620285034\n",
      "epoch: 78, batch: 31, loss: 1.7376972436904907\n",
      "epoch: 78, batch: 32, loss: 1.3225774765014648\n",
      "epoch: 78, batch: 33, loss: 1.7568273544311523\n",
      "epoch: 78, batch: 34, loss: 1.7099759578704834\n",
      "epoch: 78, batch: 35, loss: 1.1159073114395142\n",
      "epoch: 78, batch: 36, loss: 1.6052424907684326\n",
      "epoch: 78, batch: 37, loss: 1.6531273126602173\n",
      "epoch: 78, batch: 38, loss: 1.3416974544525146\n",
      "epoch: 78, batch: 39, loss: 1.502314805984497\n",
      "epoch: 78, batch: 40, loss: 1.4554363489151\n",
      "epoch: 78, batch: 41, loss: 1.5583215951919556\n",
      "epoch: 78, batch: 42, loss: 1.6900310516357422\n",
      "epoch: 78, batch: 43, loss: 1.9067054986953735\n",
      "epoch: 78, batch: 44, loss: 1.6620460748672485\n",
      "epoch: 78, batch: 45, loss: 1.172684669494629\n",
      "epoch: 78, batch: 46, loss: 1.7180458307266235\n",
      "epoch: 78, batch: 47, loss: 1.492226004600525\n",
      "epoch: 78, batch: 48, loss: 1.823016881942749\n",
      "epoch: 78, batch: 49, loss: 1.6611251831054688\n",
      "epoch: 78, batch: 50, loss: 1.842028260231018\n",
      "epoch: 78, batch: 51, loss: 1.5482008457183838\n",
      "epoch: 78, batch: 52, loss: 1.5125209093093872\n",
      "epoch: 78, batch: 53, loss: 1.2286162376403809\n",
      "epoch: 78, batch: 54, loss: 1.7001636028289795\n",
      "epoch: 78, batch: 55, loss: 1.9081989526748657\n",
      "epoch: 78, batch: 56, loss: 1.2854726314544678\n",
      "epoch: 78, batch: 57, loss: 1.5021321773529053\n",
      "epoch: 78, batch: 58, loss: 1.6710231304168701\n",
      "epoch: 78, batch: 59, loss: 1.6150054931640625\n",
      "epoch: 78, batch: 60, loss: 1.2854301929473877\n",
      "epoch: 78, batch: 61, loss: 1.2285302877426147\n",
      "epoch: 78, batch: 62, loss: 1.1156281232833862\n",
      "epoch: 78, batch: 63, loss: 1.7855455875396729\n",
      "epoch: 78, batch: 64, loss: 1.34138822555542\n",
      "epoch: 78, batch: 65, loss: 1.7002719640731812\n",
      "epoch: 78, batch: 66, loss: 2.002309560775757\n",
      "epoch: 78, batch: 67, loss: 1.285355806350708\n",
      "epoch: 78, batch: 68, loss: 1.6149015426635742\n",
      "epoch: 78, batch: 69, loss: 1.5965250730514526\n",
      "epoch: 78, batch: 70, loss: 1.2284334897994995\n",
      "epoch: 78, batch: 71, loss: 1.5010862350463867\n",
      "epoch: 78, batch: 72, loss: 1.7672220468521118\n",
      "epoch: 78, batch: 73, loss: 1.115520715713501\n",
      "epoch: 78, batch: 74, loss: 1.7746139764785767\n",
      "epoch: 78, batch: 75, loss: 1.4177240133285522\n",
      "epoch: 78, batch: 76, loss: 1.727685570716858\n",
      "epoch: 78, batch: 77, loss: 1.322170615196228\n",
      "epoch: 78, batch: 78, loss: 1.1154714822769165\n",
      "epoch: 78, batch: 79, loss: 1.1623718738555908\n",
      "epoch: 78, batch: 80, loss: 1.4277658462524414\n",
      "epoch: 78, batch: 81, loss: 1.700429916381836\n",
      "epoch: 78, batch: 82, loss: 1.4540380239486694\n",
      "epoch: 78, batch: 83, loss: 1.5018608570098877\n",
      "epoch: 78, batch: 84, loss: 1.61471688747406\n",
      "epoch: 78, batch: 85, loss: 1.6137813329696655\n",
      "epoch: 78, batch: 86, loss: 1.7474006414413452\n",
      "epoch: 78, batch: 87, loss: 1.597599744796753\n",
      "epoch: 78, batch: 88, loss: 1.1153720617294312\n",
      "epoch: 78, batch: 89, loss: 1.275147795677185\n",
      "epoch: 78, batch: 90, loss: 1.5017892122268677\n",
      "epoch: 78, batch: 91, loss: 1.7005293369293213\n",
      "epoch: 78, batch: 92, loss: 1.5577278137207031\n",
      "epoch: 78, batch: 93, loss: 1.3419710397720337\n",
      "epoch: 78, batch: 94, loss: 1.2751004695892334\n",
      "epoch: 78, batch: 95, loss: 1.2581802606582642\n",
      "epoch: 78, batch: 96, loss: 1.7274277210235596\n",
      "epoch: 78, batch: 97, loss: 1.5946495532989502\n",
      "epoch: 78, batch: 98, loss: 1.3978674411773682\n",
      "epoch: 78, batch: 99, loss: 1.5016944408416748\n",
      "epoch: 78, batch: 100, loss: 1.6515209674835205\n",
      "epoch: 78, batch: 101, loss: 1.9272661209106445\n",
      "epoch: 78, batch: 102, loss: 1.4180552959442139\n",
      "epoch: 78, batch: 103, loss: 1.6536989212036133\n",
      "epoch: 78, batch: 104, loss: 1.115216612815857\n",
      "epoch: 78, batch: 105, loss: 1.3219505548477173\n",
      "epoch: 78, batch: 106, loss: 1.7173526287078857\n",
      "epoch: 78, batch: 107, loss: 1.3878178596496582\n",
      "epoch: 78, batch: 108, loss: 1.5945557355880737\n",
      "epoch: 78, batch: 109, loss: 1.6272027492523193\n",
      "epoch: 78, batch: 110, loss: 1.4546129703521729\n",
      "epoch: 78, batch: 111, loss: 1.7870590686798096\n",
      "epoch: 78, batch: 112, loss: 1.418181300163269\n",
      "epoch: 78, batch: 113, loss: 1.5144997835159302\n",
      "epoch: 78, batch: 114, loss: 1.7576453685760498\n",
      "epoch: 78, batch: 115, loss: 1.8605579137802124\n",
      "epoch: 78, batch: 116, loss: 1.6712204217910767\n",
      "epoch: 78, batch: 117, loss: 1.1150962114334106\n",
      "epoch: 78, batch: 118, loss: 1.6043860912322998\n",
      "epoch: 78, batch: 119, loss: 1.717199683189392\n",
      "epoch: 78, batch: 120, loss: 1.1620380878448486\n",
      "epoch: 78, batch: 121, loss: 1.3317382335662842\n",
      "epoch: 78, batch: 122, loss: 1.4445502758026123\n",
      "epoch: 78, batch: 123, loss: 1.3975681066513062\n",
      "epoch: 78, batch: 124, loss: 1.1620049476623535\n",
      "epoch: 78, batch: 125, loss: 1.2278430461883545\n",
      "epoch: 78, batch: 126, loss: 1.5013891458511353\n",
      "epoch: 78, batch: 127, loss: 1.8879399299621582\n",
      "epoch: 79, batch: 0, loss: 1.4282840490341187\n",
      "epoch: 79, batch: 1, loss: 1.8606923818588257\n",
      "epoch: 79, batch: 2, loss: 1.8805080652236938\n",
      "epoch: 79, batch: 3, loss: 1.588106393814087\n",
      "epoch: 79, batch: 4, loss: 1.6141315698623657\n",
      "epoch: 79, batch: 5, loss: 1.604220986366272\n",
      "epoch: 79, batch: 6, loss: 1.6700451374053955\n",
      "epoch: 79, batch: 7, loss: 1.2747260332107544\n",
      "epoch: 79, batch: 8, loss: 1.4843103885650635\n",
      "epoch: 79, batch: 9, loss: 1.4843218326568604\n",
      "epoch: 79, batch: 10, loss: 1.588178038597107\n",
      "epoch: 79, batch: 11, loss: 1.4443649053573608\n",
      "epoch: 79, batch: 12, loss: 1.1618746519088745\n",
      "epoch: 79, batch: 13, loss: 1.8237043619155884\n",
      "epoch: 79, batch: 14, loss: 1.651100516319275\n",
      "epoch: 79, batch: 15, loss: 1.3315143585205078\n",
      "epoch: 79, batch: 16, loss: 1.6911728382110596\n",
      "epoch: 79, batch: 17, loss: 1.6708080768585205\n",
      "epoch: 79, batch: 18, loss: 1.434420108795166\n",
      "epoch: 79, batch: 19, loss: 1.5570809841156006\n",
      "epoch: 79, batch: 20, loss: 1.3972829580307007\n",
      "epoch: 79, batch: 21, loss: 1.6912320852279663\n",
      "epoch: 79, batch: 22, loss: 1.4442436695098877\n",
      "epoch: 79, batch: 23, loss: 1.644272804260254\n",
      "epoch: 79, batch: 24, loss: 1.418682336807251\n",
      "epoch: 79, batch: 25, loss: 1.4343593120574951\n",
      "epoch: 79, batch: 26, loss: 1.4187082052230835\n",
      "epoch: 79, batch: 27, loss: 1.9111270904541016\n",
      "epoch: 79, batch: 28, loss: 1.9276107549667358\n",
      "epoch: 79, batch: 29, loss: 1.7579923868179321\n",
      "epoch: 79, batch: 30, loss: 1.541396975517273\n",
      "epoch: 79, batch: 31, loss: 1.588389277458191\n",
      "epoch: 79, batch: 32, loss: 1.4286208152770996\n",
      "epoch: 79, batch: 33, loss: 1.259020447731018\n",
      "epoch: 79, batch: 34, loss: 1.6038868427276611\n",
      "epoch: 79, batch: 35, loss: 1.2843226194381714\n",
      "epoch: 79, batch: 36, loss: 1.6136894226074219\n",
      "epoch: 79, batch: 37, loss: 1.1616833209991455\n",
      "epoch: 79, batch: 38, loss: 1.4846757650375366\n",
      "epoch: 79, batch: 39, loss: 1.434234619140625\n",
      "epoch: 79, batch: 40, loss: 1.7110872268676758\n",
      "epoch: 79, batch: 41, loss: 1.603803277015686\n",
      "epoch: 79, batch: 42, loss: 1.3312278985977173\n",
      "epoch: 79, batch: 43, loss: 1.0018671751022339\n",
      "epoch: 79, batch: 44, loss: 1.7580921649932861\n",
      "epoch: 79, batch: 45, loss: 1.8610851764678955\n",
      "epoch: 79, batch: 46, loss: 1.5983365774154663\n",
      "epoch: 79, batch: 47, loss: 1.5885435342788696\n",
      "epoch: 79, batch: 48, loss: 1.4439531564712524\n",
      "epoch: 79, batch: 49, loss: 1.5317937135696411\n",
      "epoch: 79, batch: 50, loss: 1.3401920795440674\n",
      "epoch: 79, batch: 51, loss: 1.3311281204223633\n",
      "epoch: 79, batch: 52, loss: 1.7013814449310303\n",
      "epoch: 79, batch: 53, loss: 1.5664722919464111\n",
      "epoch: 79, batch: 54, loss: 1.3969156742095947\n",
      "epoch: 79, batch: 55, loss: 1.531869888305664\n",
      "epoch: 79, batch: 56, loss: 1.4536384344100952\n",
      "epoch: 79, batch: 57, loss: 1.6297118663787842\n",
      "epoch: 79, batch: 58, loss: 1.7014366388320923\n",
      "epoch: 79, batch: 59, loss: 1.7014459371566772\n",
      "epoch: 79, batch: 60, loss: 1.2840607166290283\n",
      "epoch: 79, batch: 61, loss: 1.3870817422866821\n",
      "epoch: 79, batch: 62, loss: 1.4289252758026123\n",
      "epoch: 79, batch: 63, loss: 1.274274468421936\n",
      "epoch: 79, batch: 64, loss: 1.7897052764892578\n",
      "epoch: 79, batch: 65, loss: 1.5004713535308838\n",
      "epoch: 79, batch: 66, loss: 1.3212134838104248\n",
      "epoch: 79, batch: 67, loss: 1.7112607955932617\n",
      "epoch: 79, batch: 68, loss: 1.5095598697662354\n",
      "epoch: 79, batch: 69, loss: 1.598479986190796\n",
      "epoch: 79, batch: 70, loss: 1.0016766786575317\n",
      "epoch: 79, batch: 71, loss: 1.7387874126434326\n",
      "epoch: 79, batch: 72, loss: 1.5467513799667358\n",
      "epoch: 79, batch: 73, loss: 1.8466854095458984\n",
      "epoch: 79, batch: 74, loss: 1.7259225845336914\n",
      "epoch: 79, batch: 75, loss: 1.259584903717041\n",
      "epoch: 79, batch: 76, loss: 1.8241081237792969\n",
      "epoch: 79, batch: 77, loss: 1.4852001667022705\n",
      "epoch: 79, batch: 78, loss: 1.691917061805725\n",
      "epoch: 79, batch: 79, loss: 1.3869493007659912\n",
      "epoch: 79, batch: 80, loss: 1.3399889469146729\n",
      "epoch: 79, batch: 81, loss: 1.6546967029571533\n",
      "epoch: 79, batch: 82, loss: 1.4532614946365356\n",
      "epoch: 79, batch: 83, loss: 1.3307634592056274\n",
      "epoch: 79, batch: 84, loss: 1.5563440322875977\n",
      "epoch: 79, batch: 85, loss: 1.6547355651855469\n",
      "epoch: 79, batch: 86, loss: 1.274101734161377\n",
      "epoch: 79, batch: 87, loss: 1.8470993041992188\n",
      "epoch: 79, batch: 88, loss: 1.6690962314605713\n",
      "epoch: 79, batch: 89, loss: 1.5093393325805664\n",
      "epoch: 79, batch: 90, loss: 1.9503204822540283\n",
      "epoch: 79, batch: 91, loss: 1.3306739330291748\n",
      "epoch: 79, batch: 92, loss: 1.8472480773925781\n",
      "epoch: 79, batch: 93, loss: 1.2740490436553955\n",
      "epoch: 79, batch: 94, loss: 1.1612454652786255\n",
      "epoch: 79, batch: 95, loss: 1.556221604347229\n",
      "epoch: 79, batch: 96, loss: 1.320974588394165\n",
      "epoch: 79, batch: 97, loss: 1.0014790296554565\n",
      "epoch: 79, batch: 98, loss: 1.1708531379699707\n",
      "epoch: 79, batch: 99, loss: 1.7018171548843384\n",
      "epoch: 79, batch: 100, loss: 1.4433703422546387\n",
      "epoch: 79, batch: 101, loss: 1.5934734344482422\n",
      "epoch: 79, batch: 102, loss: 1.5657696723937988\n",
      "epoch: 79, batch: 103, loss: 1.5324891805648804\n",
      "epoch: 79, batch: 104, loss: 1.1142216920852661\n",
      "epoch: 79, batch: 105, loss: 1.3305199146270752\n",
      "epoch: 79, batch: 106, loss: 1.5934312343597412\n",
      "epoch: 79, batch: 107, loss: 1.4529054164886475\n",
      "epoch: 79, batch: 108, loss: 1.598716139793396\n",
      "epoch: 79, batch: 109, loss: 1.114182710647583\n",
      "epoch: 79, batch: 110, loss: 1.6787586212158203\n",
      "epoch: 79, batch: 111, loss: 1.711235761642456\n",
      "epoch: 79, batch: 112, loss: 1.2834984064102173\n",
      "epoch: 79, batch: 113, loss: 1.6029669046401978\n",
      "epoch: 79, batch: 114, loss: 1.7489023208618164\n",
      "epoch: 79, batch: 115, loss: 1.4994678497314453\n",
      "epoch: 79, batch: 116, loss: 1.5422389507293701\n",
      "epoch: 79, batch: 117, loss: 1.3866560459136963\n",
      "epoch: 79, batch: 118, loss: 1.3962299823760986\n",
      "epoch: 79, batch: 119, loss: 1.7115848064422607\n",
      "epoch: 79, batch: 120, loss: 1.6218013763427734\n",
      "epoch: 79, batch: 121, loss: 1.838273286819458\n",
      "epoch: 79, batch: 122, loss: 1.5089828968048096\n",
      "epoch: 79, batch: 123, loss: 1.386609673500061\n",
      "epoch: 79, batch: 124, loss: 1.2738057374954224\n",
      "epoch: 79, batch: 125, loss: 1.555890679359436\n",
      "epoch: 79, batch: 126, loss: 1.5892764329910278\n",
      "epoch: 79, batch: 127, loss: 1.55352783203125\n",
      "epoch: 80, batch: 0, loss: 1.7020912170410156\n",
      "epoch: 80, batch: 1, loss: 1.3207111358642578\n",
      "epoch: 80, batch: 2, loss: 1.612316370010376\n",
      "epoch: 80, batch: 3, loss: 1.6783158779144287\n",
      "epoch: 80, batch: 4, loss: 1.78155517578125\n",
      "epoch: 80, batch: 5, loss: 1.1609464883804321\n",
      "epoch: 80, batch: 6, loss: 1.273733377456665\n",
      "epoch: 80, batch: 7, loss: 1.3731632232666016\n",
      "epoch: 80, batch: 8, loss: 1.2832348346710205\n",
      "epoch: 80, batch: 9, loss: 1.3301678895950317\n",
      "epoch: 80, batch: 10, loss: 1.4296550750732422\n",
      "epoch: 80, batch: 11, loss: 1.3959929943084717\n",
      "epoch: 80, batch: 12, loss: 1.5799146890640259\n",
      "epoch: 80, batch: 13, loss: 1.2736871242523193\n",
      "epoch: 80, batch: 14, loss: 1.4429038763046265\n",
      "epoch: 80, batch: 15, loss: 1.8055393695831299\n",
      "epoch: 80, batch: 16, loss: 1.4992492198944092\n",
      "epoch: 80, batch: 17, loss: 1.4202489852905273\n",
      "epoch: 80, batch: 18, loss: 1.7117249965667725\n",
      "epoch: 80, batch: 19, loss: 1.4333847761154175\n",
      "epoch: 80, batch: 20, loss: 1.3958890438079834\n",
      "epoch: 80, batch: 21, loss: 1.420305609703064\n",
      "epoch: 80, batch: 22, loss: 1.3864173889160156\n",
      "epoch: 80, batch: 23, loss: 1.2736213207244873\n",
      "epoch: 80, batch: 24, loss: 1.3300018310546875\n",
      "epoch: 80, batch: 25, loss: 1.862059235572815\n",
      "epoch: 80, batch: 26, loss: 1.8021587133407593\n",
      "epoch: 80, batch: 27, loss: 1.5331780910491943\n",
      "epoch: 80, batch: 28, loss: 1.5331916809082031\n",
      "epoch: 80, batch: 29, loss: 1.5555227994918823\n",
      "epoch: 80, batch: 30, loss: 1.6024599075317383\n",
      "epoch: 80, batch: 31, loss: 1.5990712642669678\n",
      "epoch: 80, batch: 32, loss: 1.633196234703064\n",
      "epoch: 80, batch: 33, loss: 1.6460460424423218\n",
      "epoch: 80, batch: 34, loss: 1.2735440731048584\n",
      "epoch: 80, batch: 35, loss: 1.4520630836486816\n",
      "epoch: 80, batch: 36, loss: 1.533298373222351\n",
      "epoch: 80, batch: 37, loss: 1.8715591430664062\n",
      "epoch: 80, batch: 38, loss: 1.486375093460083\n",
      "epoch: 80, batch: 39, loss: 1.5896687507629395\n",
      "epoch: 80, batch: 40, loss: 1.5333505868911743\n",
      "epoch: 80, batch: 41, loss: 1.749423623085022\n",
      "epoch: 80, batch: 42, loss: 1.6116969585418701\n",
      "epoch: 80, batch: 43, loss: 1.5647319555282593\n",
      "epoch: 80, batch: 44, loss: 1.7933355569839478\n",
      "epoch: 80, batch: 45, loss: 1.815294861793518\n",
      "epoch: 80, batch: 46, loss: 1.7401134967803955\n",
      "epoch: 80, batch: 47, loss: 1.9062182903289795\n",
      "epoch: 80, batch: 48, loss: 1.5553066730499268\n",
      "epoch: 80, batch: 49, loss: 1.4300340414047241\n",
      "epoch: 80, batch: 50, loss: 1.3955504894256592\n",
      "epoch: 80, batch: 51, loss: 1.4424915313720703\n",
      "epoch: 80, batch: 52, loss: 1.564601182937622\n",
      "epoch: 80, batch: 53, loss: 1.2264477014541626\n",
      "epoch: 80, batch: 54, loss: 1.1606132984161377\n",
      "epoch: 80, batch: 55, loss: 1.5645577907562256\n",
      "epoch: 80, batch: 56, loss: 1.486602544784546\n",
      "epoch: 80, batch: 57, loss: 1.5805222988128662\n",
      "epoch: 80, batch: 58, loss: 1.5458695888519287\n",
      "epoch: 80, batch: 59, loss: 1.6677274703979492\n",
      "epoch: 80, batch: 60, loss: 1.580561637878418\n",
      "epoch: 80, batch: 61, loss: 1.714887022972107\n",
      "epoch: 80, batch: 62, loss: 1.5992085933685303\n",
      "epoch: 80, batch: 63, loss: 1.498870849609375\n",
      "epoch: 80, batch: 64, loss: 1.338881254196167\n",
      "epoch: 80, batch: 65, loss: 1.6585814952850342\n",
      "epoch: 80, batch: 66, loss: 1.6676095724105835\n",
      "epoch: 80, batch: 67, loss: 1.4867393970489502\n",
      "epoch: 80, batch: 68, loss: 1.4423058032989502\n",
      "epoch: 80, batch: 69, loss: 1.0007824897766113\n",
      "epoch: 80, batch: 70, loss: 1.2732709646224976\n",
      "epoch: 80, batch: 71, loss: 1.508082389831543\n",
      "epoch: 80, batch: 72, loss: 1.7333152294158936\n",
      "epoch: 80, batch: 73, loss: 1.6558170318603516\n",
      "epoch: 80, batch: 74, loss: 1.5995925664901733\n",
      "epoch: 80, batch: 75, loss: 1.5337998867034912\n",
      "epoch: 80, batch: 76, loss: 1.2732268571853638\n",
      "epoch: 80, batch: 77, loss: 1.4329429864883423\n",
      "epoch: 80, batch: 78, loss: 1.3294349908828735\n",
      "epoch: 80, batch: 79, loss: 1.5456883907318115\n",
      "epoch: 80, batch: 80, loss: 1.5431194305419922\n",
      "epoch: 80, batch: 81, loss: 1.3294051885604858\n",
      "epoch: 80, batch: 82, loss: 1.4514007568359375\n",
      "epoch: 80, batch: 83, loss: 1.4303923845291138\n",
      "epoch: 80, batch: 84, loss: 1.3293755054473877\n",
      "epoch: 80, batch: 85, loss: 1.3293659687042236\n",
      "epoch: 80, batch: 86, loss: 1.6350116729736328\n",
      "epoch: 80, batch: 87, loss: 1.4869873523712158\n",
      "epoch: 80, batch: 88, loss: 1.7029178142547607\n",
      "epoch: 80, batch: 89, loss: 1.2731302976608276\n",
      "epoch: 80, batch: 90, loss: 1.489036202430725\n",
      "epoch: 80, batch: 91, loss: 1.7499182224273682\n",
      "epoch: 80, batch: 92, loss: 1.702955961227417\n",
      "epoch: 80, batch: 93, loss: 1.702965497970581\n",
      "epoch: 80, batch: 94, loss: 1.5074154138565063\n",
      "epoch: 80, batch: 95, loss: 1.6353130340576172\n",
      "epoch: 80, batch: 96, loss: 1.667101263999939\n",
      "epoch: 80, batch: 97, loss: 1.3384464979171753\n",
      "epoch: 80, batch: 98, loss: 1.712209939956665\n",
      "epoch: 80, batch: 99, loss: 1.4985389709472656\n",
      "epoch: 80, batch: 100, loss: 1.487146258354187\n",
      "epoch: 80, batch: 101, loss: 1.6560633182525635\n",
      "epoch: 80, batch: 102, loss: 1.3383774757385254\n",
      "epoch: 80, batch: 103, loss: 1.385765790939331\n",
      "epoch: 80, batch: 104, loss: 1.3857581615447998\n",
      "epoch: 80, batch: 105, loss: 1.8515198230743408\n",
      "epoch: 80, batch: 106, loss: 1.7704938650131226\n",
      "epoch: 80, batch: 107, loss: 1.3857351541519165\n",
      "epoch: 80, batch: 108, loss: 1.8174476623535156\n",
      "epoch: 80, batch: 109, loss: 1.545438528060913\n",
      "epoch: 80, batch: 110, loss: 1.6485509872436523\n",
      "epoch: 80, batch: 111, loss: 1.48728609085083\n",
      "epoch: 80, batch: 112, loss: 1.3745673894882202\n",
      "epoch: 80, batch: 113, loss: 1.2729588747024536\n",
      "epoch: 80, batch: 114, loss: 1.3948148488998413\n",
      "epoch: 80, batch: 115, loss: 1.7142391204833984\n",
      "epoch: 80, batch: 116, loss: 1.4417808055877686\n",
      "epoch: 80, batch: 117, loss: 1.6667194366455078\n",
      "epoch: 80, batch: 118, loss: 1.5904744863510132\n",
      "epoch: 80, batch: 119, loss: 1.656219482421875\n",
      "epoch: 80, batch: 120, loss: 1.666663408279419\n",
      "epoch: 80, batch: 121, loss: 1.4307889938354492\n",
      "epoch: 80, batch: 122, loss: 1.5996159315109253\n",
      "epoch: 80, batch: 123, loss: 1.169265627861023\n",
      "epoch: 80, batch: 124, loss: 1.4308198690414429\n",
      "epoch: 80, batch: 125, loss: 1.703265905380249\n",
      "epoch: 80, batch: 126, loss: 1.450753092765808\n",
      "epoch: 80, batch: 127, loss: 1.2114235162734985\n",
      "epoch: 81, batch: 0, loss: 1.703294038772583\n",
      "epoch: 81, batch: 1, loss: 1.272854208946228\n",
      "epoch: 81, batch: 2, loss: 1.1601309776306152\n",
      "epoch: 81, batch: 3, loss: 1.4416143894195557\n",
      "epoch: 81, batch: 4, loss: 1.5906140804290771\n",
      "epoch: 81, batch: 5, loss: 1.6670230627059937\n",
      "epoch: 81, batch: 6, loss: 1.488579511642456\n",
      "epoch: 81, batch: 7, loss: 1.6563583612442017\n",
      "epoch: 81, batch: 8, loss: 1.6563670635223389\n",
      "epoch: 81, batch: 9, loss: 1.3385206460952759\n",
      "epoch: 81, batch: 10, loss: 1.4309581518173218\n",
      "epoch: 81, batch: 11, loss: 1.545222282409668\n",
      "epoch: 81, batch: 12, loss: 1.4415134191513062\n",
      "epoch: 81, batch: 13, loss: 1.3377941846847534\n",
      "epoch: 81, batch: 14, loss: 1.769129991531372\n",
      "epoch: 81, batch: 15, loss: 1.4884828329086304\n",
      "epoch: 81, batch: 16, loss: 1.5347392559051514\n",
      "epoch: 81, batch: 17, loss: 1.8631633520126343\n",
      "epoch: 81, batch: 18, loss: 1.7034581899642944\n",
      "epoch: 81, batch: 19, loss: 1.6011489629745483\n",
      "epoch: 81, batch: 20, loss: 1.807220697402954\n",
      "epoch: 81, batch: 21, loss: 1.5541194677352905\n",
      "epoch: 81, batch: 22, loss: 1.2727339267730713\n",
      "epoch: 81, batch: 23, loss: 1.1600219011306763\n",
      "epoch: 81, batch: 24, loss: 1.6659835577011108\n",
      "epoch: 81, batch: 25, loss: 1.3286616802215576\n",
      "epoch: 81, batch: 26, loss: 1.5997565984725952\n",
      "epoch: 81, batch: 27, loss: 1.965735673904419\n",
      "epoch: 81, batch: 28, loss: 1.5247151851654053\n",
      "epoch: 81, batch: 29, loss: 1.6476341485977173\n",
      "epoch: 81, batch: 30, loss: 1.769258737564087\n",
      "epoch: 81, batch: 31, loss: 1.7594897747039795\n",
      "epoch: 81, batch: 32, loss: 1.1688737869262695\n",
      "epoch: 81, batch: 33, loss: 1.6947033405303955\n",
      "epoch: 81, batch: 34, loss: 1.9748268127441406\n",
      "epoch: 81, batch: 35, loss: 1.656599998474121\n",
      "epoch: 81, batch: 36, loss: 1.6566085815429688\n",
      "epoch: 81, batch: 37, loss: 1.4880372285842896\n",
      "epoch: 81, batch: 38, loss: 1.535064935684204\n",
      "epoch: 81, batch: 39, loss: 1.6009206771850586\n",
      "epoch: 81, batch: 40, loss: 1.693708062171936\n",
      "epoch: 81, batch: 41, loss: 1.1129186153411865\n",
      "epoch: 81, batch: 42, loss: 1.4323363304138184\n",
      "epoch: 81, batch: 43, loss: 1.5998215675354004\n",
      "epoch: 81, batch: 44, loss: 1.2726142406463623\n",
      "epoch: 81, batch: 45, loss: 1.488158106803894\n",
      "epoch: 81, batch: 46, loss: 1.3754734992980957\n",
      "epoch: 81, batch: 47, loss: 1.9104944467544556\n",
      "epoch: 81, batch: 48, loss: 1.5998404026031494\n",
      "epoch: 81, batch: 49, loss: 1.8164310455322266\n",
      "epoch: 81, batch: 50, loss: 1.5625699758529663\n",
      "epoch: 81, batch: 51, loss: 1.7509212493896484\n",
      "epoch: 81, batch: 52, loss: 1.5067341327667236\n",
      "epoch: 81, batch: 53, loss: 1.4968477487564087\n",
      "epoch: 81, batch: 54, loss: 1.4968311786651611\n",
      "epoch: 81, batch: 55, loss: 1.2725551128387451\n",
      "epoch: 81, batch: 56, loss: 1.4410184621810913\n",
      "epoch: 81, batch: 57, loss: 1.5449448823928833\n",
      "epoch: 81, batch: 58, loss: 1.6477324962615967\n",
      "epoch: 81, batch: 59, loss: 1.440985083580017\n",
      "epoch: 81, batch: 60, loss: 1.7508633136749268\n",
      "epoch: 81, batch: 61, loss: 1.4322359561920166\n",
      "epoch: 81, batch: 62, loss: 1.5536372661590576\n",
      "epoch: 81, batch: 63, loss: 1.5998950004577637\n",
      "epoch: 81, batch: 64, loss: 1.1598231792449951\n",
      "epoch: 81, batch: 65, loss: 1.5623054504394531\n",
      "epoch: 81, batch: 66, loss: 1.4884536266326904\n",
      "epoch: 81, batch: 67, loss: 1.8165836334228516\n",
      "epoch: 81, batch: 68, loss: 1.6649973392486572\n",
      "epoch: 81, batch: 69, loss: 1.4315251111984253\n",
      "epoch: 81, batch: 70, loss: 1.4878923892974854\n",
      "epoch: 81, batch: 71, loss: 1.440850853919983\n",
      "epoch: 81, batch: 72, loss: 1.4408397674560547\n",
      "epoch: 81, batch: 73, loss: 1.7509857416152954\n",
      "epoch: 81, batch: 74, loss: 1.7775377035140991\n",
      "epoch: 81, batch: 75, loss: 1.5442615747451782\n",
      "epoch: 81, batch: 76, loss: 1.2810842990875244\n",
      "epoch: 81, batch: 77, loss: 1.431601881980896\n",
      "epoch: 81, batch: 78, loss: 1.63918137550354\n",
      "epoch: 81, batch: 79, loss: 1.4230029582977295\n",
      "epoch: 81, batch: 80, loss: 1.5534255504608154\n",
      "epoch: 81, batch: 81, loss: 1.2810287475585938\n",
      "epoch: 81, batch: 82, loss: 1.2724192142486572\n",
      "epoch: 81, batch: 83, loss: 1.8076610565185547\n",
      "epoch: 81, batch: 84, loss: 1.2809951305389404\n",
      "epoch: 81, batch: 85, loss: 1.8461748361587524\n",
      "epoch: 81, batch: 86, loss: 1.449252724647522\n",
      "epoch: 81, batch: 87, loss: 1.168286919593811\n",
      "epoch: 81, batch: 88, loss: 1.6570520401000977\n",
      "epoch: 81, batch: 89, loss: 1.5914217233657837\n",
      "epoch: 81, batch: 90, loss: 1.4876707792282104\n",
      "epoch: 81, batch: 91, loss: 1.5447603464126587\n",
      "epoch: 81, batch: 92, loss: 1.6088523864746094\n",
      "epoch: 81, batch: 93, loss: 1.7682138681411743\n",
      "epoch: 81, batch: 94, loss: 1.6571037769317627\n",
      "epoch: 81, batch: 95, loss: 1.3379974365234375\n",
      "epoch: 81, batch: 96, loss: 1.3935315608978271\n",
      "epoch: 81, batch: 97, loss: 1.2723519802093506\n",
      "epoch: 81, batch: 98, loss: 1.2723476886749268\n",
      "epoch: 81, batch: 99, loss: 1.4320528507232666\n",
      "epoch: 81, batch: 100, loss: 1.3850114345550537\n",
      "epoch: 81, batch: 101, loss: 1.1596627235412598\n",
      "epoch: 81, batch: 102, loss: 1.3934615850448608\n",
      "epoch: 81, batch: 103, loss: 1.3849977254867554\n",
      "epoch: 81, batch: 104, loss: 1.1126110553741455\n",
      "epoch: 81, batch: 105, loss: 1.1680835485458374\n",
      "epoch: 81, batch: 106, loss: 1.393414855003357\n",
      "epoch: 81, batch: 107, loss: 1.4404432773590088\n",
      "epoch: 81, batch: 108, loss: 1.4790557622909546\n",
      "epoch: 81, batch: 109, loss: 1.5445606708526611\n",
      "epoch: 81, batch: 110, loss: 1.1680266857147217\n",
      "epoch: 81, batch: 111, loss: 1.5445778369903564\n",
      "epoch: 81, batch: 112, loss: 1.4958199262619019\n",
      "epoch: 81, batch: 113, loss: 1.3849495649337769\n",
      "epoch: 81, batch: 114, loss: 1.4403645992279053\n",
      "epoch: 81, batch: 115, loss: 1.7765220403671265\n",
      "epoch: 81, batch: 116, loss: 1.4403424263000488\n",
      "epoch: 81, batch: 117, loss: 2.017256498336792\n",
      "epoch: 81, batch: 118, loss: 1.3849246501922607\n",
      "epoch: 81, batch: 119, loss: 1.696032166481018\n",
      "epoch: 81, batch: 120, loss: 1.8234504461288452\n",
      "epoch: 81, batch: 121, loss: 1.272240400314331\n",
      "epoch: 81, batch: 122, loss: 1.544623613357544\n",
      "epoch: 81, batch: 123, loss: 1.4402639865875244\n",
      "epoch: 81, batch: 124, loss: 1.6573588848114014\n",
      "epoch: 81, batch: 125, loss: 1.544610619544983\n",
      "epoch: 81, batch: 126, loss: 1.448523759841919\n",
      "epoch: 81, batch: 127, loss: 1.7499703168869019\n",
      "epoch: 82, batch: 0, loss: 1.4237757921218872\n",
      "epoch: 82, batch: 1, loss: 1.3275282382965088\n",
      "epoch: 82, batch: 2, loss: 1.2721972465515137\n",
      "epoch: 82, batch: 3, loss: 1.6491594314575195\n",
      "epoch: 82, batch: 4, loss: 1.3767831325531006\n",
      "epoch: 82, batch: 5, loss: 1.4483975172042847\n",
      "epoch: 82, batch: 6, loss: 1.2251217365264893\n",
      "epoch: 82, batch: 7, loss: 1.7207543849945068\n",
      "epoch: 82, batch: 8, loss: 1.2803934812545776\n",
      "epoch: 82, batch: 9, loss: 1.3930494785308838\n",
      "epoch: 82, batch: 10, loss: 1.5998222827911377\n",
      "epoch: 82, batch: 11, loss: 1.7701542377471924\n",
      "epoch: 82, batch: 12, loss: 1.3930139541625977\n",
      "epoch: 82, batch: 13, loss: 1.2721433639526367\n",
      "epoch: 82, batch: 14, loss: 1.3192001581192017\n",
      "epoch: 82, batch: 15, loss: 1.2721338272094727\n",
      "epoch: 82, batch: 16, loss: 1.6885887384414673\n",
      "epoch: 82, batch: 17, loss: 1.5448719263076782\n",
      "epoch: 82, batch: 18, loss: 1.4870719909667969\n",
      "epoch: 82, batch: 19, loss: 1.2802654504776\n",
      "epoch: 82, batch: 20, loss: 1.4481298923492432\n",
      "epoch: 82, batch: 21, loss: 1.6001125574111938\n",
      "epoch: 82, batch: 22, loss: 1.7046476602554321\n",
      "epoch: 82, batch: 23, loss: 1.3272879123687744\n",
      "epoch: 82, batch: 24, loss: 1.7127845287322998\n",
      "epoch: 82, batch: 25, loss: 1.5449411869049072\n",
      "epoch: 82, batch: 26, loss: 1.6077579259872437\n",
      "epoch: 82, batch: 27, loss: 1.4399070739746094\n",
      "epoch: 82, batch: 28, loss: 1.5606495141983032\n",
      "epoch: 82, batch: 29, loss: 1.7047125101089478\n",
      "epoch: 82, batch: 30, loss: 1.2249829769134521\n",
      "epoch: 82, batch: 31, loss: 1.495009183883667\n",
      "epoch: 82, batch: 32, loss: 1.2720496654510498\n",
      "epoch: 82, batch: 33, loss: 1.3847064971923828\n",
      "epoch: 82, batch: 34, loss: 1.7598861455917358\n",
      "epoch: 82, batch: 35, loss: 1.6025657653808594\n",
      "epoch: 82, batch: 36, loss: 1.3927243947982788\n",
      "epoch: 82, batch: 37, loss: 1.9116138219833374\n",
      "epoch: 82, batch: 38, loss: 1.3846826553344727\n",
      "epoch: 82, batch: 39, loss: 1.7128143310546875\n",
      "epoch: 82, batch: 40, loss: 1.6026378870010376\n",
      "epoch: 82, batch: 41, loss: 1.3773335218429565\n",
      "epoch: 82, batch: 42, loss: 1.5921732187271118\n",
      "epoch: 82, batch: 43, loss: 1.5298173427581787\n",
      "epoch: 82, batch: 44, loss: 1.6001636981964111\n",
      "epoch: 82, batch: 45, loss: 1.1593328714370728\n",
      "epoch: 82, batch: 46, loss: 1.5523532629013062\n",
      "epoch: 82, batch: 47, loss: 1.6897132396697998\n",
      "epoch: 82, batch: 48, loss: 1.3925755023956299\n",
      "epoch: 82, batch: 49, loss: 1.5602519512176514\n",
      "epoch: 82, batch: 50, loss: 1.5451489686965942\n",
      "epoch: 82, batch: 51, loss: 1.5451571941375732\n",
      "epoch: 82, batch: 52, loss: 1.1121995449066162\n",
      "epoch: 82, batch: 53, loss: 1.7599414587020874\n",
      "epoch: 82, batch: 54, loss: 1.719918966293335\n",
      "epoch: 82, batch: 55, loss: 1.5451889038085938\n",
      "epoch: 82, batch: 56, loss: 1.4395849704742432\n",
      "epoch: 82, batch: 57, loss: 1.8647388219833374\n",
      "epoch: 82, batch: 58, loss: 1.697119951248169\n",
      "epoch: 82, batch: 59, loss: 1.6029056310653687\n",
      "epoch: 82, batch: 60, loss: 1.584501028060913\n",
      "epoch: 82, batch: 61, loss: 1.3190395832061768\n",
      "epoch: 82, batch: 62, loss: 1.3346939086914062\n",
      "epoch: 82, batch: 63, loss: 1.592374563217163\n",
      "epoch: 82, batch: 64, loss: 1.537447214126587\n",
      "epoch: 82, batch: 65, loss: 1.3268331289291382\n",
      "epoch: 82, batch: 66, loss: 1.6619760990142822\n",
      "epoch: 82, batch: 67, loss: 1.7128530740737915\n",
      "epoch: 82, batch: 68, loss: 1.3268009424209595\n",
      "epoch: 82, batch: 69, loss: 1.3845367431640625\n",
      "epoch: 82, batch: 70, loss: 1.712856650352478\n",
      "epoch: 82, batch: 71, loss: 1.5443156957626343\n",
      "epoch: 82, batch: 72, loss: 1.5991981029510498\n",
      "epoch: 82, batch: 73, loss: 1.3373796939849854\n",
      "epoch: 82, batch: 74, loss: 1.2247226238250732\n",
      "epoch: 82, batch: 75, loss: 1.9197981357574463\n",
      "epoch: 82, batch: 76, loss: 1.5924978256225586\n",
      "epoch: 82, batch: 77, loss: 1.6985151767730713\n",
      "epoch: 82, batch: 78, loss: 1.7051671743392944\n",
      "epoch: 82, batch: 79, loss: 1.6503379344940186\n",
      "epoch: 82, batch: 80, loss: 1.537703514099121\n",
      "epoch: 82, batch: 81, loss: 1.4864610433578491\n",
      "epoch: 82, batch: 82, loss: 1.6986567974090576\n",
      "epoch: 82, batch: 83, loss: 1.6539033651351929\n",
      "epoch: 82, batch: 84, loss: 1.6975734233856201\n",
      "epoch: 82, batch: 85, loss: 1.5377838611602783\n",
      "epoch: 82, batch: 86, loss: 1.8037869930267334\n",
      "epoch: 82, batch: 87, loss: 1.3844585418701172\n",
      "epoch: 82, batch: 88, loss: 1.5518823862075806\n",
      "epoch: 82, batch: 89, loss: 1.4468308687210083\n",
      "epoch: 82, batch: 90, loss: 1.5314687490463257\n",
      "epoch: 82, batch: 91, loss: 1.4863630533218384\n",
      "epoch: 82, batch: 92, loss: 1.6002334356307983\n",
      "epoch: 82, batch: 93, loss: 1.705306053161621\n",
      "epoch: 82, batch: 94, loss: 1.2793490886688232\n",
      "epoch: 82, batch: 95, loss: 1.8255380392074585\n",
      "epoch: 82, batch: 96, loss: 1.5593504905700684\n",
      "epoch: 82, batch: 97, loss: 1.7053426504135132\n",
      "epoch: 82, batch: 98, loss: 1.2793009281158447\n",
      "epoch: 82, batch: 99, loss: 1.2245841026306152\n",
      "epoch: 82, batch: 100, loss: 1.7525434494018555\n",
      "epoch: 82, batch: 101, loss: 1.6064307689666748\n",
      "epoch: 82, batch: 102, loss: 1.3189188241958618\n",
      "epoch: 82, batch: 103, loss: 1.6643686294555664\n",
      "epoch: 82, batch: 104, loss: 1.6535546779632568\n",
      "epoch: 82, batch: 105, loss: 1.4390451908111572\n",
      "epoch: 82, batch: 106, loss: 1.4390342235565186\n",
      "epoch: 82, batch: 107, loss: 1.2245423793792725\n",
      "epoch: 82, batch: 108, loss: 1.9172331094741821\n",
      "epoch: 82, batch: 109, loss: 1.493636131286621\n",
      "epoch: 82, batch: 110, loss: 1.3188945055007935\n",
      "epoch: 82, batch: 111, loss: 1.438978910446167\n",
      "epoch: 82, batch: 112, loss: 1.5913761854171753\n",
      "epoch: 82, batch: 113, loss: 1.8519502878189087\n",
      "epoch: 82, batch: 114, loss: 1.705498456954956\n",
      "epoch: 82, batch: 115, loss: 1.609644889831543\n",
      "epoch: 82, batch: 116, loss: 1.5515751838684082\n",
      "epoch: 82, batch: 117, loss: 1.5515642166137695\n",
      "epoch: 82, batch: 118, loss: 1.3843297958374023\n",
      "epoch: 82, batch: 119, loss: 1.391700029373169\n",
      "epoch: 82, batch: 120, loss: 1.4462465047836304\n",
      "epoch: 82, batch: 121, loss: 1.4388682842254639\n",
      "epoch: 82, batch: 122, loss: 1.4388574361801147\n",
      "epoch: 82, batch: 123, loss: 1.1590062379837036\n",
      "epoch: 82, batch: 124, loss: 1.39164137840271\n",
      "epoch: 82, batch: 125, loss: 1.7674534320831299\n",
      "epoch: 82, batch: 126, loss: 1.5913362503051758\n",
      "epoch: 82, batch: 127, loss: 1.1989517211914062\n",
      "epoch: 83, batch: 0, loss: 1.7637706995010376\n",
      "epoch: 83, batch: 1, loss: 1.9668470621109009\n",
      "epoch: 83, batch: 2, loss: 1.1662715673446655\n",
      "epoch: 83, batch: 3, loss: 1.8183025121688843\n",
      "epoch: 83, batch: 4, loss: 1.478667140007019\n",
      "epoch: 83, batch: 5, loss: 1.5586570501327515\n",
      "epoch: 83, batch: 6, loss: 1.6984151601791382\n",
      "epoch: 83, batch: 7, loss: 1.2788641452789307\n",
      "epoch: 83, batch: 8, loss: 1.3188035488128662\n",
      "epoch: 83, batch: 9, loss: 1.6057863235473633\n",
      "epoch: 83, batch: 10, loss: 1.4386807680130005\n",
      "epoch: 83, batch: 11, loss: 1.5985206365585327\n",
      "epoch: 83, batch: 12, loss: 1.8183822631835938\n",
      "epoch: 83, batch: 13, loss: 1.1117284297943115\n",
      "epoch: 83, batch: 14, loss: 1.2243707180023193\n",
      "epoch: 83, batch: 15, loss: 1.3259787559509277\n",
      "epoch: 83, batch: 16, loss: 1.8674776554107666\n",
      "epoch: 83, batch: 17, loss: 1.325957179069519\n",
      "epoch: 83, batch: 18, loss: 1.4385923147201538\n",
      "epoch: 83, batch: 19, loss: 1.538770318031311\n",
      "epoch: 83, batch: 20, loss: 1.2715485095977783\n",
      "epoch: 83, batch: 21, loss: 1.2787069082260132\n",
      "epoch: 83, batch: 22, loss: 1.6986697912216187\n",
      "epoch: 83, batch: 23, loss: 1.4857444763183594\n",
      "epoch: 83, batch: 24, loss: 1.6126645803451538\n",
      "epoch: 83, batch: 25, loss: 1.224318265914917\n",
      "epoch: 83, batch: 26, loss: 1.6054866313934326\n",
      "epoch: 83, batch: 27, loss: 1.438493013381958\n",
      "epoch: 83, batch: 28, loss: 1.478573203086853\n",
      "epoch: 83, batch: 29, loss: 1.4384708404541016\n",
      "epoch: 83, batch: 30, loss: 1.5338828563690186\n",
      "epoch: 83, batch: 31, loss: 1.4856570959091187\n",
      "epoch: 83, batch: 32, loss: 1.379120111465454\n",
      "epoch: 83, batch: 33, loss: 1.5510715246200562\n",
      "epoch: 83, batch: 34, loss: 1.6053466796875\n",
      "epoch: 83, batch: 35, loss: 1.5982576608657837\n",
      "epoch: 83, batch: 36, loss: 1.325748324394226\n",
      "epoch: 83, batch: 37, loss: 1.4454413652420044\n",
      "epoch: 83, batch: 38, loss: 1.7010746002197266\n",
      "epoch: 83, batch: 39, loss: 1.5342047214508057\n",
      "epoch: 83, batch: 40, loss: 1.278496503829956\n",
      "epoch: 83, batch: 41, loss: 1.2784855365753174\n",
      "epoch: 83, batch: 42, loss: 1.5461561679840088\n",
      "epoch: 83, batch: 43, loss: 1.2784634828567505\n",
      "epoch: 83, batch: 44, loss: 1.8068825006484985\n",
      "epoch: 83, batch: 45, loss: 1.2714335918426514\n",
      "epoch: 83, batch: 46, loss: 1.651839256286621\n",
      "epoch: 83, batch: 47, loss: 1.4335534572601318\n",
      "epoch: 83, batch: 48, loss: 1.7060638666152954\n",
      "epoch: 83, batch: 49, loss: 1.7130542993545532\n",
      "epoch: 83, batch: 50, loss: 1.5392539501190186\n",
      "epoch: 83, batch: 51, loss: 1.7599292993545532\n",
      "epoch: 83, batch: 52, loss: 1.6163126230239868\n",
      "epoch: 83, batch: 53, loss: 1.3255550861358643\n",
      "epoch: 83, batch: 54, loss: 1.8187698125839233\n",
      "epoch: 83, batch: 55, loss: 1.8142181634902954\n",
      "epoch: 83, batch: 56, loss: 1.2783172130584717\n",
      "epoch: 83, batch: 57, loss: 1.6452213525772095\n",
      "epoch: 83, batch: 58, loss: 1.4381507635116577\n",
      "epoch: 83, batch: 59, loss: 1.6520462036132812\n",
      "epoch: 83, batch: 60, loss: 1.325475811958313\n",
      "epoch: 83, batch: 61, loss: 1.5394246578216553\n",
      "epoch: 83, batch: 62, loss: 1.3909025192260742\n",
      "epoch: 83, batch: 63, loss: 1.4853006601333618\n",
      "epoch: 83, batch: 64, loss: 1.6590073108673096\n",
      "epoch: 83, batch: 65, loss: 1.753426194190979\n",
      "epoch: 83, batch: 66, loss: 1.4921371936798096\n",
      "epoch: 83, batch: 67, loss: 1.706239938735962\n",
      "epoch: 83, batch: 68, loss: 1.224119782447815\n",
      "epoch: 83, batch: 69, loss: 1.4268925189971924\n",
      "epoch: 83, batch: 70, loss: 1.7062677145004272\n",
      "epoch: 83, batch: 71, loss: 1.278146743774414\n",
      "epoch: 83, batch: 72, loss: 1.5574842691421509\n",
      "epoch: 83, batch: 73, loss: 1.5464327335357666\n",
      "epoch: 83, batch: 74, loss: 1.5506337881088257\n",
      "epoch: 83, batch: 75, loss: 1.3907592296600342\n",
      "epoch: 83, batch: 76, loss: 1.3184934854507446\n",
      "epoch: 83, batch: 77, loss: 1.7063329219818115\n",
      "epoch: 83, batch: 78, loss: 1.6564610004425049\n",
      "epoch: 83, batch: 79, loss: 1.3798364400863647\n",
      "epoch: 83, batch: 80, loss: 1.383927583694458\n",
      "epoch: 83, batch: 81, loss: 1.7063701152801514\n",
      "epoch: 83, batch: 82, loss: 1.4965786933898926\n",
      "epoch: 83, batch: 83, loss: 1.658468246459961\n",
      "epoch: 83, batch: 84, loss: 1.4311176538467407\n",
      "epoch: 83, batch: 85, loss: 1.5937471389770508\n",
      "epoch: 83, batch: 86, loss: 1.3251855373382568\n",
      "epoch: 83, batch: 87, loss: 1.5504963397979736\n",
      "epoch: 83, batch: 88, loss: 1.5504857301712036\n",
      "epoch: 83, batch: 89, loss: 1.3318707942962646\n",
      "epoch: 83, batch: 90, loss: 1.5870814323425293\n",
      "epoch: 83, batch: 91, loss: 1.8191251754760742\n",
      "epoch: 83, batch: 92, loss: 1.3838715553283691\n",
      "epoch: 83, batch: 93, loss: 1.7064822912216187\n",
      "epoch: 83, batch: 94, loss: 1.6582168340682983\n",
      "epoch: 83, batch: 95, loss: 1.1585335731506348\n",
      "epoch: 83, batch: 96, loss: 1.390526533126831\n",
      "epoch: 83, batch: 97, loss: 1.71318781375885\n",
      "epoch: 83, batch: 98, loss: 1.638140082359314\n",
      "epoch: 83, batch: 99, loss: 1.7065385580062866\n",
      "epoch: 83, batch: 100, loss: 1.336621642112732\n",
      "epoch: 83, batch: 101, loss: 1.9754302501678467\n",
      "epoch: 83, batch: 102, loss: 1.427392601966858\n",
      "epoch: 83, batch: 103, loss: 1.39044988155365\n",
      "epoch: 83, batch: 104, loss: 1.5467102527618408\n",
      "epoch: 83, batch: 105, loss: 1.2711471319198608\n",
      "epoch: 83, batch: 106, loss: 1.8695271015167236\n",
      "epoch: 83, batch: 107, loss: 1.7167625427246094\n",
      "epoch: 83, batch: 108, loss: 1.3903956413269043\n",
      "epoch: 83, batch: 109, loss: 1.277721881866455\n",
      "epoch: 83, batch: 110, loss: 1.5467652082443237\n",
      "epoch: 83, batch: 111, loss: 1.7573261260986328\n",
      "epoch: 83, batch: 112, loss: 1.3903518915176392\n",
      "epoch: 83, batch: 113, loss: 1.4441226720809937\n",
      "epoch: 83, batch: 114, loss: 1.2776668071746826\n",
      "epoch: 83, batch: 115, loss: 1.556751012802124\n",
      "epoch: 83, batch: 116, loss: 1.3903075456619263\n",
      "epoch: 83, batch: 117, loss: 1.8160240650177002\n",
      "epoch: 83, batch: 118, loss: 1.4374992847442627\n",
      "epoch: 83, batch: 119, loss: 1.6038954257965088\n",
      "epoch: 83, batch: 120, loss: 1.7721805572509766\n",
      "epoch: 83, batch: 121, loss: 1.638045072555542\n",
      "epoch: 83, batch: 122, loss: 1.706751823425293\n",
      "epoch: 83, batch: 123, loss: 1.9663501977920532\n",
      "epoch: 83, batch: 124, loss: 1.2710602283477783\n",
      "epoch: 83, batch: 125, loss: 1.1648820638656616\n",
      "epoch: 83, batch: 126, loss: 1.3247512578964233\n",
      "epoch: 83, batch: 127, loss: 1.5471258163452148\n",
      "epoch: 84, batch: 0, loss: 1.8731544017791748\n",
      "epoch: 84, batch: 1, loss: 1.7604963779449463\n",
      "epoch: 84, batch: 2, loss: 1.4342831373214722\n",
      "epoch: 84, batch: 3, loss: 1.772273302078247\n",
      "epoch: 84, batch: 4, loss: 1.6596214771270752\n",
      "epoch: 84, batch: 5, loss: 1.6059764623641968\n",
      "epoch: 84, batch: 6, loss: 1.7004388570785522\n",
      "epoch: 84, batch: 7, loss: 1.4963343143463135\n",
      "epoch: 84, batch: 8, loss: 1.4933626651763916\n",
      "epoch: 84, batch: 9, loss: 1.277408242225647\n",
      "epoch: 84, batch: 10, loss: 1.4372822046279907\n",
      "epoch: 84, batch: 11, loss: 1.7605221271514893\n",
      "epoch: 84, batch: 12, loss: 1.4372605085372925\n",
      "epoch: 84, batch: 13, loss: 1.3900208473205566\n",
      "epoch: 84, batch: 14, loss: 1.330946445465088\n",
      "epoch: 84, batch: 15, loss: 1.6098308563232422\n",
      "epoch: 84, batch: 16, loss: 1.5470654964447021\n",
      "epoch: 84, batch: 17, loss: 1.5498626232147217\n",
      "epoch: 84, batch: 18, loss: 1.7042036056518555\n",
      "epoch: 84, batch: 19, loss: 1.540764331817627\n",
      "epoch: 84, batch: 20, loss: 1.6443017721176147\n",
      "epoch: 84, batch: 21, loss: 1.2709602117538452\n",
      "epoch: 84, batch: 22, loss: 1.76055109500885\n",
      "epoch: 84, batch: 23, loss: 1.1110594272613525\n",
      "epoch: 84, batch: 24, loss: 1.700739860534668\n",
      "epoch: 84, batch: 25, loss: 1.5943840742111206\n",
      "epoch: 84, batch: 26, loss: 1.597005009651184\n",
      "epoch: 84, batch: 27, loss: 1.6598103046417236\n",
      "epoch: 84, batch: 28, loss: 1.6535625457763672\n",
      "epoch: 84, batch: 29, loss: 1.4843237400054932\n",
      "epoch: 84, batch: 30, loss: 1.7070823907852173\n",
      "epoch: 84, batch: 31, loss: 1.4370554685592651\n",
      "epoch: 84, batch: 32, loss: 1.4905191659927368\n",
      "epoch: 84, batch: 33, loss: 1.430816650390625\n",
      "epoch: 84, batch: 34, loss: 1.5434629917144775\n",
      "epoch: 84, batch: 35, loss: 1.6093170642852783\n",
      "epoch: 84, batch: 36, loss: 1.6512750387191772\n",
      "epoch: 84, batch: 37, loss: 1.4308040142059326\n",
      "epoch: 84, batch: 38, loss: 1.4284257888793945\n",
      "epoch: 84, batch: 39, loss: 1.4903959035873413\n",
      "epoch: 84, batch: 40, loss: 1.3243108987808228\n",
      "epoch: 84, batch: 41, loss: 1.7094981670379639\n",
      "epoch: 84, batch: 42, loss: 1.6599340438842773\n",
      "epoch: 84, batch: 43, loss: 1.3242791891098022\n",
      "epoch: 84, batch: 44, loss: 1.7544686794281006\n",
      "epoch: 84, batch: 45, loss: 1.5022903680801392\n",
      "epoch: 84, batch: 46, loss: 1.2236082553863525\n",
      "epoch: 84, batch: 47, loss: 1.5412192344665527\n",
      "epoch: 84, batch: 48, loss: 1.5495164394378662\n",
      "epoch: 84, batch: 49, loss: 1.4841214418411255\n",
      "epoch: 84, batch: 50, loss: 1.7627522945404053\n",
      "epoch: 84, batch: 51, loss: 1.4368387460708618\n",
      "epoch: 84, batch: 52, loss: 1.7606210708618164\n",
      "epoch: 84, batch: 53, loss: 1.4940539598464966\n",
      "epoch: 84, batch: 54, loss: 1.8178889751434326\n",
      "epoch: 84, batch: 55, loss: 1.2768888473510742\n",
      "epoch: 84, batch: 56, loss: 1.5946755409240723\n",
      "epoch: 84, batch: 57, loss: 1.7073278427124023\n",
      "epoch: 84, batch: 58, loss: 1.3180862665176392\n",
      "epoch: 84, batch: 59, loss: 1.2235552072525024\n",
      "epoch: 84, batch: 60, loss: 1.5414292812347412\n",
      "epoch: 84, batch: 61, loss: 1.1641819477081299\n",
      "epoch: 84, batch: 62, loss: 1.7565313577651978\n",
      "epoch: 84, batch: 63, loss: 1.489971399307251\n",
      "epoch: 84, batch: 64, loss: 1.2235352993011475\n",
      "epoch: 84, batch: 65, loss: 1.4366865158081055\n",
      "epoch: 84, batch: 66, loss: 1.555294394493103\n",
      "epoch: 84, batch: 67, loss: 1.1581474542617798\n",
      "epoch: 84, batch: 68, loss: 1.5415576696395874\n",
      "epoch: 84, batch: 69, loss: 1.4348901510238647\n",
      "epoch: 84, batch: 70, loss: 1.7057126760482788\n",
      "epoch: 84, batch: 71, loss: 1.6437917947769165\n",
      "epoch: 84, batch: 72, loss: 1.5475584268569946\n",
      "epoch: 84, batch: 73, loss: 1.3893349170684814\n",
      "epoch: 84, batch: 74, loss: 1.483853816986084\n",
      "epoch: 84, batch: 75, loss: 1.270756721496582\n",
      "epoch: 84, batch: 76, loss: 1.4365673065185547\n",
      "epoch: 84, batch: 77, loss: 1.6618375778198242\n",
      "epoch: 84, batch: 78, loss: 1.4290770292282104\n",
      "epoch: 84, batch: 79, loss: 1.2707406282424927\n",
      "epoch: 84, batch: 80, loss: 1.4349910020828247\n",
      "epoch: 84, batch: 81, loss: 1.3818604946136475\n",
      "epoch: 84, batch: 82, loss: 1.5949139595031738\n",
      "epoch: 84, batch: 83, loss: 1.9205951690673828\n",
      "epoch: 84, batch: 84, loss: 1.3833614587783813\n",
      "epoch: 84, batch: 85, loss: 1.4423191547393799\n",
      "epoch: 84, batch: 86, loss: 1.4837214946746826\n",
      "epoch: 84, batch: 87, loss: 1.6603387594223022\n",
      "epoch: 84, batch: 88, loss: 1.7062362432479858\n",
      "epoch: 84, batch: 89, loss: 1.72062087059021\n",
      "epoch: 84, batch: 90, loss: 1.2234331369400024\n",
      "epoch: 84, batch: 91, loss: 1.872037649154663\n",
      "epoch: 84, batch: 92, loss: 1.1638457775115967\n",
      "epoch: 84, batch: 93, loss: 1.2706818580627441\n",
      "epoch: 84, batch: 94, loss: 1.653359055519104\n",
      "epoch: 84, batch: 95, loss: 1.4363616704940796\n",
      "epoch: 84, batch: 96, loss: 1.595038652420044\n",
      "epoch: 84, batch: 97, loss: 1.7661117315292358\n",
      "epoch: 84, batch: 98, loss: 1.3833072185516357\n",
      "epoch: 84, batch: 99, loss: 1.5962241888046265\n",
      "epoch: 84, batch: 100, loss: 1.2764017581939697\n",
      "epoch: 84, batch: 101, loss: 1.436297059059143\n",
      "epoch: 84, batch: 102, loss: 1.4362865686416626\n",
      "epoch: 84, batch: 103, loss: 1.4362757205963135\n",
      "epoch: 84, batch: 104, loss: 1.3822219371795654\n",
      "epoch: 84, batch: 105, loss: 1.5961618423461914\n",
      "epoch: 84, batch: 106, loss: 1.5016348361968994\n",
      "epoch: 84, batch: 107, loss: 1.3889745473861694\n",
      "epoch: 84, batch: 108, loss: 1.595144510269165\n",
      "epoch: 84, batch: 109, loss: 1.3235605955123901\n",
      "epoch: 84, batch: 110, loss: 1.820465326309204\n",
      "epoch: 84, batch: 111, loss: 1.6017735004425049\n",
      "epoch: 84, batch: 112, loss: 1.6549075841903687\n",
      "epoch: 84, batch: 113, loss: 1.760759949684143\n",
      "epoch: 84, batch: 114, loss: 1.5422871112823486\n",
      "epoch: 84, batch: 115, loss: 1.4890515804290771\n",
      "epoch: 84, batch: 116, loss: 1.8205236196517944\n",
      "epoch: 84, batch: 117, loss: 1.4958895444869995\n",
      "epoch: 84, batch: 118, loss: 1.276201844215393\n",
      "epoch: 84, batch: 119, loss: 1.7071224451065063\n",
      "epoch: 84, batch: 120, loss: 1.5896414518356323\n",
      "epoch: 84, batch: 121, loss: 1.4833426475524902\n",
      "epoch: 84, batch: 122, loss: 1.3359571695327759\n",
      "epoch: 84, batch: 123, loss: 1.3234058618545532\n",
      "epoch: 84, batch: 124, loss: 1.595967173576355\n",
      "epoch: 84, batch: 125, loss: 1.5014376640319824\n",
      "epoch: 84, batch: 126, loss: 1.3359415531158447\n",
      "epoch: 84, batch: 127, loss: 1.4864578247070312\n",
      "epoch: 85, batch: 0, loss: 1.4360086917877197\n",
      "epoch: 85, batch: 1, loss: 1.600881576538086\n",
      "epoch: 85, batch: 2, loss: 1.601447343826294\n",
      "epoch: 85, batch: 3, loss: 1.2699788808822632\n",
      "epoch: 85, batch: 4, loss: 1.435441255569458\n",
      "epoch: 85, batch: 5, loss: 1.1578562259674072\n",
      "epoch: 85, batch: 6, loss: 1.2705106735229492\n",
      "epoch: 85, batch: 7, loss: 1.5541000366210938\n",
      "epoch: 85, batch: 8, loss: 1.6008970737457275\n",
      "epoch: 85, batch: 9, loss: 1.6013275384902954\n",
      "epoch: 85, batch: 10, loss: 1.2704941034317017\n",
      "epoch: 85, batch: 11, loss: 1.49411141872406\n",
      "epoch: 85, batch: 12, loss: 1.3886170387268066\n",
      "epoch: 85, batch: 13, loss: 1.8734920024871826\n",
      "epoch: 85, batch: 14, loss: 1.4885809421539307\n",
      "epoch: 85, batch: 15, loss: 1.2701594829559326\n",
      "epoch: 85, batch: 16, loss: 1.9858661890029907\n",
      "epoch: 85, batch: 17, loss: 1.5482126474380493\n",
      "epoch: 85, batch: 18, loss: 1.2758878469467163\n",
      "epoch: 85, batch: 19, loss: 1.5957368612289429\n",
      "epoch: 85, batch: 20, loss: 1.8208322525024414\n",
      "epoch: 85, batch: 21, loss: 1.7027771472930908\n",
      "epoch: 85, batch: 22, loss: 1.6483714580535889\n",
      "epoch: 85, batch: 23, loss: 1.5482628345489502\n",
      "epoch: 85, batch: 24, loss: 1.382958173751831\n",
      "epoch: 85, batch: 25, loss: 1.3177086114883423\n",
      "epoch: 85, batch: 26, loss: 1.5429270267486572\n",
      "epoch: 85, batch: 27, loss: 1.3830955028533936\n",
      "epoch: 85, batch: 28, loss: 1.5430326461791992\n",
      "epoch: 85, batch: 29, loss: 1.868194818496704\n",
      "epoch: 85, batch: 30, loss: 1.1104767322540283\n",
      "epoch: 85, batch: 31, loss: 1.9755287170410156\n",
      "epoch: 85, batch: 32, loss: 1.8682260513305664\n",
      "epoch: 85, batch: 33, loss: 1.3883812427520752\n",
      "epoch: 85, batch: 34, loss: 1.7030036449432373\n",
      "epoch: 85, batch: 35, loss: 0.997783362865448\n",
      "epoch: 85, batch: 36, loss: 1.4881846904754639\n",
      "epoch: 85, batch: 37, loss: 1.6136785745620728\n",
      "epoch: 85, batch: 38, loss: 1.713491678237915\n",
      "epoch: 85, batch: 39, loss: 1.863170862197876\n",
      "epoch: 85, batch: 40, loss: 1.482862114906311\n",
      "epoch: 85, batch: 41, loss: 1.275620698928833\n",
      "epoch: 85, batch: 42, loss: 1.7554676532745361\n",
      "epoch: 85, batch: 43, loss: 1.8160362243652344\n",
      "epoch: 85, batch: 44, loss: 1.5432183742523193\n",
      "epoch: 85, batch: 45, loss: 1.477595567703247\n",
      "epoch: 85, batch: 46, loss: 1.6006824970245361\n",
      "epoch: 85, batch: 47, loss: 1.768617033958435\n",
      "epoch: 85, batch: 48, loss: 1.3830242156982422\n",
      "epoch: 85, batch: 49, loss: 1.600951910018921\n",
      "epoch: 85, batch: 50, loss: 1.4358171224594116\n",
      "epoch: 85, batch: 51, loss: 1.595782995223999\n",
      "epoch: 85, batch: 52, loss: 1.4406229257583618\n",
      "epoch: 85, batch: 53, loss: 1.3881657123565674\n",
      "epoch: 85, batch: 54, loss: 1.7687333822250366\n",
      "epoch: 85, batch: 55, loss: 1.4354242086410522\n",
      "epoch: 85, batch: 56, loss: 1.6612275838851929\n",
      "epoch: 85, batch: 57, loss: 1.3829904794692993\n",
      "epoch: 85, batch: 58, loss: 1.1103473901748657\n",
      "epoch: 85, batch: 59, loss: 1.8787345886230469\n",
      "epoch: 85, batch: 60, loss: 1.6612651348114014\n",
      "epoch: 85, batch: 61, loss: 1.5480420589447021\n",
      "epoch: 85, batch: 62, loss: 1.4353502988815308\n",
      "epoch: 85, batch: 63, loss: 1.2702856063842773\n",
      "epoch: 85, batch: 64, loss: 1.8212635517120361\n",
      "epoch: 85, batch: 65, loss: 1.6562315225601196\n",
      "epoch: 85, batch: 66, loss: 1.7609531879425049\n",
      "epoch: 85, batch: 67, loss: 1.3829514980316162\n",
      "epoch: 85, batch: 68, loss: 1.5486562252044678\n",
      "epoch: 85, batch: 69, loss: 1.6613479852676392\n",
      "epoch: 85, batch: 70, loss: 1.2229756116867065\n",
      "epoch: 85, batch: 71, loss: 1.5436439514160156\n",
      "epoch: 85, batch: 72, loss: 1.1102840900421143\n",
      "epoch: 85, batch: 73, loss: 1.3837103843688965\n",
      "epoch: 85, batch: 74, loss: 1.5487099885940552\n",
      "epoch: 85, batch: 75, loss: 1.4402259588241577\n",
      "epoch: 85, batch: 76, loss: 1.7559789419174194\n",
      "epoch: 85, batch: 77, loss: 1.435193419456482\n",
      "epoch: 85, batch: 78, loss: 1.5951511859893799\n",
      "epoch: 85, batch: 79, loss: 1.5951416492462158\n",
      "epoch: 85, batch: 80, loss: 1.43516206741333\n",
      "epoch: 85, batch: 81, loss: 1.3838300704956055\n",
      "epoch: 85, batch: 82, loss: 1.6564972400665283\n",
      "epoch: 85, batch: 83, loss: 1.601036787033081\n",
      "epoch: 85, batch: 84, loss: 1.5448429584503174\n",
      "epoch: 85, batch: 85, loss: 1.3828742504119873\n",
      "epoch: 85, batch: 86, loss: 1.387807011604309\n",
      "epoch: 85, batch: 87, loss: 1.4350898265838623\n",
      "epoch: 85, batch: 88, loss: 1.5999786853790283\n",
      "epoch: 85, batch: 89, loss: 1.5004537105560303\n",
      "epoch: 85, batch: 90, loss: 1.3839629888534546\n",
      "epoch: 85, batch: 91, loss: 1.7512314319610596\n",
      "epoch: 85, batch: 92, loss: 1.275059461593628\n",
      "epoch: 85, batch: 93, loss: 1.5525976419448853\n",
      "epoch: 85, batch: 94, loss: 1.322338342666626\n",
      "epoch: 85, batch: 95, loss: 1.4362144470214844\n",
      "epoch: 85, batch: 96, loss: 1.382824182510376\n",
      "epoch: 85, batch: 97, loss: 1.6520010232925415\n",
      "epoch: 85, batch: 98, loss: 1.710168480873108\n",
      "epoch: 85, batch: 99, loss: 1.3222880363464355\n",
      "epoch: 85, batch: 100, loss: 1.4301139116287231\n",
      "epoch: 85, batch: 101, loss: 1.4774200916290283\n",
      "epoch: 85, batch: 102, loss: 1.552442193031311\n",
      "epoch: 85, batch: 103, loss: 1.5900933742523193\n",
      "epoch: 85, batch: 104, loss: 1.43491530418396\n",
      "epoch: 85, batch: 105, loss: 1.5962938070297241\n",
      "epoch: 85, batch: 106, loss: 1.5523736476898193\n",
      "epoch: 85, batch: 107, loss: 1.6517820358276367\n",
      "epoch: 85, batch: 108, loss: 1.436328649520874\n",
      "epoch: 85, batch: 109, loss: 1.5996395349502563\n",
      "epoch: 85, batch: 110, loss: 1.870507836341858\n",
      "epoch: 85, batch: 111, loss: 1.4821621179580688\n",
      "epoch: 85, batch: 112, loss: 1.5490412712097168\n",
      "epoch: 85, batch: 113, loss: 1.4348225593566895\n",
      "epoch: 85, batch: 114, loss: 1.7138038873672485\n",
      "epoch: 85, batch: 115, loss: 1.439540982246399\n",
      "epoch: 85, batch: 116, loss: 1.2700588703155518\n",
      "epoch: 85, batch: 117, loss: 1.3827329874038696\n",
      "epoch: 85, batch: 118, loss: 1.4347705841064453\n",
      "epoch: 85, batch: 119, loss: 1.2747571468353271\n",
      "epoch: 85, batch: 120, loss: 1.6554043292999268\n",
      "epoch: 85, batch: 121, loss: 1.4970980882644653\n",
      "epoch: 85, batch: 122, loss: 1.4971134662628174\n",
      "epoch: 85, batch: 123, loss: 1.431775689125061\n",
      "epoch: 85, batch: 124, loss: 1.3827064037322998\n",
      "epoch: 85, batch: 125, loss: 1.6467006206512451\n",
      "epoch: 85, batch: 126, loss: 1.5445029735565186\n",
      "epoch: 85, batch: 127, loss: 1.679049015045166\n",
      "epoch: 86, batch: 0, loss: 1.599323034286499\n",
      "epoch: 86, batch: 1, loss: 1.2700062990188599\n",
      "epoch: 86, batch: 2, loss: 1.3826828002929688\n",
      "epoch: 86, batch: 3, loss: 1.7092193365097046\n",
      "epoch: 86, batch: 4, loss: 1.756561279296875\n",
      "epoch: 86, batch: 5, loss: 1.3265459537506104\n",
      "epoch: 86, batch: 6, loss: 1.3872710466384888\n",
      "epoch: 86, batch: 7, loss: 1.5992074012756348\n",
      "epoch: 86, batch: 8, loss: 1.3264944553375244\n",
      "epoch: 86, batch: 9, loss: 1.1099565029144287\n",
      "epoch: 86, batch: 10, loss: 1.7047078609466553\n",
      "epoch: 86, batch: 11, loss: 1.8738842010498047\n",
      "epoch: 86, batch: 12, loss: 1.2720235586166382\n",
      "epoch: 86, batch: 13, loss: 1.499875783920288\n",
      "epoch: 86, batch: 14, loss: 1.5447537899017334\n",
      "epoch: 86, batch: 15, loss: 1.3847497701644897\n",
      "epoch: 86, batch: 16, loss: 1.434501051902771\n",
      "epoch: 86, batch: 17, loss: 1.1099177598953247\n",
      "epoch: 86, batch: 18, loss: 2.036109209060669\n",
      "epoch: 86, batch: 19, loss: 1.5516692399978638\n",
      "epoch: 86, batch: 20, loss: 1.5944827795028687\n",
      "epoch: 86, batch: 21, loss: 1.3217675685882568\n",
      "epoch: 86, batch: 22, loss: 1.5989611148834229\n",
      "epoch: 86, batch: 23, loss: 1.4344288110733032\n",
      "epoch: 86, batch: 24, loss: 1.4389022588729858\n",
      "epoch: 86, batch: 25, loss: 1.2743812799453735\n",
      "epoch: 86, batch: 26, loss: 1.551550269126892\n",
      "epoch: 86, batch: 27, loss: 1.70733642578125\n",
      "epoch: 86, batch: 28, loss: 1.6641982793807983\n",
      "epoch: 86, batch: 29, loss: 1.646193265914917\n",
      "epoch: 86, batch: 30, loss: 1.3261154890060425\n",
      "epoch: 86, batch: 31, loss: 1.8221702575683594\n",
      "epoch: 86, batch: 32, loss: 1.659701943397522\n",
      "epoch: 86, batch: 33, loss: 1.6621557474136353\n",
      "epoch: 86, batch: 34, loss: 1.5943498611450195\n",
      "epoch: 86, batch: 35, loss: 1.5425831079483032\n",
      "epoch: 86, batch: 36, loss: 1.3351905345916748\n",
      "epoch: 86, batch: 37, loss: 1.4324342012405396\n",
      "epoch: 86, batch: 38, loss: 1.5924886465072632\n",
      "epoch: 86, batch: 39, loss: 1.8222465515136719\n",
      "epoch: 86, batch: 40, loss: 1.4342529773712158\n",
      "epoch: 86, batch: 41, loss: 1.5451818704605103\n",
      "epoch: 86, batch: 42, loss: 1.5451979637145996\n",
      "epoch: 86, batch: 43, loss: 1.873989462852478\n",
      "epoch: 86, batch: 44, loss: 1.4902483224868774\n",
      "epoch: 86, batch: 45, loss: 1.820665717124939\n",
      "epoch: 86, batch: 46, loss: 1.6579440832138062\n",
      "epoch: 86, batch: 47, loss: 1.7139555215835571\n",
      "epoch: 86, batch: 48, loss: 1.4341703653335571\n",
      "epoch: 86, batch: 49, loss: 1.3824927806854248\n",
      "epoch: 86, batch: 50, loss: 1.8783038854599\n",
      "epoch: 86, batch: 51, loss: 1.3257452249526978\n",
      "epoch: 86, batch: 52, loss: 1.4900562763214111\n",
      "epoch: 86, batch: 53, loss: 1.4383902549743652\n",
      "epoch: 86, batch: 54, loss: 1.5984232425689697\n",
      "epoch: 86, batch: 55, loss: 1.5984060764312744\n",
      "epoch: 86, batch: 56, loss: 1.659446120262146\n",
      "epoch: 86, batch: 57, loss: 1.705493688583374\n",
      "epoch: 86, batch: 58, loss: 1.659424066543579\n",
      "epoch: 86, batch: 59, loss: 1.3854182958602905\n",
      "epoch: 86, batch: 60, loss: 1.594101905822754\n",
      "epoch: 86, batch: 61, loss: 1.4340355396270752\n",
      "epoch: 86, batch: 62, loss: 1.1097097396850586\n",
      "epoch: 86, batch: 63, loss: 1.4981584548950195\n",
      "epoch: 86, batch: 64, loss: 1.4813873767852783\n",
      "epoch: 86, batch: 65, loss: 1.3824336528778076\n",
      "epoch: 86, batch: 66, loss: 1.4329146146774292\n",
      "epoch: 86, batch: 67, loss: 1.5466482639312744\n",
      "epoch: 86, batch: 68, loss: 1.5971633195877075\n",
      "epoch: 86, batch: 69, loss: 1.3254231214523315\n",
      "epoch: 86, batch: 70, loss: 1.429806113243103\n",
      "epoch: 86, batch: 71, loss: 1.1611945629119873\n",
      "epoch: 86, batch: 72, loss: 1.5498096942901611\n",
      "epoch: 86, batch: 73, loss: 1.5980894565582275\n",
      "epoch: 86, batch: 74, loss: 1.447676658630371\n",
      "epoch: 86, batch: 75, loss: 1.489479899406433\n",
      "epoch: 86, batch: 76, loss: 1.4950640201568604\n",
      "epoch: 86, batch: 77, loss: 1.4331023693084717\n",
      "epoch: 86, batch: 78, loss: 1.2697169780731201\n",
      "epoch: 86, batch: 79, loss: 1.5939202308654785\n",
      "epoch: 86, batch: 80, loss: 1.334975242614746\n",
      "epoch: 86, batch: 81, loss: 2.0375442504882812\n",
      "epoch: 86, batch: 82, loss: 1.6619325876235962\n",
      "epoch: 86, batch: 83, loss: 1.641290307044983\n",
      "epoch: 86, batch: 84, loss: 1.3251550197601318\n",
      "epoch: 86, batch: 85, loss: 1.3863762617111206\n",
      "epoch: 86, batch: 86, loss: 1.5933336019515991\n",
      "epoch: 86, batch: 87, loss: 1.5938447713851929\n",
      "epoch: 86, batch: 88, loss: 1.1570203304290771\n",
      "epoch: 86, batch: 89, loss: 1.6013561487197876\n",
      "epoch: 86, batch: 90, loss: 1.6112333536148071\n",
      "epoch: 86, batch: 91, loss: 1.4475816488265991\n",
      "epoch: 86, batch: 92, loss: 1.1609697341918945\n",
      "epoch: 86, batch: 93, loss: 1.4949917793273926\n",
      "epoch: 86, batch: 94, loss: 1.3862712383270264\n",
      "epoch: 86, batch: 95, loss: 1.3210232257843018\n",
      "epoch: 86, batch: 96, loss: 1.5974416732788086\n",
      "epoch: 86, batch: 97, loss: 1.549803614616394\n",
      "epoch: 86, batch: 98, loss: 1.5935478210449219\n",
      "epoch: 86, batch: 99, loss: 1.5463016033172607\n",
      "epoch: 86, batch: 100, loss: 1.597621202468872\n",
      "epoch: 86, batch: 101, loss: 1.8740193843841553\n",
      "epoch: 86, batch: 102, loss: 1.1569797992706299\n",
      "epoch: 86, batch: 103, loss: 1.870204210281372\n",
      "epoch: 86, batch: 104, loss: 1.6627094745635986\n",
      "epoch: 86, batch: 105, loss: 1.6627528667449951\n",
      "epoch: 86, batch: 106, loss: 1.8228756189346313\n",
      "epoch: 86, batch: 107, loss: 1.9394607543945312\n",
      "epoch: 86, batch: 108, loss: 1.2735286951065063\n",
      "epoch: 86, batch: 109, loss: 1.9177711009979248\n",
      "epoch: 86, batch: 110, loss: 1.4336597919464111\n",
      "epoch: 86, batch: 111, loss: 1.269603967666626\n",
      "epoch: 86, batch: 112, loss: 1.7538973093032837\n",
      "epoch: 86, batch: 113, loss: 1.386049509048462\n",
      "epoch: 86, batch: 114, loss: 1.4296966791152954\n",
      "epoch: 86, batch: 115, loss: 1.8229362964630127\n",
      "epoch: 86, batch: 116, loss: 1.1094783544540405\n",
      "epoch: 86, batch: 117, loss: 1.1569291353225708\n",
      "epoch: 86, batch: 118, loss: 1.5938966274261475\n",
      "epoch: 86, batch: 119, loss: 1.5464555025100708\n",
      "epoch: 86, batch: 120, loss: 1.1569173336029053\n",
      "epoch: 86, batch: 121, loss: 1.2733094692230225\n",
      "epoch: 86, batch: 122, loss: 1.644720435142517\n",
      "epoch: 86, batch: 123, loss: 1.5972440242767334\n",
      "epoch: 86, batch: 124, loss: 1.775559425354004\n",
      "epoch: 86, batch: 125, loss: 1.550268530845642\n",
      "epoch: 86, batch: 126, loss: 1.5971972942352295\n",
      "epoch: 86, batch: 127, loss: 1.1969153881072998\n",
      "epoch: 87, batch: 0, loss: 1.5971664190292358\n",
      "epoch: 87, batch: 1, loss: 1.273223638534546\n",
      "epoch: 87, batch: 2, loss: 1.5503137111663818\n",
      "epoch: 87, batch: 3, loss: 1.5503227710723877\n",
      "epoch: 87, batch: 4, loss: 1.5977998971939087\n",
      "epoch: 87, batch: 5, loss: 1.8231090307235718\n",
      "epoch: 87, batch: 6, loss: 1.870587706565857\n",
      "epoch: 87, batch: 7, loss: 1.546700119972229\n",
      "epoch: 87, batch: 8, loss: 1.3346750736236572\n",
      "epoch: 87, batch: 9, loss: 1.4369053840637207\n",
      "epoch: 87, batch: 10, loss: 1.43324875831604\n",
      "epoch: 87, batch: 11, loss: 1.658538579940796\n",
      "epoch: 87, batch: 12, loss: 1.6630542278289795\n",
      "epoch: 87, batch: 13, loss: 1.546791434288025\n",
      "epoch: 87, batch: 14, loss: 1.8742870092391968\n",
      "epoch: 87, batch: 15, loss: 1.2740468978881836\n",
      "epoch: 87, batch: 16, loss: 1.596917748451233\n",
      "epoch: 87, batch: 17, loss: 1.7534325122833252\n",
      "epoch: 87, batch: 18, loss: 1.1093230247497559\n",
      "epoch: 87, batch: 19, loss: 1.710595726966858\n",
      "epoch: 87, batch: 20, loss: 1.7059271335601807\n",
      "epoch: 87, batch: 21, loss: 1.221956491470337\n",
      "epoch: 87, batch: 22, loss: 1.546929121017456\n",
      "epoch: 87, batch: 23, loss: 1.7130165100097656\n",
      "epoch: 87, batch: 24, loss: 1.1603282690048218\n",
      "epoch: 87, batch: 25, loss: 1.7106540203094482\n",
      "epoch: 87, batch: 26, loss: 1.1092818975448608\n",
      "epoch: 87, batch: 27, loss: 1.2742234468460083\n",
      "epoch: 87, batch: 28, loss: 1.5932039022445679\n",
      "epoch: 87, batch: 29, loss: 1.7638938426971436\n",
      "epoch: 87, batch: 30, loss: 1.7142175436019897\n",
      "epoch: 87, batch: 31, loss: 1.434419870376587\n",
      "epoch: 87, batch: 32, loss: 1.8282772302627563\n",
      "epoch: 87, batch: 33, loss: 1.3855262994766235\n",
      "epoch: 87, batch: 34, loss: 1.5946067571640015\n",
      "epoch: 87, batch: 35, loss: 1.4329992532730103\n",
      "epoch: 87, batch: 36, loss: 1.4364646673202515\n",
      "epoch: 87, batch: 37, loss: 1.483945608139038\n",
      "epoch: 87, batch: 38, loss: 1.316861867904663\n",
      "epoch: 87, batch: 39, loss: 1.3345047235488892\n",
      "epoch: 87, batch: 40, loss: 1.5965439081192017\n",
      "epoch: 87, batch: 41, loss: 1.2727925777435303\n",
      "epoch: 87, batch: 42, loss: 1.6440156698226929\n",
      "epoch: 87, batch: 43, loss: 1.5506806373596191\n",
      "epoch: 87, batch: 44, loss: 1.381982684135437\n",
      "epoch: 87, batch: 45, loss: 1.6633424758911133\n",
      "epoch: 87, batch: 46, loss: 1.2727372646331787\n",
      "epoch: 87, batch: 47, loss: 1.6599591970443726\n",
      "epoch: 87, batch: 48, loss: 1.758388876914978\n",
      "epoch: 87, batch: 49, loss: 1.334450125694275\n",
      "epoch: 87, batch: 50, loss: 1.5454933643341064\n",
      "epoch: 87, batch: 51, loss: 1.4945969581604004\n",
      "epoch: 87, batch: 52, loss: 1.3168212175369263\n",
      "epoch: 87, batch: 53, loss: 1.5474088191986084\n",
      "epoch: 87, batch: 54, loss: 1.2692973613739014\n",
      "epoch: 87, batch: 55, loss: 1.710947036743164\n",
      "epoch: 87, batch: 56, loss: 1.3872917890548706\n",
      "epoch: 87, batch: 57, loss: 0.9964777827262878\n",
      "epoch: 87, batch: 58, loss: 1.4294464588165283\n",
      "epoch: 87, batch: 59, loss: 1.7630465030670166\n",
      "epoch: 87, batch: 60, loss: 1.3234200477600098\n",
      "epoch: 87, batch: 61, loss: 1.272571325302124\n",
      "epoch: 87, batch: 62, loss: 1.545371413230896\n",
      "epoch: 87, batch: 63, loss: 1.94400155544281\n",
      "epoch: 87, batch: 64, loss: 1.5420676469802856\n",
      "epoch: 87, batch: 65, loss: 1.5984057188034058\n",
      "epoch: 87, batch: 66, loss: 1.758591651916504\n",
      "epoch: 87, batch: 67, loss: 1.2725045680999756\n",
      "epoch: 87, batch: 68, loss: 1.6436455249786377\n",
      "epoch: 87, batch: 69, loss: 1.1598435640335083\n",
      "epoch: 87, batch: 70, loss: 1.320013403892517\n",
      "epoch: 87, batch: 71, loss: 1.7111053466796875\n",
      "epoch: 87, batch: 72, loss: 1.643589973449707\n",
      "epoch: 87, batch: 73, loss: 1.500171422958374\n",
      "epoch: 87, batch: 74, loss: 1.4801599979400635\n",
      "epoch: 87, batch: 75, loss: 1.4351106882095337\n",
      "epoch: 87, batch: 76, loss: 1.6467363834381104\n",
      "epoch: 87, batch: 77, loss: 1.5477794408798218\n",
      "epoch: 87, batch: 78, loss: 1.2691930532455444\n",
      "epoch: 87, batch: 79, loss: 1.6128944158554077\n",
      "epoch: 87, batch: 80, loss: 1.2216298580169678\n",
      "epoch: 87, batch: 81, loss: 1.4769290685653687\n",
      "epoch: 87, batch: 82, loss: 1.7789806127548218\n",
      "epoch: 87, batch: 83, loss: 1.4832375049591064\n",
      "epoch: 87, batch: 84, loss: 1.592710256576538\n",
      "epoch: 87, batch: 85, loss: 1.7143875360488892\n",
      "epoch: 87, batch: 86, loss: 1.2722947597503662\n",
      "epoch: 87, batch: 87, loss: 1.5419914722442627\n",
      "epoch: 87, batch: 88, loss: 1.6605870723724365\n",
      "epoch: 87, batch: 89, loss: 1.429345726966858\n",
      "epoch: 87, batch: 90, loss: 1.5479786396026611\n",
      "epoch: 87, batch: 91, loss: 1.4293386936187744\n",
      "epoch: 87, batch: 92, loss: 1.5511085987091064\n",
      "epoch: 87, batch: 93, loss: 1.3228784799575806\n",
      "epoch: 87, batch: 94, loss: 1.4384870529174805\n",
      "epoch: 87, batch: 95, loss: 1.7650737762451172\n",
      "epoch: 87, batch: 96, loss: 1.4943907260894775\n",
      "epoch: 87, batch: 97, loss: 1.758932113647461\n",
      "epoch: 87, batch: 98, loss: 1.5511629581451416\n",
      "epoch: 87, batch: 99, loss: 1.381737232208252\n",
      "epoch: 87, batch: 100, loss: 1.7052074670791626\n",
      "epoch: 87, batch: 101, loss: 1.3197047710418701\n",
      "epoch: 87, batch: 102, loss: 1.5925489664077759\n",
      "epoch: 87, batch: 103, loss: 1.8748451471328735\n",
      "epoch: 87, batch: 104, loss: 1.384742021560669\n",
      "epoch: 87, batch: 105, loss: 1.2753479480743408\n",
      "epoch: 87, batch: 106, loss: 1.3847213983535767\n",
      "epoch: 87, batch: 107, loss: 1.5449278354644775\n",
      "epoch: 87, batch: 108, loss: 1.3816959857940674\n",
      "epoch: 87, batch: 109, loss: 1.7178373336791992\n",
      "epoch: 87, batch: 110, loss: 1.5448987483978271\n",
      "epoch: 87, batch: 111, loss: 1.2214622497558594\n",
      "epoch: 87, batch: 112, loss: 1.1088142395019531\n",
      "epoch: 87, batch: 113, loss: 1.5547558069229126\n",
      "epoch: 87, batch: 114, loss: 1.7179819345474243\n",
      "epoch: 87, batch: 115, loss: 1.4322073459625244\n",
      "epoch: 87, batch: 116, loss: 1.4321978092193604\n",
      "epoch: 87, batch: 117, loss: 1.8277580738067627\n",
      "epoch: 87, batch: 118, loss: 1.5982990264892578\n",
      "epoch: 87, batch: 119, loss: 1.7621068954467773\n",
      "epoch: 87, batch: 120, loss: 1.3816386461257935\n",
      "epoch: 87, batch: 121, loss: 1.5953078269958496\n",
      "epoch: 87, batch: 122, loss: 1.707937240600586\n",
      "epoch: 87, batch: 123, loss: 1.2689800262451172\n",
      "epoch: 87, batch: 124, loss: 1.429206371307373\n",
      "epoch: 87, batch: 125, loss: 1.9978301525115967\n",
      "epoch: 87, batch: 126, loss: 1.8718855381011963\n",
      "epoch: 87, batch: 127, loss: 1.8044921159744263\n",
      "epoch: 88, batch: 0, loss: 1.592315912246704\n",
      "epoch: 88, batch: 1, loss: 1.547600269317627\n",
      "epoch: 88, batch: 2, loss: 1.7621625661849976\n",
      "epoch: 88, batch: 3, loss: 1.59516179561615\n",
      "epoch: 88, batch: 4, loss: 1.384448528289795\n",
      "epoch: 88, batch: 5, loss: 1.6136738061904907\n",
      "epoch: 88, batch: 6, loss: 1.4796161651611328\n",
      "epoch: 88, batch: 7, loss: 1.5446580648422241\n",
      "epoch: 88, batch: 8, loss: 1.432002305984497\n",
      "epoch: 88, batch: 9, loss: 1.7020431756973267\n",
      "epoch: 88, batch: 10, loss: 1.7117716073989868\n",
      "epoch: 88, batch: 11, loss: 1.4347985982894897\n",
      "epoch: 88, batch: 12, loss: 1.5515474081039429\n",
      "epoch: 88, batch: 13, loss: 1.7187583446502686\n",
      "epoch: 88, batch: 14, loss: 1.4291346073150635\n",
      "epoch: 88, batch: 15, loss: 1.4319331645965576\n",
      "epoch: 88, batch: 16, loss: 1.594966173171997\n",
      "epoch: 88, batch: 17, loss: 1.271667242050171\n",
      "epoch: 88, batch: 18, loss: 1.2759249210357666\n",
      "epoch: 88, batch: 19, loss: 1.4318938255310059\n",
      "epoch: 88, batch: 20, loss: 1.2716363668441772\n",
      "epoch: 88, batch: 21, loss: 1.5488637685775757\n",
      "epoch: 88, batch: 22, loss: 1.4318642616271973\n",
      "epoch: 88, batch: 23, loss: 1.3814958333969116\n",
      "epoch: 88, batch: 24, loss: 1.548909306526184\n",
      "epoch: 88, batch: 25, loss: 1.711918592453003\n",
      "epoch: 88, batch: 26, loss: 1.9444282054901123\n",
      "epoch: 88, batch: 27, loss: 1.6451351642608643\n",
      "epoch: 88, batch: 28, loss: 1.3842002153396606\n",
      "epoch: 88, batch: 29, loss: 1.2715431451797485\n",
      "epoch: 88, batch: 30, loss: 1.4793914556503296\n",
      "epoch: 88, batch: 31, loss: 2.112128496170044\n",
      "epoch: 88, batch: 32, loss: 1.8079884052276611\n",
      "epoch: 88, batch: 33, loss: 1.384148359298706\n",
      "epoch: 88, batch: 34, loss: 1.7596153020858765\n",
      "epoch: 88, batch: 35, loss: 1.2687962055206299\n",
      "epoch: 88, batch: 36, loss: 1.3888362646102905\n",
      "epoch: 88, batch: 37, loss: 1.429044485092163\n",
      "epoch: 88, batch: 38, loss: 1.4793171882629395\n",
      "epoch: 88, batch: 39, loss: 1.5946143865585327\n",
      "epoch: 88, batch: 40, loss: 1.4290335178375244\n",
      "epoch: 88, batch: 41, loss: 1.5919359922409058\n",
      "epoch: 88, batch: 42, loss: 1.7072150707244873\n",
      "epoch: 88, batch: 43, loss: 1.3190100193023682\n",
      "epoch: 88, batch: 44, loss: 1.434276819229126\n",
      "epoch: 88, batch: 45, loss: 1.4818737506866455\n",
      "epoch: 88, batch: 46, loss: 1.3163628578186035\n",
      "epoch: 88, batch: 47, loss: 1.5968749523162842\n",
      "epoch: 88, batch: 48, loss: 1.5994956493377686\n",
      "epoch: 88, batch: 49, loss: 1.3839845657348633\n",
      "epoch: 88, batch: 50, loss: 1.782270073890686\n",
      "epoch: 88, batch: 51, loss: 1.7121727466583252\n",
      "epoch: 88, batch: 52, loss: 1.3890769481658936\n",
      "epoch: 88, batch: 53, loss: 1.6420085430145264\n",
      "epoch: 88, batch: 54, loss: 1.481730341911316\n",
      "epoch: 88, batch: 55, loss: 1.276472806930542\n",
      "epoch: 88, batch: 56, loss: 1.3163199424743652\n",
      "epoch: 88, batch: 57, loss: 1.3188674449920654\n",
      "epoch: 88, batch: 58, loss: 1.3891690969467163\n",
      "epoch: 88, batch: 59, loss: 1.7747504711151123\n",
      "epoch: 88, batch: 60, loss: 1.318835973739624\n",
      "epoch: 88, batch: 61, loss: 1.2686866521835327\n",
      "epoch: 88, batch: 62, loss: 1.3813354969024658\n",
      "epoch: 88, batch: 63, loss: 1.428940773010254\n",
      "epoch: 88, batch: 64, loss: 1.6621782779693604\n",
      "epoch: 88, batch: 65, loss: 1.3813241720199585\n",
      "epoch: 88, batch: 66, loss: 1.4765344858169556\n",
      "epoch: 88, batch: 67, loss: 1.1584994792938232\n",
      "epoch: 88, batch: 68, loss: 1.1559982299804688\n",
      "epoch: 88, batch: 69, loss: 1.4814891815185547\n",
      "epoch: 88, batch: 70, loss: 1.3837848901748657\n",
      "epoch: 88, batch: 71, loss: 1.4939591884613037\n",
      "epoch: 88, batch: 72, loss: 1.4313690662384033\n",
      "epoch: 88, batch: 73, loss: 1.271097183227539\n",
      "epoch: 88, batch: 74, loss: 1.4394735097885132\n",
      "epoch: 88, batch: 75, loss: 1.6704944372177124\n",
      "epoch: 88, batch: 76, loss: 1.478927493095398\n",
      "epoch: 88, batch: 77, loss: 1.8275359869003296\n",
      "epoch: 88, batch: 78, loss: 1.4963769912719727\n",
      "epoch: 88, batch: 79, loss: 1.664849877357483\n",
      "epoch: 88, batch: 80, loss: 1.7775264978408813\n",
      "epoch: 88, batch: 81, loss: 1.4337068796157837\n",
      "epoch: 88, batch: 82, loss: 1.7566354274749756\n",
      "epoch: 88, batch: 83, loss: 1.428842306137085\n",
      "epoch: 88, batch: 84, loss: 1.502229928970337\n",
      "epoch: 88, batch: 85, loss: 1.108311414718628\n",
      "epoch: 88, batch: 86, loss: 1.7125146389007568\n",
      "epoch: 88, batch: 87, loss: 1.1582893133163452\n",
      "epoch: 88, batch: 88, loss: 1.4312103986740112\n",
      "epoch: 88, batch: 89, loss: 1.2209631204605103\n",
      "epoch: 88, batch: 90, loss: 1.5438628196716309\n",
      "epoch: 88, batch: 91, loss: 1.710187315940857\n",
      "epoch: 88, batch: 92, loss: 1.108276128768921\n",
      "epoch: 88, batch: 93, loss: 1.599910855293274\n",
      "epoch: 88, batch: 94, loss: 1.8252683877944946\n",
      "epoch: 88, batch: 95, loss: 1.3835536241531372\n",
      "epoch: 88, batch: 96, loss: 1.5024124383926392\n",
      "epoch: 88, batch: 97, loss: 1.4373397827148438\n",
      "epoch: 88, batch: 98, loss: 1.8752387762069702\n",
      "epoch: 88, batch: 99, loss: 2.0027878284454346\n",
      "epoch: 88, batch: 100, loss: 1.2684979438781738\n",
      "epoch: 88, batch: 101, loss: 1.4397226572036743\n",
      "epoch: 88, batch: 102, loss: 1.3207160234451294\n",
      "epoch: 88, batch: 103, loss: 1.721361756324768\n",
      "epoch: 88, batch: 104, loss: 1.8276824951171875\n",
      "epoch: 88, batch: 105, loss: 1.4786328077316284\n",
      "epoch: 88, batch: 106, loss: 1.1081998348236084\n",
      "epoch: 88, batch: 107, loss: 1.8318630456924438\n",
      "epoch: 88, batch: 108, loss: 1.4374973773956299\n",
      "epoch: 88, batch: 109, loss: 1.5501924753189087\n",
      "epoch: 88, batch: 110, loss: 1.600074052810669\n",
      "epoch: 88, batch: 111, loss: 1.433258056640625\n",
      "epoch: 88, batch: 112, loss: 1.5978267192840576\n",
      "epoch: 88, batch: 113, loss: 1.7105249166488647\n",
      "epoch: 88, batch: 114, loss: 1.4937951564788818\n",
      "epoch: 88, batch: 115, loss: 1.7105560302734375\n",
      "epoch: 88, batch: 116, loss: 1.2706619501113892\n",
      "epoch: 88, batch: 117, loss: 1.2706515789031982\n",
      "epoch: 88, batch: 118, loss: 1.873113989830017\n",
      "epoch: 88, batch: 119, loss: 1.5458256006240845\n",
      "epoch: 88, batch: 120, loss: 1.6584994792938232\n",
      "epoch: 88, batch: 121, loss: 1.593388557434082\n",
      "epoch: 88, batch: 122, loss: 1.8255665302276611\n",
      "epoch: 88, batch: 123, loss: 1.7150951623916626\n",
      "epoch: 88, batch: 124, loss: 1.6155109405517578\n",
      "epoch: 88, batch: 125, loss: 1.6387271881103516\n",
      "epoch: 88, batch: 126, loss: 1.7129175662994385\n",
      "epoch: 88, batch: 127, loss: 1.6919574737548828\n",
      "epoch: 89, batch: 0, loss: 1.4937416315078735\n",
      "epoch: 89, batch: 1, loss: 1.270526647567749\n",
      "epoch: 89, batch: 2, loss: 1.5526704788208008\n",
      "epoch: 89, batch: 3, loss: 1.615614891052246\n",
      "epoch: 89, batch: 4, loss: 1.8711074590682983\n",
      "epoch: 89, batch: 5, loss: 1.4329293966293335\n",
      "epoch: 89, batch: 6, loss: 1.712998390197754\n",
      "epoch: 89, batch: 7, loss: 1.5931909084320068\n",
      "epoch: 89, batch: 8, loss: 1.4958451986312866\n",
      "epoch: 89, batch: 9, loss: 1.8257243633270264\n",
      "epoch: 89, batch: 10, loss: 1.6157153844833374\n",
      "epoch: 89, batch: 11, loss: 1.1577268838882446\n",
      "epoch: 89, batch: 12, loss: 1.5931198596954346\n",
      "epoch: 89, batch: 13, loss: 1.6727354526519775\n",
      "epoch: 89, batch: 14, loss: 1.6727689504623413\n",
      "epoch: 89, batch: 15, loss: 1.317983865737915\n",
      "epoch: 89, batch: 16, loss: 1.6024971008300781\n",
      "epoch: 89, batch: 17, loss: 1.882861852645874\n",
      "epoch: 89, batch: 18, loss: 1.6025059223175049\n",
      "epoch: 89, batch: 19, loss: 1.955423355102539\n",
      "epoch: 89, batch: 20, loss: 1.5412570238113403\n",
      "epoch: 89, batch: 21, loss: 1.270318627357483\n",
      "epoch: 89, batch: 22, loss: 1.590919017791748\n",
      "epoch: 89, batch: 23, loss: 1.5412460565567017\n",
      "epoch: 89, batch: 24, loss: 1.2682397365570068\n",
      "epoch: 89, batch: 25, loss: 1.66353440284729\n",
      "epoch: 89, batch: 26, loss: 1.4781911373138428\n",
      "epoch: 89, batch: 27, loss: 1.430566430091858\n",
      "epoch: 89, batch: 28, loss: 1.4325803518295288\n",
      "epoch: 89, batch: 29, loss: 1.155522108078003\n",
      "epoch: 89, batch: 30, loss: 1.5452438592910767\n",
      "epoch: 89, batch: 31, loss: 1.2702131271362305\n",
      "epoch: 89, batch: 32, loss: 1.2682021856307983\n",
      "epoch: 89, batch: 33, loss: 1.7132701873779297\n",
      "epoch: 89, batch: 34, loss: 1.616040587425232\n",
      "epoch: 89, batch: 35, loss: 1.7132905721664429\n",
      "epoch: 89, batch: 36, loss: 1.157467246055603\n",
      "epoch: 89, batch: 37, loss: 1.5411937236785889\n",
      "epoch: 89, batch: 38, loss: 1.333235740661621\n",
      "epoch: 89, batch: 39, loss: 1.6637388467788696\n",
      "epoch: 89, batch: 40, loss: 1.9213005304336548\n",
      "epoch: 89, batch: 41, loss: 1.2701077461242676\n",
      "epoch: 89, batch: 42, loss: 1.317733645439148\n",
      "epoch: 89, batch: 43, loss: 2.0062613487243652\n",
      "epoch: 89, batch: 44, loss: 1.6364450454711914\n",
      "epoch: 89, batch: 45, loss: 1.430397391319275\n",
      "epoch: 89, batch: 46, loss: 1.7629646062850952\n",
      "epoch: 89, batch: 47, loss: 1.4935109615325928\n",
      "epoch: 89, batch: 48, loss: 1.8261127471923828\n",
      "epoch: 89, batch: 49, loss: 1.380810022354126\n",
      "epoch: 89, batch: 50, loss: 1.7529233694076538\n",
      "epoch: 89, batch: 51, loss: 1.7510168552398682\n",
      "epoch: 89, batch: 52, loss: 1.430332064628601\n",
      "epoch: 89, batch: 53, loss: 1.4798564910888672\n",
      "epoch: 89, batch: 54, loss: 1.5925318002700806\n",
      "epoch: 89, batch: 55, loss: 1.7217798233032227\n",
      "epoch: 89, batch: 56, loss: 1.1553970575332642\n",
      "epoch: 89, batch: 57, loss: 1.7868820428848267\n",
      "epoch: 89, batch: 58, loss: 1.4797890186309814\n",
      "epoch: 89, batch: 59, loss: 1.1572366952896118\n",
      "epoch: 89, batch: 60, loss: 1.3175700902938843\n",
      "epoch: 89, batch: 61, loss: 1.1077167987823486\n",
      "epoch: 89, batch: 62, loss: 1.4302380084991455\n",
      "epoch: 89, batch: 63, loss: 1.1077059507369995\n",
      "epoch: 89, batch: 64, loss: 1.317533254623413\n",
      "epoch: 89, batch: 65, loss: 1.4952363967895508\n",
      "epoch: 89, batch: 66, loss: 1.2844078540802002\n",
      "epoch: 89, batch: 67, loss: 1.7136132717132568\n",
      "epoch: 89, batch: 68, loss: 1.1553394794464111\n",
      "epoch: 89, batch: 69, loss: 1.641791582107544\n",
      "epoch: 89, batch: 70, loss: 1.4283618927001953\n",
      "epoch: 89, batch: 71, loss: 1.5991723537445068\n",
      "epoch: 89, batch: 72, loss: 1.8844982385635376\n",
      "epoch: 89, batch: 73, loss: 1.4301344156265259\n",
      "epoch: 89, batch: 74, loss: 1.5533348321914673\n",
      "epoch: 89, batch: 75, loss: 1.267988920211792\n",
      "epoch: 89, batch: 76, loss: 1.2697560787200928\n",
      "epoch: 89, batch: 77, loss: 1.5533638000488281\n",
      "epoch: 89, batch: 78, loss: 1.706650733947754\n",
      "epoch: 89, batch: 79, loss: 1.5516247749328613\n",
      "epoch: 89, batch: 80, loss: 0.9949243664741516\n",
      "epoch: 89, batch: 81, loss: 1.5534026622772217\n",
      "epoch: 89, batch: 82, loss: 1.3806402683258057\n",
      "epoch: 89, batch: 83, loss: 1.3173518180847168\n",
      "epoch: 89, batch: 84, loss: 1.8036072254180908\n",
      "epoch: 89, batch: 85, loss: 1.722796082496643\n",
      "epoch: 89, batch: 86, loss: 1.7138051986694336\n",
      "epoch: 89, batch: 87, loss: 1.5409713983535767\n",
      "epoch: 89, batch: 88, loss: 1.332945704460144\n",
      "epoch: 89, batch: 89, loss: 1.4810707569122314\n",
      "epoch: 89, batch: 90, loss: 1.4316799640655518\n",
      "epoch: 89, batch: 91, loss: 1.7155570983886719\n",
      "epoch: 89, batch: 92, loss: 1.4793174266815186\n",
      "epoch: 89, batch: 93, loss: 1.4316365718841553\n",
      "epoch: 89, batch: 94, loss: 1.66621994972229\n",
      "epoch: 89, batch: 95, loss: 1.4391615390777588\n",
      "epoch: 89, batch: 96, loss: 1.4792598485946655\n",
      "epoch: 89, batch: 97, loss: 1.4932650327682495\n",
      "epoch: 89, batch: 98, loss: 1.4282337427139282\n",
      "epoch: 89, batch: 99, loss: 1.6362617015838623\n",
      "epoch: 89, batch: 100, loss: 1.6012508869171143\n",
      "epoch: 89, batch: 101, loss: 1.3822014331817627\n",
      "epoch: 89, batch: 102, loss: 1.8360650539398193\n",
      "epoch: 89, batch: 103, loss: 1.2201769351959229\n",
      "epoch: 89, batch: 104, loss: 1.540907859802246\n",
      "epoch: 89, batch: 105, loss: 1.4932349920272827\n",
      "epoch: 89, batch: 106, loss: 1.5901920795440674\n",
      "epoch: 89, batch: 107, loss: 1.7633049488067627\n",
      "epoch: 89, batch: 108, loss: 1.6647469997406006\n",
      "epoch: 89, batch: 109, loss: 1.3328479528427124\n",
      "epoch: 89, batch: 110, loss: 1.269412875175476\n",
      "epoch: 89, batch: 111, loss: 1.6029531955718994\n",
      "epoch: 89, batch: 112, loss: 1.6663987636566162\n",
      "epoch: 89, batch: 113, loss: 1.6013758182525635\n",
      "epoch: 89, batch: 114, loss: 1.6664186716079712\n",
      "epoch: 89, batch: 115, loss: 2.0461320877075195\n",
      "epoch: 89, batch: 116, loss: 1.107405424118042\n",
      "epoch: 89, batch: 117, loss: 1.391800880432129\n",
      "epoch: 89, batch: 118, loss: 1.5521914958953857\n",
      "epoch: 89, batch: 119, loss: 1.5045340061187744\n",
      "epoch: 89, batch: 120, loss: 1.3185365200042725\n",
      "epoch: 89, batch: 121, loss: 1.2351890802383423\n",
      "epoch: 89, batch: 122, loss: 1.8253352642059326\n",
      "epoch: 89, batch: 123, loss: 1.801169991493225\n",
      "epoch: 89, batch: 124, loss: 1.4296530485153198\n",
      "epoch: 89, batch: 125, loss: 1.5915521383285522\n",
      "epoch: 89, batch: 126, loss: 1.7634079456329346\n",
      "epoch: 89, batch: 127, loss: 1.3378933668136597\n",
      "epoch: 90, batch: 0, loss: 1.761907935142517\n",
      "epoch: 90, batch: 1, loss: 1.4787886142730713\n",
      "epoch: 90, batch: 2, loss: 1.31279718875885\n",
      "epoch: 90, batch: 3, loss: 1.5538755655288696\n",
      "epoch: 90, batch: 4, loss: 1.3920072317123413\n",
      "epoch: 90, batch: 5, loss: 1.7312885522842407\n",
      "epoch: 90, batch: 6, loss: 1.702659010887146\n",
      "epoch: 90, batch: 7, loss: 1.699928641319275\n",
      "epoch: 90, batch: 8, loss: 1.7057440280914307\n",
      "epoch: 90, batch: 9, loss: 1.5437071323394775\n",
      "epoch: 90, batch: 10, loss: 1.5291610956192017\n",
      "epoch: 90, batch: 11, loss: 1.8509740829467773\n",
      "epoch: 90, batch: 12, loss: 1.9154293537139893\n",
      "epoch: 90, batch: 13, loss: 1.315338134765625\n",
      "epoch: 90, batch: 14, loss: 1.558379888534546\n",
      "epoch: 90, batch: 15, loss: 1.6933863162994385\n",
      "epoch: 90, batch: 16, loss: 1.3167507648468018\n",
      "epoch: 90, batch: 17, loss: 1.5511705875396729\n",
      "epoch: 90, batch: 18, loss: 1.439884901046753\n",
      "epoch: 90, batch: 19, loss: 1.4294376373291016\n",
      "epoch: 90, batch: 20, loss: 1.381744146347046\n",
      "epoch: 90, batch: 21, loss: 1.528484582901001\n",
      "epoch: 90, batch: 22, loss: 1.5526608228683472\n",
      "epoch: 90, batch: 23, loss: 1.0356879234313965\n",
      "epoch: 90, batch: 24, loss: 1.2689855098724365\n",
      "epoch: 90, batch: 25, loss: 1.5527058839797974\n",
      "epoch: 90, batch: 26, loss: 1.3092409372329712\n",
      "epoch: 90, batch: 27, loss: 1.7158657312393188\n",
      "epoch: 90, batch: 28, loss: 1.2675889730453491\n",
      "epoch: 90, batch: 29, loss: 1.4943803548812866\n",
      "epoch: 90, batch: 30, loss: 1.9520490169525146\n",
      "epoch: 90, batch: 31, loss: 1.6776043176651\n",
      "epoch: 90, batch: 32, loss: 1.2689025402069092\n",
      "epoch: 90, batch: 33, loss: 1.552825689315796\n",
      "epoch: 90, batch: 34, loss: 1.1561548709869385\n",
      "epoch: 90, batch: 35, loss: 1.3178737163543701\n",
      "epoch: 90, batch: 36, loss: 1.3924531936645508\n",
      "epoch: 90, batch: 37, loss: 1.6534242630004883\n",
      "epoch: 90, batch: 38, loss: 1.3815683126449585\n",
      "epoch: 90, batch: 39, loss: 1.3802692890167236\n",
      "epoch: 90, batch: 40, loss: 1.4305241107940674\n",
      "epoch: 90, batch: 41, loss: 1.7636172771453857\n",
      "epoch: 90, batch: 42, loss: 1.6534147262573242\n",
      "epoch: 90, batch: 43, loss: 1.6019353866577148\n",
      "epoch: 90, batch: 44, loss: 1.6669811010360718\n",
      "epoch: 90, batch: 45, loss: 1.429194688796997\n",
      "epoch: 90, batch: 46, loss: 1.2687530517578125\n",
      "epoch: 90, batch: 47, loss: 1.4291763305664062\n",
      "epoch: 90, batch: 48, loss: 1.4291670322418213\n",
      "epoch: 90, batch: 49, loss: 1.8751662969589233\n",
      "epoch: 90, batch: 50, loss: 1.5431044101715088\n",
      "epoch: 90, batch: 51, loss: 1.3164033889770508\n",
      "epoch: 90, batch: 52, loss: 1.2686879634857178\n",
      "epoch: 90, batch: 53, loss: 1.603234887123108\n",
      "epoch: 90, batch: 54, loss: 1.6020435094833374\n",
      "epoch: 90, batch: 55, loss: 1.2686551809310913\n",
      "epoch: 90, batch: 56, loss: 0.9942754507064819\n",
      "epoch: 90, batch: 57, loss: 1.6670992374420166\n",
      "epoch: 90, batch: 58, loss: 1.541813611984253\n",
      "epoch: 90, batch: 59, loss: 1.6395591497421265\n",
      "epoch: 90, batch: 60, loss: 1.4290564060211182\n",
      "epoch: 90, batch: 61, loss: 1.4767650365829468\n",
      "epoch: 90, batch: 62, loss: 1.5544028282165527\n",
      "epoch: 90, batch: 63, loss: 1.3324521780014038\n",
      "epoch: 90, batch: 64, loss: 1.7510759830474854\n",
      "epoch: 90, batch: 65, loss: 1.6660493612289429\n",
      "epoch: 90, batch: 66, loss: 1.5428581237792969\n",
      "epoch: 90, batch: 67, loss: 1.505610704421997\n",
      "epoch: 90, batch: 68, loss: 1.4278825521469116\n",
      "epoch: 90, batch: 69, loss: 1.4289734363555908\n",
      "epoch: 90, batch: 70, loss: 1.3173054456710815\n",
      "epoch: 90, batch: 71, loss: 1.8404791355133057\n",
      "epoch: 90, batch: 72, loss: 2.048722505569458\n",
      "epoch: 90, batch: 73, loss: 1.3172554969787598\n",
      "epoch: 90, batch: 74, loss: 1.2673847675323486\n",
      "epoch: 90, batch: 75, loss: 1.5406147241592407\n",
      "epoch: 90, batch: 76, loss: 1.716051697731018\n",
      "epoch: 90, batch: 77, loss: 1.749882459640503\n",
      "epoch: 90, batch: 78, loss: 1.2673680782318115\n",
      "epoch: 90, batch: 79, loss: 1.5416302680969238\n",
      "epoch: 90, batch: 80, loss: 1.4288723468780518\n",
      "epoch: 90, batch: 81, loss: 1.7150598764419556\n",
      "epoch: 90, batch: 82, loss: 1.5058163404464722\n",
      "epoch: 90, batch: 83, loss: 1.4775915145874023\n",
      "epoch: 90, batch: 84, loss: 1.3810808658599854\n",
      "epoch: 90, batch: 85, loss: 1.7271379232406616\n",
      "epoch: 90, batch: 86, loss: 1.4418504238128662\n",
      "epoch: 90, batch: 87, loss: 1.3810484409332275\n",
      "epoch: 90, batch: 88, loss: 1.4418665170669556\n",
      "epoch: 90, batch: 89, loss: 1.9234203100204468\n",
      "epoch: 90, batch: 90, loss: 1.2682628631591797\n",
      "epoch: 90, batch: 91, loss: 1.6024096012115479\n",
      "epoch: 90, batch: 92, loss: 1.267310380935669\n",
      "epoch: 90, batch: 93, loss: 2.001615047454834\n",
      "epoch: 90, batch: 94, loss: 1.6024391651153564\n",
      "epoch: 90, batch: 95, loss: 1.6370338201522827\n",
      "epoch: 90, batch: 96, loss: 1.492806077003479\n",
      "epoch: 90, batch: 97, loss: 1.666555643081665\n",
      "epoch: 90, batch: 98, loss: 1.637904405593872\n",
      "epoch: 90, batch: 99, loss: 1.5547103881835938\n",
      "epoch: 90, batch: 100, loss: 1.4295625686645508\n",
      "epoch: 90, batch: 101, loss: 1.55472731590271\n",
      "epoch: 90, batch: 102, loss: 1.3800283670425415\n",
      "epoch: 90, batch: 103, loss: 1.3800245523452759\n",
      "epoch: 90, batch: 104, loss: 1.1553465127944946\n",
      "epoch: 90, batch: 105, loss: 1.5900285243988037\n",
      "epoch: 90, batch: 106, loss: 1.427800178527832\n",
      "epoch: 90, batch: 107, loss: 1.4420158863067627\n",
      "epoch: 90, batch: 108, loss: 1.268062949180603\n",
      "epoch: 90, batch: 109, loss: 1.4763954877853394\n",
      "epoch: 90, batch: 110, loss: 1.2680410146713257\n",
      "epoch: 90, batch: 111, loss: 1.3150233030319214\n",
      "epoch: 90, batch: 112, loss: 1.2672268152236938\n",
      "epoch: 90, batch: 113, loss: 1.541335105895996\n",
      "epoch: 90, batch: 114, loss: 1.4277807474136353\n",
      "epoch: 90, batch: 115, loss: 1.7796096801757812\n",
      "epoch: 90, batch: 116, loss: 1.5898728370666504\n",
      "epoch: 90, batch: 117, loss: 1.8282005786895752\n",
      "epoch: 90, batch: 118, loss: 2.0159401893615723\n",
      "epoch: 90, batch: 119, loss: 1.5405384302139282\n",
      "epoch: 90, batch: 120, loss: 1.8411080837249756\n",
      "epoch: 90, batch: 121, loss: 1.6019680500030518\n",
      "epoch: 90, batch: 122, loss: 1.5063766241073608\n",
      "epoch: 90, batch: 123, loss: 1.540529727935791\n",
      "epoch: 90, batch: 124, loss: 1.714789628982544\n",
      "epoch: 90, batch: 125, loss: 1.7148061990737915\n",
      "epoch: 90, batch: 126, loss: 1.7032102346420288\n",
      "epoch: 90, batch: 127, loss: 1.6346087455749512\n",
      "epoch: 91, batch: 0, loss: 1.4291211366653442\n",
      "epoch: 91, batch: 1, loss: 1.4277373552322388\n",
      "epoch: 91, batch: 2, loss: 1.428412675857544\n",
      "epoch: 91, batch: 3, loss: 1.4768869876861572\n",
      "epoch: 91, batch: 4, loss: 1.5889838933944702\n",
      "epoch: 91, batch: 5, loss: 1.5543439388275146\n",
      "epoch: 91, batch: 6, loss: 1.8900582790374756\n",
      "epoch: 91, batch: 7, loss: 1.5404958724975586\n",
      "epoch: 91, batch: 8, loss: 1.7156273126602173\n",
      "epoch: 91, batch: 9, loss: 1.4761631488800049\n",
      "epoch: 91, batch: 10, loss: 1.5550518035888672\n",
      "epoch: 91, batch: 11, loss: 1.4283300638198853\n",
      "epoch: 91, batch: 12, loss: 1.4932862520217896\n",
      "epoch: 91, batch: 13, loss: 1.7156782150268555\n",
      "epoch: 91, batch: 14, loss: 1.4761189222335815\n",
      "epoch: 91, batch: 15, loss: 1.3938913345336914\n",
      "epoch: 91, batch: 16, loss: 1.5888841152191162\n",
      "epoch: 91, batch: 17, loss: 1.3798593282699585\n",
      "epoch: 91, batch: 18, loss: 2.003192901611328\n",
      "epoch: 91, batch: 19, loss: 1.2676541805267334\n",
      "epoch: 91, batch: 20, loss: 1.3148797750473022\n",
      "epoch: 91, batch: 21, loss: 1.7635780572891235\n",
      "epoch: 91, batch: 22, loss: 1.2670519351959229\n",
      "epoch: 91, batch: 23, loss: 1.4423904418945312\n",
      "epoch: 91, batch: 24, loss: 1.715226411819458\n",
      "epoch: 91, batch: 25, loss: 1.428760051727295\n",
      "epoch: 91, batch: 26, loss: 1.5552053451538086\n",
      "epoch: 91, batch: 27, loss: 1.5546653270721436\n",
      "epoch: 91, batch: 28, loss: 1.7163759469985962\n",
      "epoch: 91, batch: 29, loss: 1.6537396907806396\n",
      "epoch: 91, batch: 30, loss: 1.5404069423675537\n",
      "epoch: 91, batch: 31, loss: 1.2675392627716064\n",
      "epoch: 91, batch: 32, loss: 1.6025549173355103\n",
      "epoch: 91, batch: 33, loss: 1.4281272888183594\n",
      "epoch: 91, batch: 34, loss: 1.5403896570205688\n",
      "epoch: 91, batch: 35, loss: 1.3148044347763062\n",
      "epoch: 91, batch: 36, loss: 1.702007532119751\n",
      "epoch: 91, batch: 37, loss: 1.9559367895126343\n",
      "epoch: 91, batch: 38, loss: 1.5408728122711182\n",
      "epoch: 91, batch: 39, loss: 1.715942144393921\n",
      "epoch: 91, batch: 40, loss: 1.3152707815170288\n",
      "epoch: 91, batch: 41, loss: 1.5408461093902588\n",
      "epoch: 91, batch: 42, loss: 1.7642719745635986\n",
      "epoch: 91, batch: 43, loss: 1.4280351400375366\n",
      "epoch: 91, batch: 44, loss: 1.5408189296722412\n",
      "epoch: 91, batch: 45, loss: 1.5071094036102295\n",
      "epoch: 91, batch: 46, loss: 1.7023454904556274\n",
      "epoch: 91, batch: 47, loss: 1.5407921075820923\n",
      "epoch: 91, batch: 48, loss: 1.7155717611312866\n",
      "epoch: 91, batch: 49, loss: 1.843499779701233\n",
      "epoch: 91, batch: 50, loss: 1.1541178226470947\n",
      "epoch: 91, batch: 51, loss: 1.1063015460968018\n",
      "epoch: 91, batch: 52, loss: 1.6825382709503174\n",
      "epoch: 91, batch: 53, loss: 1.4761942625045776\n",
      "epoch: 91, batch: 54, loss: 1.5550510883331299\n",
      "epoch: 91, batch: 55, loss: 1.4422709941864014\n",
      "epoch: 91, batch: 56, loss: 1.8432776927947998\n",
      "epoch: 91, batch: 57, loss: 1.3318623304367065\n",
      "epoch: 91, batch: 58, loss: 1.6539074182510376\n",
      "epoch: 91, batch: 59, loss: 1.5410994291305542\n",
      "epoch: 91, batch: 60, loss: 1.3796567916870117\n",
      "epoch: 91, batch: 61, loss: 1.7310590744018555\n",
      "epoch: 91, batch: 62, loss: 1.5406548976898193\n",
      "epoch: 91, batch: 63, loss: 1.2672477960586548\n",
      "epoch: 91, batch: 64, loss: 1.5884453058242798\n",
      "epoch: 91, batch: 65, loss: 1.427832841873169\n",
      "epoch: 91, batch: 66, loss: 1.5888159275054932\n",
      "epoch: 91, batch: 67, loss: 1.3946329355239868\n",
      "epoch: 91, batch: 68, loss: 1.2668216228485107\n",
      "epoch: 91, batch: 69, loss: 1.2818673849105835\n",
      "epoch: 91, batch: 70, loss: 1.266811728477478\n",
      "epoch: 91, batch: 71, loss: 1.4281466007232666\n",
      "epoch: 91, batch: 72, loss: 1.3795948028564453\n",
      "epoch: 91, batch: 73, loss: 1.636174201965332\n",
      "epoch: 91, batch: 74, loss: 1.379940152168274\n",
      "epoch: 91, batch: 75, loss: 1.379930853843689\n",
      "epoch: 91, batch: 76, loss: 1.668503999710083\n",
      "epoch: 91, batch: 77, loss: 1.2671195268630981\n",
      "epoch: 91, batch: 78, loss: 2.052769422531128\n",
      "epoch: 91, batch: 79, loss: 1.4429494142532349\n",
      "epoch: 91, batch: 80, loss: 1.4429595470428467\n",
      "epoch: 91, batch: 81, loss: 1.4426428079605103\n",
      "epoch: 91, batch: 82, loss: 1.2189359664916992\n",
      "epoch: 91, batch: 83, loss: 1.282065510749817\n",
      "epoch: 91, batch: 84, loss: 1.3795311450958252\n",
      "epoch: 91, batch: 85, loss: 1.5885683298110962\n",
      "epoch: 91, batch: 86, loss: 1.7645403146743774\n",
      "epoch: 91, batch: 87, loss: 1.5555189847946167\n",
      "epoch: 91, batch: 88, loss: 1.6039447784423828\n",
      "epoch: 91, batch: 89, loss: 1.267006278038025\n",
      "epoch: 91, batch: 90, loss: 1.5885045528411865\n",
      "epoch: 91, batch: 91, loss: 1.4754178524017334\n",
      "epoch: 91, batch: 92, loss: 1.2669775485992432\n",
      "epoch: 91, batch: 93, loss: 1.2666932344436646\n",
      "epoch: 91, batch: 94, loss: 1.5403614044189453\n",
      "epoch: 91, batch: 95, loss: 1.6037204265594482\n",
      "epoch: 91, batch: 96, loss: 1.5403435230255127\n",
      "epoch: 91, batch: 97, loss: 1.6039974689483643\n",
      "epoch: 91, batch: 98, loss: 1.8449335098266602\n",
      "epoch: 91, batch: 99, loss: 1.2666622400283813\n",
      "epoch: 91, batch: 100, loss: 1.620667815208435\n",
      "epoch: 91, batch: 101, loss: 1.6037819385528564\n",
      "epoch: 91, batch: 102, loss: 1.5405240058898926\n",
      "epoch: 91, batch: 103, loss: 1.3144699335098267\n",
      "epoch: 91, batch: 104, loss: 1.5557594299316406\n",
      "epoch: 91, batch: 105, loss: 1.5079437494277954\n",
      "epoch: 91, batch: 106, loss: 1.7166252136230469\n",
      "epoch: 91, batch: 107, loss: 1.314662218093872\n",
      "epoch: 91, batch: 108, loss: 1.314857840538025\n",
      "epoch: 91, batch: 109, loss: 1.4756702184677124\n",
      "epoch: 91, batch: 110, loss: 1.3793981075286865\n",
      "epoch: 91, batch: 111, loss: 1.7647030353546143\n",
      "epoch: 91, batch: 112, loss: 1.4275953769683838\n",
      "epoch: 91, batch: 113, loss: 1.6039047241210938\n",
      "epoch: 91, batch: 114, loss: 1.7325999736785889\n",
      "epoch: 91, batch: 115, loss: 1.556086540222168\n",
      "epoch: 91, batch: 116, loss: 1.4755445718765259\n",
      "epoch: 91, batch: 117, loss: 1.4923198223114014\n",
      "epoch: 91, batch: 118, loss: 1.877232551574707\n",
      "epoch: 91, batch: 119, loss: 1.3953351974487305\n",
      "epoch: 91, batch: 120, loss: 1.4753260612487793\n",
      "epoch: 91, batch: 121, loss: 1.7817347049713135\n",
      "epoch: 91, batch: 122, loss: 1.877301573753357\n",
      "epoch: 91, batch: 123, loss: 1.635674238204956\n",
      "epoch: 91, batch: 124, loss: 1.5881931781768799\n",
      "epoch: 91, batch: 125, loss: 1.4272940158843994\n",
      "epoch: 91, batch: 126, loss: 1.4921236038208008\n",
      "epoch: 91, batch: 127, loss: 1.1340715885162354\n",
      "epoch: 92, batch: 0, loss: 1.7169559001922607\n",
      "epoch: 92, batch: 1, loss: 1.4273536205291748\n",
      "epoch: 92, batch: 2, loss: 1.5401403903961182\n",
      "epoch: 92, batch: 3, loss: 1.1537851095199585\n",
      "epoch: 92, batch: 4, loss: 1.5562411546707153\n",
      "epoch: 92, batch: 5, loss: 1.540097951889038\n",
      "epoch: 92, batch: 6, loss: 1.2665553092956543\n",
      "epoch: 92, batch: 7, loss: 1.5562679767608643\n",
      "epoch: 92, batch: 8, loss: 1.266534686088562\n",
      "epoch: 92, batch: 9, loss: 1.266524076461792\n",
      "epoch: 92, batch: 10, loss: 1.6041557788848877\n",
      "epoch: 92, batch: 11, loss: 1.2186025381088257\n",
      "epoch: 92, batch: 12, loss: 1.4271923303604126\n",
      "epoch: 92, batch: 13, loss: 1.6691253185272217\n",
      "epoch: 92, batch: 14, loss: 1.7333825826644897\n",
      "epoch: 92, batch: 15, loss: 1.4750151634216309\n",
      "epoch: 92, batch: 16, loss: 1.4271312952041626\n",
      "epoch: 92, batch: 17, loss: 1.7170330286026\n",
      "epoch: 92, batch: 18, loss: 1.7170432806015015\n",
      "epoch: 92, batch: 19, loss: 1.6691919565200806\n",
      "epoch: 92, batch: 20, loss: 1.4441627264022827\n",
      "epoch: 92, batch: 21, loss: 1.508535623550415\n",
      "epoch: 92, batch: 22, loss: 1.5563957691192627\n",
      "epoch: 92, batch: 23, loss: 1.4920294284820557\n",
      "epoch: 92, batch: 24, loss: 1.42710280418396\n",
      "epoch: 92, batch: 25, loss: 1.3792129755020142\n",
      "epoch: 92, batch: 26, loss: 1.7483736276626587\n",
      "epoch: 92, batch: 27, loss: 1.556505560874939\n",
      "epoch: 92, batch: 28, loss: 1.9593830108642578\n",
      "epoch: 92, batch: 29, loss: 1.3142015933990479\n",
      "epoch: 92, batch: 30, loss: 1.4269120693206787\n",
      "epoch: 92, batch: 31, loss: 1.4269936084747314\n",
      "epoch: 92, batch: 32, loss: 1.6043819189071655\n",
      "epoch: 92, batch: 33, loss: 1.218462586402893\n",
      "epoch: 92, batch: 34, loss: 1.7964341640472412\n",
      "epoch: 92, batch: 35, loss: 1.2663614749908447\n",
      "epoch: 92, batch: 36, loss: 1.7651488780975342\n",
      "epoch: 92, batch: 37, loss: 1.4437083005905151\n",
      "epoch: 92, batch: 38, loss: 1.2663482427597046\n",
      "epoch: 92, batch: 39, loss: 1.686295747756958\n",
      "epoch: 92, batch: 40, loss: 1.2184146642684937\n",
      "epoch: 92, batch: 41, loss: 1.2661701440811157\n",
      "epoch: 92, batch: 42, loss: 1.7480316162109375\n",
      "epoch: 92, batch: 43, loss: 1.4270671606063843\n",
      "epoch: 92, batch: 44, loss: 1.314255952835083\n",
      "epoch: 92, batch: 45, loss: 1.7822017669677734\n",
      "epoch: 92, batch: 46, loss: 1.4268629550933838\n",
      "epoch: 92, batch: 47, loss: 1.4917221069335938\n",
      "epoch: 92, batch: 48, loss: 1.3789021968841553\n",
      "epoch: 92, batch: 49, loss: 1.4268367290496826\n",
      "epoch: 92, batch: 50, loss: 1.314018964767456\n",
      "epoch: 92, batch: 51, loss: 1.313780665397644\n",
      "epoch: 92, batch: 52, loss: 1.283307433128357\n",
      "epoch: 92, batch: 53, loss: 1.5393683910369873\n",
      "epoch: 92, batch: 54, loss: 1.830234169960022\n",
      "epoch: 92, batch: 55, loss: 1.621771216392517\n",
      "epoch: 92, batch: 56, loss: 2.056079387664795\n",
      "epoch: 92, batch: 57, loss: 1.5569522380828857\n",
      "epoch: 92, batch: 58, loss: 1.314225435256958\n",
      "epoch: 92, batch: 59, loss: 1.105474591255188\n",
      "epoch: 92, batch: 60, loss: 1.764885663986206\n",
      "epoch: 92, batch: 61, loss: 1.4747083187103271\n",
      "epoch: 92, batch: 62, loss: 1.6695327758789062\n",
      "epoch: 92, batch: 63, loss: 1.5871962308883667\n",
      "epoch: 92, batch: 64, loss: 1.6047242879867554\n",
      "epoch: 92, batch: 65, loss: 1.9266431331634521\n",
      "epoch: 92, batch: 66, loss: 1.1530855894088745\n",
      "epoch: 92, batch: 67, loss: 1.5567657947540283\n",
      "epoch: 92, batch: 68, loss: 1.443966269493103\n",
      "epoch: 92, batch: 69, loss: 1.4746592044830322\n",
      "epoch: 92, batch: 70, loss: 1.4266549348831177\n",
      "epoch: 92, batch: 71, loss: 1.9264154434204102\n",
      "epoch: 92, batch: 72, loss: 1.5394461154937744\n",
      "epoch: 92, batch: 73, loss: 1.378624677658081\n",
      "epoch: 92, batch: 74, loss: 1.1053693294525146\n",
      "epoch: 92, batch: 75, loss: 1.6350520849227905\n",
      "epoch: 92, batch: 76, loss: 1.1533664464950562\n",
      "epoch: 92, batch: 77, loss: 1.5870181322097778\n",
      "epoch: 92, batch: 78, loss: 1.426990270614624\n",
      "epoch: 92, batch: 79, loss: 1.491370439529419\n",
      "epoch: 92, batch: 80, loss: 1.717703104019165\n",
      "epoch: 92, batch: 81, loss: 1.4265594482421875\n",
      "epoch: 92, batch: 82, loss: 1.5393614768981934\n",
      "epoch: 92, batch: 83, loss: 1.2657073736190796\n",
      "epoch: 92, batch: 84, loss: 1.8481212854385376\n",
      "epoch: 92, batch: 85, loss: 1.26568603515625\n",
      "epoch: 92, batch: 86, loss: 1.2656749486923218\n",
      "epoch: 92, batch: 87, loss: 1.6521320343017578\n",
      "epoch: 92, batch: 88, loss: 1.2180877923965454\n",
      "epoch: 92, batch: 89, loss: 1.5868637561798096\n",
      "epoch: 92, batch: 90, loss: 1.5574395656585693\n",
      "epoch: 92, batch: 91, loss: 1.4749953746795654\n",
      "epoch: 92, batch: 92, loss: 1.58780837059021\n",
      "epoch: 92, batch: 93, loss: 1.4739984273910522\n",
      "epoch: 92, batch: 94, loss: 1.2655885219573975\n",
      "epoch: 92, batch: 95, loss: 1.5387418270111084\n",
      "epoch: 92, batch: 96, loss: 1.7476356029510498\n",
      "epoch: 92, batch: 97, loss: 1.3136050701141357\n",
      "epoch: 92, batch: 98, loss: 1.670372724533081\n",
      "epoch: 92, batch: 99, loss: 1.313586950302124\n",
      "epoch: 92, batch: 100, loss: 1.2660647630691528\n",
      "epoch: 92, batch: 101, loss: 1.7184734344482422\n",
      "epoch: 92, batch: 102, loss: 1.1532394886016846\n",
      "epoch: 92, batch: 103, loss: 1.9139697551727295\n",
      "epoch: 92, batch: 104, loss: 1.4263585805892944\n",
      "epoch: 92, batch: 105, loss: 1.8782777786254883\n",
      "epoch: 92, batch: 106, loss: 1.8788601160049438\n",
      "epoch: 92, batch: 107, loss: 1.8308079242706299\n",
      "epoch: 92, batch: 108, loss: 1.670522689819336\n",
      "epoch: 92, batch: 109, loss: 1.2654294967651367\n",
      "epoch: 92, batch: 110, loss: 1.8494508266448975\n",
      "epoch: 92, batch: 111, loss: 1.444326639175415\n",
      "epoch: 92, batch: 112, loss: 1.6052253246307373\n",
      "epoch: 92, batch: 113, loss: 1.849552869796753\n",
      "epoch: 92, batch: 114, loss: 1.6519079208374023\n",
      "epoch: 92, batch: 115, loss: 1.2840940952301025\n",
      "epoch: 92, batch: 116, loss: 1.313434362411499\n",
      "epoch: 92, batch: 117, loss: 1.7187340259552002\n",
      "epoch: 92, batch: 118, loss: 1.6888233423233032\n",
      "epoch: 92, batch: 119, loss: 1.473659873008728\n",
      "epoch: 92, batch: 120, loss: 1.6046552658081055\n",
      "epoch: 92, batch: 121, loss: 1.5396883487701416\n",
      "epoch: 92, batch: 122, loss: 1.8966137170791626\n",
      "epoch: 92, batch: 123, loss: 1.699918508529663\n",
      "epoch: 92, batch: 124, loss: 1.7181707620620728\n",
      "epoch: 92, batch: 125, loss: 1.6060407161712646\n",
      "epoch: 92, batch: 126, loss: 1.313344955444336\n",
      "epoch: 92, batch: 127, loss: 1.6985621452331543\n",
      "epoch: 93, batch: 0, loss: 1.8493640422821045\n",
      "epoch: 93, batch: 1, loss: 1.4261395931243896\n",
      "epoch: 93, batch: 2, loss: 1.9813144207000732\n",
      "epoch: 93, batch: 3, loss: 1.4261223077774048\n",
      "epoch: 93, batch: 4, loss: 1.153088927268982\n",
      "epoch: 93, batch: 5, loss: 1.5870246887207031\n",
      "epoch: 93, batch: 6, loss: 1.670905351638794\n",
      "epoch: 93, batch: 7, loss: 1.5862776041030884\n",
      "epoch: 93, batch: 8, loss: 1.2177929878234863\n",
      "epoch: 93, batch: 9, loss: 1.7005615234375\n",
      "epoch: 93, batch: 10, loss: 1.7471659183502197\n",
      "epoch: 93, batch: 11, loss: 1.8504295349121094\n",
      "epoch: 93, batch: 12, loss: 1.313218116760254\n",
      "epoch: 93, batch: 13, loss: 1.634305715560913\n",
      "epoch: 93, batch: 14, loss: 1.7175929546356201\n",
      "epoch: 93, batch: 15, loss: 1.4260175228118896\n",
      "epoch: 93, batch: 16, loss: 1.490730881690979\n",
      "epoch: 93, batch: 17, loss: 1.377894401550293\n",
      "epoch: 93, batch: 18, loss: 1.4733119010925293\n",
      "epoch: 93, batch: 19, loss: 1.5582677125930786\n",
      "epoch: 93, batch: 20, loss: 1.5101745128631592\n",
      "epoch: 93, batch: 21, loss: 2.0101003646850586\n",
      "epoch: 93, batch: 22, loss: 1.3131272792816162\n",
      "epoch: 93, batch: 23, loss: 1.4259477853775024\n",
      "epoch: 93, batch: 24, loss: 1.3786437511444092\n",
      "epoch: 93, batch: 25, loss: 1.4724022150039673\n",
      "epoch: 93, batch: 26, loss: 1.6056544780731201\n",
      "epoch: 93, batch: 27, loss: 1.9637949466705322\n",
      "epoch: 93, batch: 28, loss: 1.4731820821762085\n",
      "epoch: 93, batch: 29, loss: 1.510298490524292\n",
      "epoch: 93, batch: 30, loss: 1.5575817823410034\n",
      "epoch: 93, batch: 31, loss: 1.6704227924346924\n",
      "epoch: 93, batch: 32, loss: 1.330493688583374\n",
      "epoch: 93, batch: 33, loss: 1.4739755392074585\n",
      "epoch: 93, batch: 34, loss: 1.2176507711410522\n",
      "epoch: 93, batch: 35, loss: 1.3777261972427368\n",
      "epoch: 93, batch: 36, loss: 1.6987438201904297\n",
      "epoch: 93, batch: 37, loss: 1.4456932544708252\n",
      "epoch: 93, batch: 38, loss: 1.284754753112793\n",
      "epoch: 93, batch: 39, loss: 1.7376444339752197\n",
      "epoch: 93, batch: 40, loss: 1.4748151302337646\n",
      "epoch: 93, batch: 41, loss: 1.6339702606201172\n",
      "epoch: 93, batch: 42, loss: 1.4457645416259766\n",
      "epoch: 93, batch: 43, loss: 1.5395171642303467\n",
      "epoch: 93, batch: 44, loss: 1.3776394128799438\n",
      "epoch: 93, batch: 45, loss: 1.6705560684204102\n",
      "epoch: 93, batch: 46, loss: 2.0107131004333496\n",
      "epoch: 93, batch: 47, loss: 1.3785405158996582\n",
      "epoch: 93, batch: 48, loss: 2.03088116645813\n",
      "epoch: 93, batch: 49, loss: 1.6329374313354492\n",
      "epoch: 93, batch: 50, loss: 1.6049537658691406\n",
      "epoch: 93, batch: 51, loss: 1.3785223960876465\n",
      "epoch: 93, batch: 52, loss: 1.3119041919708252\n",
      "epoch: 93, batch: 53, loss: 1.4247262477874756\n",
      "epoch: 93, batch: 54, loss: 1.6059401035308838\n",
      "epoch: 93, batch: 55, loss: 1.3138048648834229\n",
      "epoch: 93, batch: 56, loss: 1.5587996244430542\n",
      "epoch: 93, batch: 57, loss: 0.9918456077575684\n",
      "epoch: 93, batch: 58, loss: 1.312806487083435\n",
      "epoch: 93, batch: 59, loss: 1.718828797340393\n",
      "epoch: 93, batch: 60, loss: 1.3774851560592651\n",
      "epoch: 93, batch: 61, loss: 1.4450290203094482\n",
      "epoch: 93, batch: 62, loss: 1.313779592514038\n",
      "epoch: 93, batch: 63, loss: 1.623596429824829\n",
      "epoch: 93, batch: 64, loss: 1.8798675537109375\n",
      "epoch: 93, batch: 65, loss: 1.2656219005584717\n",
      "epoch: 93, batch: 66, loss: 1.5589442253112793\n",
      "epoch: 93, batch: 67, loss: 1.377418041229248\n",
      "epoch: 93, batch: 68, loss: 1.5373587608337402\n",
      "epoch: 93, batch: 69, loss: 1.3784469366073608\n",
      "epoch: 93, batch: 70, loss: 1.8328421115875244\n",
      "epoch: 93, batch: 71, loss: 1.2655937671661377\n",
      "epoch: 93, batch: 72, loss: 1.3302810192108154\n",
      "epoch: 93, batch: 73, loss: 1.3212151527404785\n",
      "epoch: 93, batch: 74, loss: 1.6072149276733398\n",
      "epoch: 93, batch: 75, loss: 1.2655744552612305\n",
      "epoch: 93, batch: 76, loss: 1.6237773895263672\n",
      "epoch: 93, batch: 77, loss: 1.4725494384765625\n",
      "epoch: 93, batch: 78, loss: 1.4254724979400635\n",
      "epoch: 93, batch: 79, loss: 1.5853731632232666\n",
      "epoch: 93, batch: 80, loss: 1.3126062154769897\n",
      "epoch: 93, batch: 81, loss: 1.4243354797363281\n",
      "epoch: 93, batch: 82, loss: 1.4254385232925415\n",
      "epoch: 93, batch: 83, loss: 1.606231927871704\n",
      "epoch: 93, batch: 84, loss: 1.473589539527893\n",
      "epoch: 93, batch: 85, loss: 2.012794017791748\n",
      "epoch: 93, batch: 86, loss: 1.5790660381317139\n",
      "epoch: 93, batch: 87, loss: 1.8790054321289062\n",
      "epoch: 93, batch: 88, loss: 1.5864125490188599\n",
      "epoch: 93, batch: 89, loss: 1.1526525020599365\n",
      "epoch: 93, batch: 90, loss: 1.4900484085083008\n",
      "epoch: 93, batch: 91, loss: 1.6063134670257568\n",
      "epoch: 93, batch: 92, loss: 1.2854293584823608\n",
      "epoch: 93, batch: 93, loss: 1.446474313735962\n",
      "epoch: 93, batch: 94, loss: 1.3783352375030518\n",
      "epoch: 93, batch: 95, loss: 1.1044394969940186\n",
      "epoch: 93, batch: 96, loss: 1.6321611404418945\n",
      "epoch: 93, batch: 97, loss: 1.6063743829727173\n",
      "epoch: 93, batch: 98, loss: 1.5112162828445435\n",
      "epoch: 93, batch: 99, loss: 1.585132360458374\n",
      "epoch: 93, batch: 100, loss: 1.1525912284851074\n",
      "epoch: 93, batch: 101, loss: 1.671087622642517\n",
      "epoch: 93, batch: 102, loss: 2.0133004188537598\n",
      "epoch: 93, batch: 103, loss: 1.879120111465454\n",
      "epoch: 93, batch: 104, loss: 1.4240219593048096\n",
      "epoch: 93, batch: 105, loss: 1.1525622606277466\n",
      "epoch: 93, batch: 106, loss: 1.4709464311599731\n",
      "epoch: 93, batch: 107, loss: 1.6979007720947266\n",
      "epoch: 93, batch: 108, loss: 1.6991395950317383\n",
      "epoch: 93, batch: 109, loss: 1.65093994140625\n",
      "epoch: 93, batch: 110, loss: 1.5380666255950928\n",
      "epoch: 93, batch: 111, loss: 1.4467161893844604\n",
      "epoch: 93, batch: 112, loss: 1.6344448328018188\n",
      "epoch: 93, batch: 113, loss: 1.5836883783340454\n",
      "epoch: 93, batch: 114, loss: 1.3135786056518555\n",
      "epoch: 93, batch: 115, loss: 1.5380277633666992\n",
      "epoch: 93, batch: 116, loss: 1.559654951095581\n",
      "epoch: 93, batch: 117, loss: 1.104292392730713\n",
      "epoch: 93, batch: 118, loss: 1.7854282855987549\n",
      "epoch: 93, batch: 119, loss: 1.42643141746521\n",
      "epoch: 93, batch: 120, loss: 1.6039724349975586\n",
      "epoch: 93, batch: 121, loss: 1.4455333948135376\n",
      "epoch: 93, batch: 122, loss: 1.7841675281524658\n",
      "epoch: 93, batch: 123, loss: 1.8323837518692017\n",
      "epoch: 93, batch: 124, loss: 1.31087064743042\n",
      "epoch: 93, batch: 125, loss: 1.4719412326812744\n",
      "epoch: 93, batch: 126, loss: 1.3121862411499023\n",
      "epoch: 93, batch: 127, loss: 1.3940695524215698\n",
      "epoch: 94, batch: 0, loss: 1.5584720373153687\n",
      "epoch: 94, batch: 1, loss: 1.4718902111053467\n",
      "epoch: 94, batch: 2, loss: 1.263938069343567\n",
      "epoch: 94, batch: 3, loss: 1.5365321636199951\n",
      "epoch: 94, batch: 4, loss: 1.4236384630203247\n",
      "epoch: 94, batch: 5, loss: 1.5378848314285278\n",
      "epoch: 94, batch: 6, loss: 1.104185700416565\n",
      "epoch: 94, batch: 7, loss: 1.854574203491211\n",
      "epoch: 94, batch: 8, loss: 1.8546078205108643\n",
      "epoch: 94, batch: 9, loss: 1.378151297569275\n",
      "epoch: 94, batch: 10, loss: 1.5585639476776123\n",
      "epoch: 94, batch: 11, loss: 1.7210878133773804\n",
      "epoch: 94, batch: 12, loss: 1.1509535312652588\n",
      "epoch: 94, batch: 13, loss: 1.6068134307861328\n",
      "epoch: 94, batch: 14, loss: 1.424926996231079\n",
      "epoch: 94, batch: 15, loss: 1.3161695003509521\n",
      "epoch: 94, batch: 16, loss: 1.8532512187957764\n",
      "epoch: 94, batch: 17, loss: 1.7197412252426147\n",
      "epoch: 94, batch: 18, loss: 1.6715242862701416\n",
      "epoch: 94, batch: 19, loss: 1.3766558170318604\n",
      "epoch: 94, batch: 20, loss: 1.4472240209579468\n",
      "epoch: 94, batch: 21, loss: 1.283961534500122\n",
      "epoch: 94, batch: 22, loss: 1.8312156200408936\n",
      "epoch: 94, batch: 23, loss: 1.7183310985565186\n",
      "epoch: 94, batch: 24, loss: 1.2651963233947754\n",
      "epoch: 94, batch: 25, loss: 1.4472935199737549\n",
      "epoch: 94, batch: 26, loss: 1.7198371887207031\n",
      "epoch: 94, batch: 27, loss: 1.4473210573196411\n",
      "epoch: 94, batch: 28, loss: 1.4233075380325317\n",
      "epoch: 94, batch: 29, loss: 1.4715356826782227\n",
      "epoch: 94, batch: 30, loss: 1.7666114568710327\n",
      "epoch: 94, batch: 31, loss: 1.6505699157714844\n",
      "epoch: 94, batch: 32, loss: 1.445867896080017\n",
      "epoch: 94, batch: 33, loss: 1.265153169631958\n",
      "epoch: 94, batch: 34, loss: 1.4247580766677856\n",
      "epoch: 94, batch: 35, loss: 1.5376447439193726\n",
      "epoch: 94, batch: 36, loss: 1.2635936737060547\n",
      "epoch: 94, batch: 37, loss: 1.6732524633407593\n",
      "epoch: 94, batch: 38, loss: 1.2651293277740479\n",
      "epoch: 94, batch: 39, loss: 1.6717197895050049\n",
      "epoch: 94, batch: 40, loss: 1.6309858560562134\n",
      "epoch: 94, batch: 41, loss: 1.4246994256973267\n",
      "epoch: 94, batch: 42, loss: 1.4729526042938232\n",
      "epoch: 94, batch: 43, loss: 1.3992809057235718\n",
      "epoch: 94, batch: 44, loss: 1.535982370376587\n",
      "epoch: 94, batch: 45, loss: 1.3779960870742798\n",
      "epoch: 94, batch: 46, loss: 2.038374423980713\n",
      "epoch: 94, batch: 47, loss: 1.3993303775787354\n",
      "epoch: 94, batch: 48, loss: 1.4459999799728394\n",
      "epoch: 94, batch: 49, loss: 1.3117308616638184\n",
      "epoch: 94, batch: 50, loss: 1.1038963794708252\n",
      "epoch: 94, batch: 51, loss: 1.2634366750717163\n",
      "epoch: 94, batch: 52, loss: 1.7683944702148438\n",
      "epoch: 94, batch: 53, loss: 1.7415682077407837\n",
      "epoch: 94, batch: 54, loss: 1.2650539875030518\n",
      "epoch: 94, batch: 55, loss: 1.4262373447418213\n",
      "epoch: 94, batch: 56, loss: 1.422912836074829\n",
      "epoch: 94, batch: 57, loss: 1.4908515214920044\n",
      "epoch: 94, batch: 58, loss: 1.4262301921844482\n",
      "epoch: 94, batch: 59, loss: 1.720192313194275\n",
      "epoch: 94, batch: 60, loss: 1.1038248538970947\n",
      "epoch: 94, batch: 61, loss: 1.2633297443389893\n",
      "epoch: 94, batch: 62, loss: 1.4461102485656738\n",
      "epoch: 94, batch: 63, loss: 1.854748010635376\n",
      "epoch: 94, batch: 64, loss: 1.741864800453186\n",
      "epoch: 94, batch: 65, loss: 1.377912163734436\n",
      "epoch: 94, batch: 66, loss: 1.329604983329773\n",
      "epoch: 94, batch: 67, loss: 1.5356700420379639\n",
      "epoch: 94, batch: 68, loss: 1.4244744777679443\n",
      "epoch: 94, batch: 69, loss: 1.6091245412826538\n",
      "epoch: 94, batch: 70, loss: 1.7686231136322021\n",
      "epoch: 94, batch: 71, loss: 1.3996152877807617\n",
      "epoch: 94, batch: 72, loss: 1.4226858615875244\n",
      "epoch: 94, batch: 73, loss: 1.6737903356552124\n",
      "epoch: 94, batch: 74, loss: 1.6286780834197998\n",
      "epoch: 94, batch: 75, loss: 1.4244170188903809\n",
      "epoch: 94, batch: 76, loss: 1.4244086742401123\n",
      "epoch: 94, batch: 77, loss: 1.4727261066436768\n",
      "epoch: 94, batch: 78, loss: 1.7221930027008057\n",
      "epoch: 94, batch: 79, loss: 1.537306785583496\n",
      "epoch: 94, batch: 80, loss: 1.743276834487915\n",
      "epoch: 94, batch: 81, loss: 1.7441282272338867\n",
      "epoch: 94, batch: 82, loss: 1.7432472705841064\n",
      "epoch: 94, batch: 83, loss: 1.424351453781128\n",
      "epoch: 94, batch: 84, loss: 1.4708538055419922\n",
      "epoch: 94, batch: 85, loss: 1.5390976667404175\n",
      "epoch: 94, batch: 86, loss: 1.5610579252243042\n",
      "epoch: 94, batch: 87, loss: 1.7223469018936157\n",
      "epoch: 94, batch: 88, loss: 1.720510482788086\n",
      "epoch: 94, batch: 89, loss: 1.7688703536987305\n",
      "epoch: 94, batch: 90, loss: 1.5837106704711914\n",
      "epoch: 94, batch: 91, loss: 1.4242852926254272\n",
      "epoch: 94, batch: 92, loss: 1.7670304775238037\n",
      "epoch: 94, batch: 93, loss: 1.1500451564788818\n",
      "epoch: 94, batch: 94, loss: 1.5371956825256348\n",
      "epoch: 94, batch: 95, loss: 1.4242521524429321\n",
      "epoch: 94, batch: 96, loss: 1.650118112564087\n",
      "epoch: 94, batch: 97, loss: 1.4725959300994873\n",
      "epoch: 94, batch: 98, loss: 1.8335578441619873\n",
      "epoch: 94, batch: 99, loss: 1.6319668292999268\n",
      "epoch: 94, batch: 100, loss: 1.6520153284072876\n",
      "epoch: 94, batch: 101, loss: 1.7871567010879517\n",
      "epoch: 94, batch: 102, loss: 1.4241936206817627\n",
      "epoch: 94, batch: 103, loss: 1.1035047769546509\n",
      "epoch: 94, batch: 104, loss: 1.4705994129180908\n",
      "epoch: 94, batch: 105, loss: 1.5351624488830566\n",
      "epoch: 94, batch: 106, loss: 1.722660779953003\n",
      "epoch: 94, batch: 107, loss: 1.5593981742858887\n",
      "epoch: 94, batch: 108, loss: 1.1498774290084839\n",
      "epoch: 94, batch: 109, loss: 1.5390552282333374\n",
      "epoch: 94, batch: 110, loss: 1.769120454788208\n",
      "epoch: 94, batch: 111, loss: 1.7187747955322266\n",
      "epoch: 94, batch: 112, loss: 1.8337186574935913\n",
      "epoch: 94, batch: 113, loss: 1.1498240232467651\n",
      "epoch: 94, batch: 114, loss: 1.5130841732025146\n",
      "epoch: 94, batch: 115, loss: 1.424084186553955\n",
      "epoch: 94, batch: 116, loss: 1.4240758419036865\n",
      "epoch: 94, batch: 117, loss: 1.7432901859283447\n",
      "epoch: 94, batch: 118, loss: 1.4260790348052979\n",
      "epoch: 94, batch: 119, loss: 1.561532735824585\n",
      "epoch: 94, batch: 120, loss: 1.4002107381820679\n",
      "epoch: 94, batch: 121, loss: 1.424033761024475\n",
      "epoch: 94, batch: 122, loss: 1.4906400442123413\n",
      "epoch: 94, batch: 123, loss: 1.4906368255615234\n",
      "epoch: 94, batch: 124, loss: 1.7208960056304932\n",
      "epoch: 94, batch: 125, loss: 1.4127686023712158\n",
      "epoch: 94, batch: 126, loss: 1.4002883434295654\n",
      "epoch: 94, batch: 127, loss: 1.5928633213043213\n",
      "epoch: 95, batch: 0, loss: 1.6100496053695679\n",
      "epoch: 95, batch: 1, loss: 1.2626173496246338\n",
      "epoch: 95, batch: 2, loss: 1.536919355392456\n",
      "epoch: 95, batch: 3, loss: 1.928625464439392\n",
      "epoch: 95, batch: 4, loss: 1.849369764328003\n",
      "epoch: 95, batch: 5, loss: 1.605932593345642\n",
      "epoch: 95, batch: 6, loss: 1.5852795839309692\n",
      "epoch: 95, batch: 7, loss: 1.3109514713287354\n",
      "epoch: 95, batch: 8, loss: 1.836098313331604\n",
      "epoch: 95, batch: 9, loss: 1.4238990545272827\n",
      "epoch: 95, batch: 10, loss: 1.1495649814605713\n",
      "epoch: 95, batch: 11, loss: 1.4004580974578857\n",
      "epoch: 95, batch: 12, loss: 1.718937873840332\n",
      "epoch: 95, batch: 13, loss: 1.470126748085022\n",
      "epoch: 95, batch: 14, loss: 1.608117699623108\n",
      "epoch: 95, batch: 15, loss: 1.151656150817871\n",
      "epoch: 95, batch: 16, loss: 1.5597435235977173\n",
      "epoch: 95, batch: 17, loss: 1.5619029998779297\n",
      "epoch: 95, batch: 18, loss: 1.2162171602249146\n",
      "epoch: 95, batch: 19, loss: 1.5619313716888428\n",
      "epoch: 95, batch: 20, loss: 1.7394983768463135\n",
      "epoch: 95, batch: 21, loss: 1.3108232021331787\n",
      "epoch: 95, batch: 22, loss: 1.4259612560272217\n",
      "epoch: 95, batch: 23, loss: 1.672787070274353\n",
      "epoch: 95, batch: 24, loss: 1.4468438625335693\n",
      "epoch: 95, batch: 25, loss: 1.5389270782470703\n",
      "epoch: 95, batch: 26, loss: 1.262381911277771\n",
      "epoch: 95, batch: 27, loss: 1.472141981124878\n",
      "epoch: 95, batch: 28, loss: 1.7718286514282227\n",
      "epoch: 95, batch: 29, loss: 1.6728500127792358\n",
      "epoch: 95, batch: 30, loss: 1.6982429027557373\n",
      "epoch: 95, batch: 31, loss: 1.103168249130249\n",
      "epoch: 95, batch: 32, loss: 1.4259202480316162\n",
      "epoch: 95, batch: 33, loss: 1.310712218284607\n",
      "epoch: 95, batch: 34, loss: 1.6980540752410889\n",
      "epoch: 95, batch: 35, loss: 1.4904975891113281\n",
      "epoch: 95, batch: 36, loss: 1.3291116952896118\n",
      "epoch: 95, batch: 37, loss: 1.8114547729492188\n",
      "epoch: 95, batch: 38, loss: 1.5621967315673828\n",
      "epoch: 95, batch: 39, loss: 1.375250220298767\n",
      "epoch: 95, batch: 40, loss: 1.6268177032470703\n",
      "epoch: 95, batch: 41, loss: 1.6752281188964844\n",
      "epoch: 95, batch: 42, loss: 1.721384048461914\n",
      "epoch: 95, batch: 43, loss: 1.472009301185608\n",
      "epoch: 95, batch: 44, loss: 1.4258668422698975\n",
      "epoch: 95, batch: 45, loss: 1.4258620738983154\n",
      "epoch: 95, batch: 46, loss: 1.8601443767547607\n",
      "epoch: 95, batch: 47, loss: 1.5849711894989014\n",
      "epoch: 95, batch: 48, loss: 1.310577154159546\n",
      "epoch: 95, batch: 49, loss: 1.4674010276794434\n",
      "epoch: 95, batch: 50, loss: 1.786065697669983\n",
      "epoch: 95, batch: 51, loss: 1.6084848642349243\n",
      "epoch: 95, batch: 52, loss: 1.610784888267517\n",
      "epoch: 95, batch: 53, loss: 1.3751304149627686\n",
      "epoch: 95, batch: 54, loss: 1.4010168313980103\n",
      "epoch: 95, batch: 55, loss: 1.2621166706085205\n",
      "epoch: 95, batch: 56, loss: 1.3774077892303467\n",
      "epoch: 95, batch: 57, loss: 1.6471855640411377\n",
      "epoch: 95, batch: 58, loss: 1.2159990072250366\n",
      "epoch: 95, batch: 59, loss: 1.9066531658172607\n",
      "epoch: 95, batch: 60, loss: 1.5341542959213257\n",
      "epoch: 95, batch: 61, loss: 1.7215869426727295\n",
      "epoch: 95, batch: 62, loss: 1.4672126770019531\n",
      "epoch: 95, batch: 63, loss: 1.2620455026626587\n",
      "epoch: 95, batch: 64, loss: 1.4495465755462646\n",
      "epoch: 95, batch: 65, loss: 1.6086304187774658\n",
      "epoch: 95, batch: 66, loss: 1.6285583972930908\n",
      "epoch: 95, batch: 67, loss: 1.5549442768096924\n",
      "epoch: 95, batch: 68, loss: 1.606319785118103\n",
      "epoch: 95, batch: 69, loss: 1.721672773361206\n",
      "epoch: 95, batch: 70, loss: 1.5824463367462158\n",
      "epoch: 95, batch: 71, loss: 1.5847886800765991\n",
      "epoch: 95, batch: 72, loss: 1.514245629310608\n",
      "epoch: 95, batch: 73, loss: 1.3103631734848022\n",
      "epoch: 95, batch: 74, loss: 1.8586864471435547\n",
      "epoch: 95, batch: 75, loss: 1.6331701278686523\n",
      "epoch: 95, batch: 76, loss: 1.767787218093872\n",
      "epoch: 95, batch: 77, loss: 1.4497193098068237\n",
      "epoch: 95, batch: 78, loss: 1.5339453220367432\n",
      "epoch: 95, batch: 79, loss: 1.2618993520736694\n",
      "epoch: 95, batch: 80, loss: 1.264277696609497\n",
      "epoch: 95, batch: 81, loss: 1.5603840351104736\n",
      "epoch: 95, batch: 82, loss: 1.420894742012024\n",
      "epoch: 95, batch: 83, loss: 1.4474003314971924\n",
      "epoch: 95, batch: 84, loss: 1.4208701848983765\n",
      "epoch: 95, batch: 85, loss: 1.3772550821304321\n",
      "epoch: 95, batch: 86, loss: 1.5846805572509766\n",
      "epoch: 95, batch: 87, loss: 1.423249363899231\n",
      "epoch: 95, batch: 88, loss: 1.425661563873291\n",
      "epoch: 95, batch: 89, loss: 1.5822350978851318\n",
      "epoch: 95, batch: 90, loss: 1.3077905178070068\n",
      "epoch: 95, batch: 91, loss: 1.7219091653823853\n",
      "epoch: 95, batch: 92, loss: 1.5629268884658813\n",
      "epoch: 95, batch: 93, loss: 1.5629408359527588\n",
      "epoch: 95, batch: 94, loss: 1.2157785892486572\n",
      "epoch: 95, batch: 95, loss: 1.6007702350616455\n",
      "epoch: 95, batch: 96, loss: 1.328773856163025\n",
      "epoch: 95, batch: 97, loss: 1.5629961490631104\n",
      "epoch: 95, batch: 98, loss: 1.628099799156189\n",
      "epoch: 95, batch: 99, loss: 1.4256187677383423\n",
      "epoch: 95, batch: 100, loss: 1.4691025018692017\n",
      "epoch: 95, batch: 101, loss: 1.7195398807525635\n",
      "epoch: 95, batch: 102, loss: 1.7195460796356201\n",
      "epoch: 95, batch: 103, loss: 1.5795912742614746\n",
      "epoch: 95, batch: 104, loss: 1.5606029033660889\n",
      "epoch: 95, batch: 105, loss: 1.1486449241638184\n",
      "epoch: 95, batch: 106, loss: 1.7894313335418701\n",
      "epoch: 95, batch: 107, loss: 1.6516088247299194\n",
      "epoch: 95, batch: 108, loss: 1.4476301670074463\n",
      "epoch: 95, batch: 109, loss: 1.5385929346084595\n",
      "epoch: 95, batch: 110, loss: 1.7195947170257568\n",
      "epoch: 95, batch: 111, loss: 1.6304309368133545\n",
      "epoch: 95, batch: 112, loss: 1.3771255016326904\n",
      "epoch: 95, batch: 113, loss: 1.1026508808135986\n",
      "epoch: 95, batch: 114, loss: 1.468939185142517\n",
      "epoch: 95, batch: 115, loss: 1.1485463380813599\n",
      "epoch: 95, batch: 116, loss: 1.2640960216522217\n",
      "epoch: 95, batch: 117, loss: 1.7706420421600342\n",
      "epoch: 95, batch: 118, loss: 1.70102858543396\n",
      "epoch: 95, batch: 119, loss: 1.8625282049179077\n",
      "epoch: 95, batch: 120, loss: 1.5844483375549316\n",
      "epoch: 95, batch: 121, loss: 1.609218955039978\n",
      "epoch: 95, batch: 122, loss: 1.676361083984375\n",
      "epoch: 95, batch: 123, loss: 1.4503484964370728\n",
      "epoch: 95, batch: 124, loss: 1.5359598398208618\n",
      "epoch: 95, batch: 125, loss: 1.4203447103500366\n",
      "epoch: 95, batch: 126, loss: 1.4713929891586304\n",
      "epoch: 95, batch: 127, loss: 1.565617322921753\n",
      "epoch: 96, batch: 0, loss: 1.2614319324493408\n",
      "epoch: 96, batch: 1, loss: 1.4900662899017334\n",
      "epoch: 96, batch: 2, loss: 1.7197113037109375\n",
      "epoch: 96, batch: 3, loss: 1.3770415782928467\n",
      "epoch: 96, batch: 4, loss: 1.5843665599822998\n",
      "epoch: 96, batch: 5, loss: 1.5608726739883423\n",
      "epoch: 96, batch: 6, loss: 1.150991678237915\n",
      "epoch: 96, batch: 7, loss: 1.9115298986434937\n",
      "epoch: 96, batch: 8, loss: 1.7431854009628296\n",
      "epoch: 96, batch: 9, loss: 0.9894827604293823\n",
      "epoch: 96, batch: 10, loss: 1.7682338953018188\n",
      "epoch: 96, batch: 11, loss: 1.6739505529403687\n",
      "epoch: 96, batch: 12, loss: 1.8147484064102173\n",
      "epoch: 96, batch: 13, loss: 1.3124513626098633\n",
      "epoch: 96, batch: 14, loss: 1.5789525508880615\n",
      "epoch: 96, batch: 15, loss: 1.6766674518585205\n",
      "epoch: 96, batch: 16, loss: 1.7251592874526978\n",
      "epoch: 96, batch: 17, loss: 1.929783582687378\n",
      "epoch: 96, batch: 18, loss: 1.6972930431365967\n",
      "epoch: 96, batch: 19, loss: 1.309730887413025\n",
      "epoch: 96, batch: 20, loss: 1.5788570642471313\n",
      "epoch: 96, batch: 21, loss: 1.6282752752304077\n",
      "epoch: 96, batch: 22, loss: 1.5610350370407104\n",
      "epoch: 96, batch: 23, loss: 1.2639267444610596\n",
      "epoch: 96, batch: 24, loss: 1.5357439517974854\n",
      "epoch: 96, batch: 25, loss: 1.7402875423431396\n",
      "epoch: 96, batch: 26, loss: 1.328460693359375\n",
      "epoch: 96, batch: 27, loss: 1.742985725402832\n",
      "epoch: 96, batch: 28, loss: 1.5153443813323975\n",
      "epoch: 96, batch: 29, loss: 1.9095280170440674\n",
      "epoch: 96, batch: 30, loss: 1.6068544387817383\n",
      "epoch: 96, batch: 31, loss: 1.612351417541504\n",
      "epoch: 96, batch: 32, loss: 1.8611316680908203\n",
      "epoch: 96, batch: 33, loss: 1.745674729347229\n",
      "epoch: 96, batch: 34, loss: 1.2638734579086304\n",
      "epoch: 96, batch: 35, loss: 1.4198616743087769\n",
      "epoch: 96, batch: 36, loss: 1.6298532485961914\n",
      "epoch: 96, batch: 37, loss: 1.2610881328582764\n",
      "epoch: 96, batch: 38, loss: 1.67422616481781\n",
      "epoch: 96, batch: 39, loss: 1.4710773229599\n",
      "epoch: 96, batch: 40, loss: 1.5356190204620361\n",
      "epoch: 96, batch: 41, loss: 1.3095386028289795\n",
      "epoch: 96, batch: 42, loss: 1.5612305402755737\n",
      "epoch: 96, batch: 43, loss: 1.1023075580596924\n",
      "epoch: 96, batch: 44, loss: 1.5355879068374634\n",
      "epoch: 96, batch: 45, loss: 1.468223214149475\n",
      "epoch: 96, batch: 46, loss: 1.263817548751831\n",
      "epoch: 96, batch: 47, loss: 1.4510550498962402\n",
      "epoch: 96, batch: 48, loss: 1.7513688802719116\n",
      "epoch: 96, batch: 49, loss: 1.451082468032837\n",
      "epoch: 96, batch: 50, loss: 1.9300267696380615\n",
      "epoch: 96, batch: 51, loss: 1.5811896324157715\n",
      "epoch: 96, batch: 52, loss: 1.1022601127624512\n",
      "epoch: 96, batch: 53, loss: 1.703042984008789\n",
      "epoch: 96, batch: 54, loss: 1.1507371664047241\n",
      "epoch: 96, batch: 55, loss: 1.5782936811447144\n",
      "epoch: 96, batch: 56, loss: 1.6098562479019165\n",
      "epoch: 96, batch: 57, loss: 1.2609096765518188\n",
      "epoch: 96, batch: 58, loss: 1.4252946376800537\n",
      "epoch: 96, batch: 59, loss: 1.5354728698730469\n",
      "epoch: 96, batch: 60, loss: 1.30649995803833\n",
      "epoch: 96, batch: 61, loss: 1.5325846672058105\n",
      "epoch: 96, batch: 62, loss: 1.215256690979004\n",
      "epoch: 96, batch: 63, loss: 1.4483871459960938\n",
      "epoch: 96, batch: 64, loss: 1.8620491027832031\n",
      "epoch: 96, batch: 65, loss: 1.263728380203247\n",
      "epoch: 96, batch: 66, loss: 1.5643625259399414\n",
      "epoch: 96, batch: 67, loss: 1.5325148105621338\n",
      "epoch: 96, batch: 68, loss: 1.5614866018295288\n",
      "epoch: 96, batch: 69, loss: 1.9272476434707642\n",
      "epoch: 96, batch: 70, loss: 1.7715339660644531\n",
      "epoch: 96, batch: 71, loss: 1.6100002527236938\n",
      "epoch: 96, batch: 72, loss: 1.4193954467773438\n",
      "epoch: 96, batch: 73, loss: 1.884627103805542\n",
      "epoch: 96, batch: 74, loss: 1.263686180114746\n",
      "epoch: 96, batch: 75, loss: 1.470778226852417\n",
      "epoch: 96, batch: 76, loss: 1.5160198211669922\n",
      "epoch: 96, batch: 77, loss: 1.3737921714782715\n",
      "epoch: 96, batch: 78, loss: 1.2607165575027466\n",
      "epoch: 96, batch: 79, loss: 1.8169472217559814\n",
      "epoch: 96, batch: 80, loss: 1.373767614364624\n",
      "epoch: 96, batch: 81, loss: 1.610094666481018\n",
      "epoch: 96, batch: 82, loss: 1.4897940158843994\n",
      "epoch: 96, batch: 83, loss: 1.3282337188720703\n",
      "epoch: 96, batch: 84, loss: 1.3767153024673462\n",
      "epoch: 96, batch: 85, loss: 1.6998332738876343\n",
      "epoch: 96, batch: 86, loss: 1.5616567134857178\n",
      "epoch: 96, batch: 87, loss: 1.373708724975586\n",
      "epoch: 96, batch: 88, loss: 1.470672845840454\n",
      "epoch: 96, batch: 89, loss: 1.7496778964996338\n",
      "epoch: 96, batch: 90, loss: 1.467646598815918\n",
      "epoch: 96, batch: 91, loss: 1.6778005361557007\n",
      "epoch: 96, batch: 92, loss: 1.3766863346099854\n",
      "epoch: 96, batch: 93, loss: 1.4221456050872803\n",
      "epoch: 96, batch: 94, loss: 1.215105652809143\n",
      "epoch: 96, batch: 95, loss: 1.7718048095703125\n",
      "epoch: 96, batch: 96, loss: 1.7202856540679932\n",
      "epoch: 96, batch: 97, loss: 1.2605355978012085\n",
      "epoch: 96, batch: 98, loss: 1.7657390832901\n",
      "epoch: 96, batch: 99, loss: 1.4866931438446045\n",
      "epoch: 96, batch: 100, loss: 1.422088861465454\n",
      "epoch: 96, batch: 101, loss: 1.3766520023345947\n",
      "epoch: 96, batch: 102, loss: 1.8395509719848633\n",
      "epoch: 96, batch: 103, loss: 1.4517982006072998\n",
      "epoch: 96, batch: 104, loss: 1.263548493385315\n",
      "epoch: 96, batch: 105, loss: 1.613415002822876\n",
      "epoch: 96, batch: 106, loss: 1.532043695449829\n",
      "epoch: 96, batch: 107, loss: 1.5867197513580322\n",
      "epoch: 96, batch: 108, loss: 1.312029242515564\n",
      "epoch: 96, batch: 109, loss: 1.674965262413025\n",
      "epoch: 96, batch: 110, loss: 1.5649888515472412\n",
      "epoch: 96, batch: 111, loss: 1.6749855279922485\n",
      "epoch: 96, batch: 112, loss: 1.6072810888290405\n",
      "epoch: 96, batch: 113, loss: 1.4188597202301025\n",
      "epoch: 96, batch: 114, loss: 1.3120052814483643\n",
      "epoch: 96, batch: 115, loss: 1.5650593042373657\n",
      "epoch: 96, batch: 116, loss: 1.4250969886779785\n",
      "epoch: 96, batch: 117, loss: 1.6935157775878906\n",
      "epoch: 96, batch: 118, loss: 1.930528998374939\n",
      "epoch: 96, batch: 119, loss: 1.5803923606872559\n",
      "epoch: 96, batch: 120, loss: 1.214965581893921\n",
      "epoch: 96, batch: 121, loss: 1.2634683847427368\n",
      "epoch: 96, batch: 122, loss: 1.2904417514801025\n",
      "epoch: 96, batch: 123, loss: 0.9887409210205078\n",
      "epoch: 96, batch: 124, loss: 1.4218944311141968\n",
      "epoch: 96, batch: 125, loss: 1.7236372232437134\n",
      "epoch: 96, batch: 126, loss: 1.5349847078323364\n",
      "epoch: 96, batch: 127, loss: 1.7053723335266113\n",
      "epoch: 97, batch: 0, loss: 1.2634344100952148\n",
      "epoch: 97, batch: 1, loss: 1.4703655242919922\n",
      "epoch: 97, batch: 2, loss: 0.9886940121650696\n",
      "epoch: 97, batch: 3, loss: 1.3280173540115356\n",
      "epoch: 97, batch: 4, loss: 2.025595188140869\n",
      "epoch: 97, batch: 5, loss: 1.4735479354858398\n",
      "epoch: 97, batch: 6, loss: 1.4218136072158813\n",
      "epoch: 97, batch: 7, loss: 1.3732913732528687\n",
      "epoch: 97, batch: 8, loss: 1.5653527975082397\n",
      "epoch: 97, batch: 9, loss: 1.4037374258041382\n",
      "epoch: 97, batch: 10, loss: 1.5801775455474854\n",
      "epoch: 97, batch: 11, loss: 1.565394639968872\n",
      "epoch: 97, batch: 12, loss: 1.3764915466308594\n",
      "epoch: 97, batch: 13, loss: 1.4217568635940552\n",
      "epoch: 97, batch: 14, loss: 1.1502453088760376\n",
      "epoch: 97, batch: 15, loss: 1.4217406511306763\n",
      "epoch: 97, batch: 16, loss: 1.5348529815673828\n",
      "epoch: 97, batch: 17, loss: 1.7238528728485107\n",
      "epoch: 97, batch: 18, loss: 1.3731991052627563\n",
      "epoch: 97, batch: 19, loss: 2.02604079246521\n",
      "epoch: 97, batch: 20, loss: 1.6753677129745483\n",
      "epoch: 97, batch: 21, loss: 1.4895762205123901\n",
      "epoch: 97, batch: 22, loss: 1.772425651550293\n",
      "epoch: 97, batch: 23, loss: 1.720632791519165\n",
      "epoch: 97, batch: 24, loss: 1.7272182703018188\n",
      "epoch: 97, batch: 25, loss: 1.2600128650665283\n",
      "epoch: 97, batch: 26, loss: 1.1468757390975952\n",
      "epoch: 97, batch: 27, loss: 1.3052144050598145\n",
      "epoch: 97, batch: 28, loss: 1.5656267404556274\n",
      "epoch: 97, batch: 29, loss: 1.4492019414901733\n",
      "epoch: 97, batch: 30, loss: 1.2147578001022339\n",
      "epoch: 97, batch: 31, loss: 1.4216116666793823\n",
      "epoch: 97, batch: 32, loss: 1.534735918045044\n",
      "epoch: 97, batch: 33, loss: 1.4249197244644165\n",
      "epoch: 97, batch: 34, loss: 1.5765889883041382\n",
      "epoch: 97, batch: 35, loss: 1.3730552196502686\n",
      "epoch: 97, batch: 36, loss: 1.6788707971572876\n",
      "epoch: 97, batch: 37, loss: 1.5346992015838623\n",
      "epoch: 97, batch: 38, loss: 1.833873987197876\n",
      "epoch: 97, batch: 39, loss: 1.2632317543029785\n",
      "epoch: 97, batch: 40, loss: 1.3117541074752808\n",
      "epoch: 97, batch: 41, loss: 1.290999174118042\n",
      "epoch: 97, batch: 42, loss: 1.146716833114624\n",
      "epoch: 97, batch: 43, loss: 1.421515703201294\n",
      "epoch: 97, batch: 44, loss: 1.910278558731079\n",
      "epoch: 97, batch: 45, loss: 1.9136765003204346\n",
      "epoch: 97, batch: 46, loss: 1.628321886062622\n",
      "epoch: 97, batch: 47, loss: 1.376331090927124\n",
      "epoch: 97, batch: 48, loss: 1.611048936843872\n",
      "epoch: 97, batch: 49, loss: 1.1466461420059204\n",
      "epoch: 97, batch: 50, loss: 1.5659267902374268\n",
      "epoch: 97, batch: 51, loss: 1.4527957439422607\n",
      "epoch: 97, batch: 52, loss: 1.4180421829223633\n",
      "epoch: 97, batch: 53, loss: 1.5174295902252197\n",
      "epoch: 97, batch: 54, loss: 1.4042950868606567\n",
      "epoch: 97, batch: 55, loss: 1.5796934366226196\n",
      "epoch: 97, batch: 56, loss: 1.7242794036865234\n",
      "epoch: 97, batch: 57, loss: 1.7379405498504639\n",
      "epoch: 97, batch: 58, loss: 1.1499797105789185\n",
      "epoch: 97, batch: 59, loss: 1.5830833911895752\n",
      "epoch: 97, batch: 60, loss: 1.1014235019683838\n",
      "epoch: 97, batch: 61, loss: 1.5345265865325928\n",
      "epoch: 97, batch: 62, loss: 1.4664644002914429\n",
      "epoch: 97, batch: 63, loss: 1.1014021635055542\n",
      "epoch: 97, batch: 64, loss: 1.417894959449768\n",
      "epoch: 97, batch: 65, loss: 1.2145464420318604\n",
      "epoch: 97, batch: 66, loss: 1.466417908668518\n",
      "epoch: 97, batch: 67, loss: 1.6077730655670166\n",
      "epoch: 97, batch: 68, loss: 1.6511123180389404\n",
      "epoch: 97, batch: 69, loss: 1.3762352466583252\n",
      "epoch: 97, batch: 70, loss: 1.5176494121551514\n",
      "epoch: 97, batch: 71, loss: 1.424778938293457\n",
      "epoch: 97, batch: 72, loss: 1.2630561590194702\n",
      "epoch: 97, batch: 73, loss: 1.720973253250122\n",
      "epoch: 97, batch: 74, loss: 1.3727147579193115\n",
      "epoch: 97, batch: 75, loss: 1.2630401849746704\n",
      "epoch: 97, batch: 76, loss: 1.7411997318267822\n",
      "epoch: 97, batch: 77, loss: 1.2630294561386108\n",
      "epoch: 97, batch: 78, loss: 1.8376970291137695\n",
      "epoch: 97, batch: 79, loss: 1.7280575037002563\n",
      "epoch: 97, batch: 80, loss: 1.7280734777450562\n",
      "epoch: 97, batch: 81, loss: 1.5663515329360962\n",
      "epoch: 97, batch: 82, loss: 1.8377459049224854\n",
      "epoch: 97, batch: 83, loss: 1.6760164499282837\n",
      "epoch: 97, batch: 84, loss: 1.3761708736419678\n",
      "epoch: 97, batch: 85, loss: 1.4247314929962158\n",
      "epoch: 97, batch: 86, loss: 1.562865972518921\n",
      "epoch: 97, batch: 87, loss: 1.4211666584014893\n",
      "epoch: 97, batch: 88, loss: 1.4661648273468018\n",
      "epoch: 97, batch: 89, loss: 1.724646806716919\n",
      "epoch: 97, batch: 90, loss: 1.6079034805297852\n",
      "epoch: 97, batch: 91, loss: 1.631461501121521\n",
      "epoch: 97, batch: 92, loss: 1.4175488948822021\n",
      "epoch: 97, batch: 93, loss: 1.5792961120605469\n",
      "epoch: 97, batch: 94, loss: 1.586458444595337\n",
      "epoch: 97, batch: 95, loss: 1.5629513263702393\n",
      "epoch: 97, batch: 96, loss: 1.404791235923767\n",
      "epoch: 97, batch: 97, loss: 2.02838397026062\n",
      "epoch: 97, batch: 98, loss: 1.4660530090332031\n",
      "epoch: 97, batch: 99, loss: 1.6312110424041748\n",
      "epoch: 97, batch: 100, loss: 1.5342563390731812\n",
      "epoch: 97, batch: 101, loss: 2.073467969894409\n",
      "epoch: 97, batch: 102, loss: 1.6923967599868774\n",
      "epoch: 97, batch: 103, loss: 1.6312663555145264\n",
      "epoch: 97, batch: 104, loss: 1.4534677267074585\n",
      "epoch: 97, batch: 105, loss: 1.8668479919433594\n",
      "epoch: 97, batch: 106, loss: 1.8219295740127563\n",
      "epoch: 97, batch: 107, loss: 1.2628661394119263\n",
      "epoch: 97, batch: 108, loss: 1.4210001230239868\n",
      "epoch: 97, batch: 109, loss: 1.9315872192382812\n",
      "epoch: 97, batch: 110, loss: 1.262850284576416\n",
      "epoch: 97, batch: 111, loss: 1.4855930805206299\n",
      "epoch: 97, batch: 112, loss: 1.9119751453399658\n",
      "epoch: 97, batch: 113, loss: 1.3723751306533813\n",
      "epoch: 97, batch: 114, loss: 1.6239970922470093\n",
      "epoch: 97, batch: 115, loss: 1.4621931314468384\n",
      "epoch: 97, batch: 116, loss: 1.5631535053253174\n",
      "epoch: 97, batch: 117, loss: 1.6276333332061768\n",
      "epoch: 97, batch: 118, loss: 1.4209210872650146\n",
      "epoch: 97, batch: 119, loss: 1.7577276229858398\n",
      "epoch: 97, batch: 120, loss: 1.575321912765503\n",
      "epoch: 97, batch: 121, loss: 1.372304916381836\n",
      "epoch: 97, batch: 122, loss: 1.7287099361419678\n",
      "epoch: 97, batch: 123, loss: 1.4208816289901733\n",
      "epoch: 97, batch: 124, loss: 1.5669372081756592\n",
      "epoch: 97, batch: 125, loss: 1.4657492637634277\n",
      "epoch: 97, batch: 126, loss: 1.7287719249725342\n",
      "epoch: 97, batch: 127, loss: 1.5702745914459229\n",
      "epoch: 98, batch: 0, loss: 1.4171158075332642\n",
      "epoch: 98, batch: 1, loss: 1.6764873266220093\n",
      "epoch: 98, batch: 2, loss: 1.769963026046753\n",
      "epoch: 98, batch: 3, loss: 1.728849172592163\n",
      "epoch: 98, batch: 4, loss: 1.769977331161499\n",
      "epoch: 98, batch: 5, loss: 1.2627346515655518\n",
      "epoch: 98, batch: 6, loss: 1.5340100526809692\n",
      "epoch: 98, batch: 7, loss: 1.3721826076507568\n",
      "epoch: 98, batch: 8, loss: 1.2589573860168457\n",
      "epoch: 98, batch: 9, loss: 1.631199598312378\n",
      "epoch: 98, batch: 10, loss: 1.6920340061187744\n",
      "epoch: 98, batch: 11, loss: 1.6765892505645752\n",
      "epoch: 98, batch: 12, loss: 1.4053336381912231\n",
      "epoch: 98, batch: 13, loss: 1.1494759321212769\n",
      "epoch: 98, batch: 14, loss: 1.3759130239486694\n",
      "epoch: 98, batch: 15, loss: 1.8679234981536865\n",
      "epoch: 98, batch: 16, loss: 1.676640272140503\n",
      "epoch: 98, batch: 17, loss: 1.2626776695251465\n",
      "epoch: 98, batch: 18, loss: 1.7252715826034546\n",
      "epoch: 98, batch: 19, loss: 1.7738940715789795\n",
      "epoch: 98, batch: 20, loss: 1.6766808032989502\n",
      "epoch: 98, batch: 21, loss: 1.2626588344573975\n",
      "epoch: 98, batch: 22, loss: 1.744349479675293\n",
      "epoch: 98, batch: 23, loss: 1.5634835958480835\n",
      "epoch: 98, batch: 24, loss: 1.2588090896606445\n",
      "epoch: 98, batch: 25, loss: 1.563502550125122\n",
      "epoch: 98, batch: 26, loss: 1.307407021522522\n",
      "epoch: 98, batch: 27, loss: 1.5747783184051514\n",
      "epoch: 98, batch: 28, loss: 1.6121488809585571\n",
      "epoch: 98, batch: 29, loss: 1.608300805091858\n",
      "epoch: 98, batch: 30, loss: 1.100764513015747\n",
      "epoch: 98, batch: 31, loss: 1.7215449810028076\n",
      "epoch: 98, batch: 32, loss: 1.7104196548461914\n",
      "epoch: 98, batch: 33, loss: 1.9816646575927734\n",
      "epoch: 98, batch: 34, loss: 1.5338090658187866\n",
      "epoch: 98, batch: 35, loss: 1.2587064504623413\n",
      "epoch: 98, batch: 36, loss: 1.3112123012542725\n",
      "epoch: 98, batch: 37, loss: 1.4691753387451172\n",
      "epoch: 98, batch: 38, loss: 1.2586777210235596\n",
      "epoch: 98, batch: 39, loss: 0.9874688386917114\n",
      "epoch: 98, batch: 40, loss: 1.4205268621444702\n",
      "epoch: 98, batch: 41, loss: 1.7403424978256226\n",
      "epoch: 98, batch: 42, loss: 1.4057093858718872\n",
      "epoch: 98, batch: 43, loss: 2.0831401348114014\n",
      "epoch: 98, batch: 44, loss: 1.2139194011688232\n",
      "epoch: 98, batch: 45, loss: 1.3111798763275146\n",
      "epoch: 98, batch: 46, loss: 1.6995474100112915\n",
      "epoch: 98, batch: 47, loss: 1.6309881210327148\n",
      "epoch: 98, batch: 48, loss: 1.3032643795013428\n",
      "epoch: 98, batch: 49, loss: 2.0607781410217285\n",
      "epoch: 98, batch: 50, loss: 1.3718119859695435\n",
      "epoch: 98, batch: 51, loss: 1.6955801248550415\n",
      "epoch: 98, batch: 52, loss: 1.725640892982483\n",
      "epoch: 98, batch: 53, loss: 1.5637601613998413\n",
      "epoch: 98, batch: 54, loss: 1.7322628498077393\n",
      "epoch: 98, batch: 55, loss: 1.6810195446014404\n",
      "epoch: 98, batch: 56, loss: 1.2585052251815796\n",
      "epoch: 98, batch: 57, loss: 1.3031413555145264\n",
      "epoch: 98, batch: 58, loss: 1.4505479335784912\n",
      "epoch: 98, batch: 59, loss: 1.7257168292999268\n",
      "epoch: 98, batch: 60, loss: 1.6770871877670288\n",
      "epoch: 98, batch: 61, loss: 1.2624711990356445\n",
      "epoch: 98, batch: 62, loss: 1.8390133380889893\n",
      "epoch: 98, batch: 63, loss: 1.371704339981079\n",
      "epoch: 98, batch: 64, loss: 1.4889898300170898\n",
      "epoch: 98, batch: 65, loss: 1.1491854190826416\n",
      "epoch: 98, batch: 66, loss: 1.4203227758407593\n",
      "epoch: 98, batch: 67, loss: 1.416272521018982\n",
      "epoch: 98, batch: 68, loss: 1.533577799797058\n",
      "epoch: 98, batch: 69, loss: 1.7481863498687744\n",
      "epoch: 98, batch: 70, loss: 1.420291543006897\n",
      "epoch: 98, batch: 71, loss: 1.760278344154358\n",
      "epoch: 98, batch: 72, loss: 1.1004979610443115\n",
      "epoch: 98, batch: 73, loss: 1.2583439350128174\n",
      "epoch: 98, batch: 74, loss: 1.26241135597229\n",
      "epoch: 98, batch: 75, loss: 1.472983717918396\n",
      "epoch: 98, batch: 76, loss: 1.6308237314224243\n",
      "epoch: 98, batch: 77, loss: 1.4061423540115356\n",
      "epoch: 98, batch: 78, loss: 1.2623924016952515\n",
      "epoch: 98, batch: 79, loss: 1.7118804454803467\n",
      "epoch: 98, batch: 80, loss: 1.4688647985458374\n",
      "epoch: 98, batch: 81, loss: 1.4647479057312012\n",
      "epoch: 98, batch: 82, loss: 1.4201973676681519\n",
      "epoch: 98, batch: 83, loss: 1.6327909231185913\n",
      "epoch: 98, batch: 84, loss: 1.7259886264801025\n",
      "epoch: 98, batch: 85, loss: 1.4201738834381104\n",
      "epoch: 98, batch: 86, loss: 1.533456563949585\n",
      "epoch: 98, batch: 87, loss: 1.6328508853912354\n",
      "epoch: 98, batch: 88, loss: 1.4201501607894897\n",
      "epoch: 98, batch: 89, loss: 1.415999174118042\n",
      "epoch: 98, batch: 90, loss: 1.8838551044464111\n",
      "epoch: 98, batch: 91, loss: 1.450818657875061\n",
      "epoch: 98, batch: 92, loss: 1.7260758876800537\n",
      "epoch: 98, batch: 93, loss: 1.5641345977783203\n",
      "epoch: 98, batch: 94, loss: 1.726097822189331\n",
      "epoch: 98, batch: 95, loss: 1.4889109134674072\n",
      "epoch: 98, batch: 96, loss: 1.788480520248413\n",
      "epoch: 98, batch: 97, loss: 1.100341558456421\n",
      "epoch: 98, batch: 98, loss: 1.4847220182418823\n",
      "epoch: 98, batch: 99, loss: 1.6816825866699219\n",
      "epoch: 98, batch: 100, loss: 1.1447885036468506\n",
      "epoch: 98, batch: 101, loss: 1.4687026739120483\n",
      "epoch: 98, batch: 102, loss: 1.699509620666504\n",
      "epoch: 98, batch: 103, loss: 1.3755786418914795\n",
      "epoch: 98, batch: 104, loss: 1.406480312347412\n",
      "epoch: 98, batch: 105, loss: 1.7220077514648438\n",
      "epoch: 98, batch: 106, loss: 1.6129157543182373\n",
      "epoch: 98, batch: 107, loss: 1.6775833368301392\n",
      "epoch: 98, batch: 108, loss: 1.4199931621551514\n",
      "epoch: 98, batch: 109, loss: 1.8261337280273438\n",
      "epoch: 98, batch: 110, loss: 1.4644049406051636\n",
      "epoch: 98, batch: 111, loss: 1.7305160760879517\n",
      "epoch: 98, batch: 112, loss: 1.2579869031906128\n",
      "epoch: 98, batch: 113, loss: 1.4643703699111938\n",
      "epoch: 98, batch: 114, loss: 1.1002368927001953\n",
      "epoch: 98, batch: 115, loss: 1.577667474746704\n",
      "epoch: 98, batch: 116, loss: 1.5643573999404907\n",
      "epoch: 98, batch: 117, loss: 1.2579419612884521\n",
      "epoch: 98, batch: 118, loss: 1.419914960861206\n",
      "epoch: 98, batch: 119, loss: 1.6819757223129272\n",
      "epoch: 98, batch: 120, loss: 1.5776143074035645\n",
      "epoch: 98, batch: 121, loss: 1.6777311563491821\n",
      "epoch: 98, batch: 122, loss: 1.5686931610107422\n",
      "epoch: 98, batch: 123, loss: 1.455379843711853\n",
      "epoch: 98, batch: 124, loss: 1.6088100671768188\n",
      "epoch: 98, batch: 125, loss: 1.7264350652694702\n",
      "epoch: 98, batch: 126, loss: 1.5775506496429443\n",
      "epoch: 98, batch: 127, loss: 1.5334739685058594\n",
      "epoch: 99, batch: 0, loss: 1.1488136053085327\n",
      "epoch: 99, batch: 1, loss: 1.326807975769043\n",
      "epoch: 99, batch: 2, loss: 1.6951534748077393\n",
      "epoch: 99, batch: 3, loss: 1.2578140497207642\n",
      "epoch: 99, batch: 4, loss: 1.7308467626571655\n",
      "epoch: 99, batch: 5, loss: 1.5645201206207275\n",
      "epoch: 99, batch: 6, loss: 1.3021247386932373\n",
      "epoch: 99, batch: 7, loss: 1.5817883014678955\n",
      "epoch: 99, batch: 8, loss: 1.5645487308502197\n",
      "epoch: 99, batch: 9, loss: 1.5331065654754639\n",
      "epoch: 99, batch: 10, loss: 1.4197595119476318\n",
      "epoch: 99, batch: 11, loss: 1.3710811138153076\n",
      "epoch: 99, batch: 12, loss: 1.613257646560669\n",
      "epoch: 99, batch: 13, loss: 1.4153759479522705\n",
      "epoch: 99, batch: 14, loss: 1.7837038040161133\n",
      "epoch: 99, batch: 15, loss: 1.2936220169067383\n",
      "epoch: 99, batch: 16, loss: 1.3063678741455078\n",
      "epoch: 99, batch: 17, loss: 1.7306102514266968\n",
      "epoch: 99, batch: 18, loss: 1.310734510421753\n",
      "epoch: 99, batch: 19, loss: 1.2133772373199463\n",
      "epoch: 99, batch: 20, loss: 1.1487008333206177\n",
      "epoch: 99, batch: 21, loss: 1.8799285888671875\n",
      "epoch: 99, batch: 22, loss: 1.4683454036712646\n",
      "epoch: 99, batch: 23, loss: 1.7311280965805054\n",
      "epoch: 99, batch: 24, loss: 1.4152371883392334\n",
      "epoch: 99, batch: 25, loss: 1.6215219497680664\n",
      "epoch: 99, batch: 26, loss: 1.415211796760559\n",
      "epoch: 99, batch: 27, loss: 1.6825100183486938\n",
      "epoch: 99, batch: 28, loss: 1.4683024883270264\n",
      "epoch: 99, batch: 29, loss: 1.326683759689331\n",
      "epoch: 99, batch: 30, loss: 1.5285193920135498\n",
      "epoch: 99, batch: 31, loss: 1.2575509548187256\n",
      "epoch: 99, batch: 32, loss: 1.1441795825958252\n",
      "epoch: 99, batch: 33, loss: 1.5692403316497803\n",
      "epoch: 99, batch: 34, loss: 1.532936930656433\n",
      "epoch: 99, batch: 35, loss: 1.213295817375183\n",
      "epoch: 99, batch: 36, loss: 1.8889163732528687\n",
      "epoch: 99, batch: 37, loss: 2.082885265350342\n",
      "epoch: 99, batch: 38, loss: 1.419541597366333\n",
      "epoch: 99, batch: 39, loss: 1.6257917881011963\n",
      "epoch: 99, batch: 40, loss: 1.257464051246643\n",
      "epoch: 99, batch: 41, loss: 1.6949542760849\n",
      "epoch: 99, batch: 42, loss: 1.3708181381225586\n",
      "epoch: 99, batch: 43, loss: 1.3061277866363525\n",
      "epoch: 99, batch: 44, loss: 1.8403257131576538\n",
      "epoch: 99, batch: 45, loss: 1.4194867610931396\n",
      "epoch: 99, batch: 46, loss: 1.7269713878631592\n",
      "epoch: 99, batch: 47, loss: 1.5649080276489258\n",
      "epoch: 99, batch: 48, loss: 1.456076979637146\n",
      "epoch: 99, batch: 49, loss: 1.569472074508667\n",
      "epoch: 99, batch: 50, loss: 1.4515526294708252\n",
      "epoch: 99, batch: 51, loss: 1.8404099941253662\n",
      "epoch: 99, batch: 52, loss: 1.613651990890503\n",
      "epoch: 99, batch: 53, loss: 1.613661527633667\n",
      "epoch: 99, batch: 54, loss: 1.763800024986267\n",
      "epoch: 99, batch: 55, loss: 1.7316477298736572\n",
      "epoch: 99, batch: 56, loss: 1.463517427444458\n",
      "epoch: 99, batch: 57, loss: 1.3105896711349487\n",
      "epoch: 99, batch: 58, loss: 1.3059921264648438\n",
      "epoch: 99, batch: 59, loss: 1.4075226783752441\n",
      "epoch: 99, batch: 60, loss: 1.2131760120391846\n",
      "epoch: 99, batch: 61, loss: 1.2618722915649414\n",
      "epoch: 99, batch: 62, loss: 1.4193536043167114\n",
      "epoch: 99, batch: 63, loss: 1.4840445518493652\n",
      "epoch: 99, batch: 64, loss: 1.3059364557266235\n",
      "epoch: 99, batch: 65, loss: 1.261854887008667\n",
      "epoch: 99, batch: 66, loss: 1.9099429845809937\n",
      "epoch: 99, batch: 67, loss: 1.780540108680725\n",
      "epoch: 99, batch: 68, loss: 2.0791735649108887\n",
      "epoch: 99, batch: 69, loss: 1.880035638809204\n",
      "epoch: 99, batch: 70, loss: 1.4076635837554932\n",
      "epoch: 99, batch: 71, loss: 1.2571699619293213\n",
      "epoch: 99, batch: 72, loss: 1.5860525369644165\n",
      "epoch: 99, batch: 73, loss: 1.2618188858032227\n",
      "epoch: 99, batch: 74, loss: 1.7226043939590454\n",
      "epoch: 99, batch: 75, loss: 1.7319653034210205\n",
      "epoch: 99, batch: 76, loss: 1.7157951593399048\n",
      "epoch: 99, batch: 77, loss: 1.678611159324646\n",
      "epoch: 99, batch: 78, loss: 1.6138954162597656\n",
      "epoch: 99, batch: 79, loss: 1.6507705450057983\n",
      "epoch: 99, batch: 80, loss: 1.467909574508667\n",
      "epoch: 99, batch: 81, loss: 1.3752129077911377\n",
      "epoch: 99, batch: 82, loss: 1.3752102851867676\n",
      "epoch: 99, batch: 83, loss: 1.4678847789764404\n",
      "epoch: 99, batch: 84, loss: 1.3704856634140015\n",
      "epoch: 99, batch: 85, loss: 1.7321199178695679\n",
      "epoch: 99, batch: 86, loss: 1.7161095142364502\n",
      "epoch: 99, batch: 87, loss: 1.0996187925338745\n",
      "epoch: 99, batch: 88, loss: 1.5213360786437988\n",
      "epoch: 99, batch: 89, loss: 2.0245022773742676\n",
      "epoch: 99, batch: 90, loss: 1.5325807332992554\n",
      "epoch: 99, batch: 91, loss: 1.256986141204834\n",
      "epoch: 99, batch: 92, loss: 1.5812594890594482\n",
      "epoch: 99, batch: 93, loss: 1.3009017705917358\n",
      "epoch: 99, batch: 94, loss: 1.310409665107727\n",
      "epoch: 99, batch: 95, loss: 1.894415259361267\n",
      "epoch: 99, batch: 96, loss: 1.4677757024765015\n",
      "epoch: 99, batch: 97, loss: 1.7922985553741455\n",
      "epoch: 99, batch: 98, loss: 1.625130295753479\n",
      "epoch: 99, batch: 99, loss: 1.521493911743164\n",
      "epoch: 99, batch: 100, loss: 1.4838342666625977\n",
      "epoch: 99, batch: 101, loss: 1.614104986190796\n",
      "epoch: 99, batch: 102, loss: 1.261680245399475\n",
      "epoch: 99, batch: 103, loss: 1.7484900951385498\n",
      "epoch: 99, batch: 104, loss: 1.6898502111434937\n",
      "epoch: 99, batch: 105, loss: 1.5654598474502563\n",
      "epoch: 99, batch: 106, loss: 1.5654696226119995\n",
      "epoch: 99, batch: 107, loss: 1.5702898502349854\n",
      "epoch: 99, batch: 108, loss: 1.841124176979065\n",
      "epoch: 99, batch: 109, loss: 1.256827473640442\n",
      "epoch: 99, batch: 110, loss: 1.3264400959014893\n",
      "epoch: 99, batch: 111, loss: 1.2129557132720947\n",
      "epoch: 99, batch: 112, loss: 1.3054802417755127\n",
      "epoch: 99, batch: 113, loss: 1.4141225814819336\n",
      "epoch: 99, batch: 114, loss: 1.294736623764038\n",
      "epoch: 99, batch: 115, loss: 1.25677490234375\n",
      "epoch: 99, batch: 116, loss: 1.4520809650421143\n",
      "epoch: 99, batch: 117, loss: 2.0365664958953857\n",
      "epoch: 99, batch: 118, loss: 1.452097773551941\n",
      "epoch: 99, batch: 119, loss: 1.418908953666687\n",
      "epoch: 99, batch: 120, loss: 1.6839592456817627\n",
      "epoch: 99, batch: 121, loss: 1.8899625539779663\n",
      "epoch: 99, batch: 122, loss: 1.5704940557479858\n",
      "epoch: 99, batch: 123, loss: 1.6248624324798584\n",
      "epoch: 99, batch: 124, loss: 1.6409881114959717\n",
      "epoch: 99, batch: 125, loss: 1.6994225978851318\n",
      "epoch: 99, batch: 126, loss: 1.5712659358978271\n",
      "epoch: 99, batch: 127, loss: 1.3304758071899414\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "loss_tracker = []\n",
    "num_batches = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    model.train()\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "  \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        with torch.no_grad():\n",
    "        outputs = model_1(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n",
    "    loss_tracker.append(loss)\n",
    "    num_batches.append(i)\n",
    "        \n",
    "        loss.backward()\n",
    "        print(f'epoch: {epoch}, batch: {i}, loss: {loss}')\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Plot the training loss\n",
    "plt.plot(train_losses['loss'], label=\"Training Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very negative', 'neutral', 'very negative', 'positive', 'neutral', 'neutral', 'positive', 'very negative', 'neutral', 'neutral']\n",
      "['positive', 'positive', 'positive', 'very negative', 'neutral', 'neutral', 'very negative', 'positive', 'neutral', 'positive']\n",
      "['positive', 'neutral', 'neutral', 'neutral', 'neutral', 'very negative', 'positive', 'very negative', 'very negative', 'very negative']\n",
      "['neutral', 'very negative', 'positive', 'neutral', 'very negative', 'neutral', 'very negative', 'very negative', 'positive', 'neutral']\n",
      "['neutral', 'very negative', 'very positive', 'very negative', 'very negative', 'neutral', 'very negative', 'positive', 'positive', 'positive']\n",
      "['neutral', 'positive', 'positive', 'very negative', 'very negative', 'positive', 'positive', 'neutral', 'neutral', 'neutral']\n",
      "['very negative', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'very negative', 'neutral', 'very negative']\n",
      "['positive', 'neutral', 'very negative', 'very negative', 'neutral', 'very negative', 'very positive', 'very negative', 'very negative', 'positive']\n",
      "['neutral', 'positive', 'very negative', 'very negative', 'very negative', 'neutral', 'neutral', 'neutral', 'neutral', 'very negative']\n",
      "['positive', 'very positive', 'very negative', 'neutral', 'neutral', 'positive', 'neutral', 'very negative', 'very negative', 'very negative']\n",
      "['very negative', 'very negative', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral']\n",
      "['very negative', 'very negative', 'positive', 'very positive', 'neutral', 'very positive', 'very negative', 'neutral', 'neutral', 'very negative']\n",
      "['very negative', 'neutral', 'very negative', 'neutral', 'neutral', 'very negative', 'very negative', 'very negative', 'positive', 'positive']\n",
      "['neutral', 'positive', 'positive', 'neutral', 'very positive', 'neutral', 'very negative', 'very negative', 'positive', 'neutral']\n",
      "['neutral', 'positive', 'neutral', 'very negative', 'very negative', 'positive', 'positive', 'neutral', 'neutral', 'very negative']\n",
      "['very negative', 'very negative', 'neutral', 'very negative', 'neutral', 'very positive', 'neutral', 'very negative', 'very negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "model_1.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    max_prob_indcs = torch.argmax(outputs.logits, dim=-1)\n",
    "    predictions = [class_labels[indx.item()] for indx in max_prob_indcs]\n",
    "    \n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7401960784313726}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model_1.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_1(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is not satisfactory so let's try increasing class sizw to make the data more diverse with oversampling and combined sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45330\n",
      "(45330, 64)\n",
      "(45330, 64)\n",
      "(45330, 64)\n",
      "torch.Size([45330, 64])\n",
      "torch.Size([45330, 64])\n",
      "torch.Size([45330, 64])\n",
      "45330\n"
     ]
    }
   ],
   "source": [
    "#Over sampling with smote\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "#converting to an appropriate format\n",
    "training_encoded_input_over = training_encoded['input_ids'].numpy()\n",
    "training_encoded_tokens_over = training_encoded[ 'token_type_ids'].numpy()\n",
    "training_encoded_attention_over = training_encoded['attention_mask'].numpy()\n",
    "\n",
    "combined_features_over = np.concatenate((training_encoded_input_over, training_encoded_tokens_over, training_encoded_attention_over), axis=-1)\n",
    "\n",
    "#applying smoteen to each array with trainY\n",
    "combined_features_resampled_over, trainY_sampled_over = smote.fit_resample(combined_features_over, trainY)\n",
    "print(len(combined_features_resampled_over))\n",
    "\n",
    "#reshaping the original encoded tensor\n",
    "features_per_tensor_over = training_encoded_input_over.shape[1]\n",
    "input_ids_over = combined_features_resampled_over[:,:features_per_tensor_over]\n",
    "print(input_ids_over.shape)\n",
    "token_type_ids_over = combined_features_resampled_over[:,features_per_tensor_over:features_per_tensor_over*2]\n",
    "print(token_type_ids_over.shape)\n",
    "attention_mask_over = combined_features_resampled_over[:,features_per_tensor_over*2:]\n",
    "print(attention_mask_over.shape)\n",
    "\n",
    "#converting to tensors and putting it back in the original\n",
    "training_encoded_sample_over = training_encoded.copy()\n",
    "training_encoded_sample_over['input_ids'] = torch.tensor(input_ids_over)\n",
    "training_encoded_sample_over['token_type_ids'] = torch.tensor(token_type_ids_over)\n",
    "training_encoded_sample_over['attention_mask'] = torch.tensor(attention_mask_over)\n",
    "\n",
    "#check if sizes of all tensors are equal\n",
    "print(training_encoded_sample_over['input_ids'].shape)\n",
    "print(training_encoded_sample_over['attention_mask'].shape)\n",
    "print(training_encoded_sample_over['token_type_ids'].shape)\n",
    "print(len(trainY_sampled_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 19021,  2243,  ...,     0,     0,     0],\n",
      "        [  101, 15117,  2015,  ...,     0,     0,     0],\n",
      "        [  101,  1008,  1008,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2191,  1037,  ...,     0,     0,     0],\n",
      "        [  101,  4349,  3011,  ...,     0,     0,     0],\n",
      "        [  101,  2151,  2062,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(training_encoded_sample_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31930\n"
     ]
    }
   ],
   "source": [
    "#Combined sampling with smoteen\n",
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "#converting to an appropriate format\n",
    "training_encoded_input = training_encoded['input_ids'].numpy()\n",
    "training_encoded_tokens = training_encoded[ 'token_type_ids'].numpy()\n",
    "training_encoded_attention = training_encoded['attention_mask'].numpy()\n",
    "\n",
    "combined_features = np.concatenate((training_encoded_input, training_encoded_tokens, training_encoded_attention), axis=-1)\n",
    "\n",
    "#applying smoteen to each array with trainY\n",
    "combined_features_resampled, trainY_sampled = smote_enn.fit_resample(combined_features, trainY)\n",
    "print(len(combined_features_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101 19021  2243 ...     0     0     0]\n",
      " [  101 15117  2015 ...     0     0     0]\n",
      " [  101  1008  1008 ...     0     0     0]\n",
      " ...\n",
      " [  101  2026  3566 ...     0     0     0]\n",
      " [  101  3988  2743 ...     0     0     0]\n",
      " [  101  3335  2063 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31930, 64)\n",
      "(31930, 64)\n",
      "(31930, 64)\n"
     ]
    }
   ],
   "source": [
    "features_per_tensor = training_encoded_input.shape[1]\n",
    "input_ids = combined_features_resampled[:,:features_per_tensor]\n",
    "print(input_ids.shape)\n",
    "token_type_ids = combined_features_resampled[:,features_per_tensor:features_per_tensor*2]\n",
    "print(token_type_ids.shape)\n",
    "attention_mask = combined_features_resampled[:,features_per_tensor*2:]\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put it back to original array after sampling\n",
    "training_encoded_sample = training_encoded.copy()\n",
    "training_encoded_sample['input_ids'] = torch.tensor(input_ids)\n",
    "training_encoded_sample['token_type_ids'] = torch.tensor(token_type_ids)\n",
    "training_encoded_sample['attention_mask'] = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "torch.Size([31930, 64])\n",
      "torch.Size([31930, 64])\n",
      "torch.Size([31930, 64])\n",
      "31930\n"
     ]
    }
   ],
   "source": [
    "print(type(training_encoded_sample))\n",
    "print(training_encoded_sample['input_ids'].shape)\n",
    "print(training_encoded_sample['attention_mask'].shape)\n",
    "print(training_encoded_sample['token_type_ids'].shape)\n",
    "print(len(trainY_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2074,  3603,  ...,     0,     0,     0],\n",
       "        [  101,  1012,  3374,  ...,     0,     0,     0],\n",
       "        [  101, 23658,  1037,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2191,  1037,  ...,     0,     0,     0],\n",
       "        [  101,  4349,  3011,  ...,     0,     0,     0],\n",
       "        [  101,  2151,  2062,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31930\n",
      "31930\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX_sample))\n",
    "print(len(trainY_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#with no sampling\n",
    "sentiment_graph(valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#after combined sampling\n",
    "sentiment_graph(trainY_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training data after over sampling\n",
    "sentiment_graph(trainY_sampled_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data has been balanced but the validation data has been kept imbalanced to reflect the distribution of real world data which is usually imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new class weights to be assigned\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(trainY_sampled_over), y=trainY_sampled_over)\n",
    "class_weights = torch.from_numpy(class_weights).float().to('cuda')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5353, 0.3627, 1.6640, 1.4376, 1.5071], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new class weights to be assigned\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(trainY), y=trainY)\n",
    "class_weights = torch.from_numpy(class_weights).float().to('cuda')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        item = {key:(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Sentiment_Dataset(training_encoded, trainY)\n",
    "val_dataset = Sentiment_Dataset(val_encoded, valY)\n",
    "test_dataset = Sentiment_Dataset(test_encoded, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1702979082873,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "i0ywj5sKtzY0"
   },
   "outputs": [],
   "source": [
    "train_dataset = Sentiment_Dataset(training_encoded_sample_over, trainY_sampled_over)\n",
    "val_dataset = Sentiment_Dataset(val_encoded, valY)\n",
    "test_dataset = Sentiment_Dataset(test_encoded, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sentiment_Dataset at 0x2aaa7ff79d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sentiment_Dataset at 0x1e353d24910>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sentiment_Dataset at 0x2aaa88020d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702979084302,
     "user": {
      "displayName": "Jouweria Hassan",
      "userId": "04013022173630025862"
     },
     "user_tz": -330
    },
    "id": "AaxlrmZqWSsj",
    "outputId": "ab4cd1b2-18b9-4ddc-b045-6926a7b805af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max GPU memory allocated: 901.45 MB\n",
      "Current GPU memory allocated: 645.70 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check the maximum memory allocated during the entire runtime\n",
    "max_memory_allocated = torch.cuda.max_memory_allocated(device=device) / 1024 ** 2  # in megabytes\n",
    "print(f\"Max GPU memory allocated: {max_memory_allocated:.2f} MB\")\n",
    "\n",
    "# Check the current memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(device=device) / 1024 ** 2  # in megabytes\n",
    "print(f\"Current GPU memory allocated: {current_memory_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels,preds,average='weighted')\n",
    "    return {'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class weighted_class_trainer(Trainer):\n",
    "    def __init__(self, model, args, data_collator=None, compute_metrics=None, weight_class=None, **kwargs):\n",
    "        super().__init__(model, args, data_collator=data_collator, compute_metrics=compute_metrics, **kwargs)\n",
    "        self.weight_class = weight_class\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs.get('labels')\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_func(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class Dropout_Layers(AutoModelForSequenceClassification):\n",
    "    def __init__ (self, num_labels, model_ckpt):\n",
    "        super().__init__(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(model_ckpt.hidden_size, num_labels)\n",
    "            \n",
    "    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None,\n",
    "                return_dict=None, **kwargs):\n",
    "        \n",
    "        x = super.forward(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
    "                          labels=labels, return_dict=return_dict, **kwargs)\n",
    "        \n",
    "        x = self.dropout(x['logits'])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19020' max='19020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19020/19020 36:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.454300</td>\n",
       "      <td>1.116705</td>\n",
       "      <td>0.631226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>0.992091</td>\n",
       "      <td>0.772937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.972232</td>\n",
       "      <td>0.800002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.966500</td>\n",
       "      <td>0.810146</td>\n",
       "      <td>0.795389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.996020</td>\n",
       "      <td>0.810193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.745100</td>\n",
       "      <td>1.051645</td>\n",
       "      <td>0.816954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.441600</td>\n",
       "      <td>1.106332</td>\n",
       "      <td>0.813673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>1.121107</td>\n",
       "      <td>0.819051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>1.210585</td>\n",
       "      <td>0.819267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.448300</td>\n",
       "      <td>1.233600</td>\n",
       "      <td>0.823952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19020, training_loss=0.724049514275902, metrics={'train_runtime': 2191.1813, 'train_samples_per_second': 43.401, 'train_steps_per_second': 8.68, 'total_flos': 783104257958400.0, 'train_loss': 0.724049514275902, 'epoch': 10.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training after increasing sample size\n",
    "output_dir = './comments_output'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir,          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    learning_rate = 5e-5,           # learning rate\n",
    "    weight_decay = 0.01,            # weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    ")\n",
    "\n",
    "model_ =  AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=5)\n",
    "trainer = weighted_class_trainer(\n",
    "    model=model_,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    weight_class = class_weights\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31930' max='31930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31930/31930 28:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.751100</td>\n",
       "      <td>1.012951</td>\n",
       "      <td>0.707342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.080800</td>\n",
       "      <td>1.062171</td>\n",
       "      <td>0.728543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.597200</td>\n",
       "      <td>0.817133</td>\n",
       "      <td>0.768010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>1.081194</td>\n",
       "      <td>0.754973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>0.750259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31930, training_loss=0.659509090534617, metrics={'train_runtime': 1734.9858, 'train_samples_per_second': 92.018, 'train_steps_per_second': 18.404, 'total_flos': 1314643478265600.0, 'train_loss': 0.659509090534617, 'epoch': 5.0})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with combined sampling (over and under sampling with Dropout Layers) \n",
    "output_dir = './twitter_miniLM_combined'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir,          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    learning_rate = 5e-5,           # learning rate\n",
    "    weight_decay = 0.1,            # weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "model_1 =   AutoModelForSequenceClassification .from_pretrained(model_ckpt, num_labels=5)\n",
    "trainer = weighted_class_trainer(\n",
    "    model=model_1,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    weight_class = class_weights\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90660' max='90660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90660/90660 1:20:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309400</td>\n",
       "      <td>0.833499</td>\n",
       "      <td>0.825964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>0.800782</td>\n",
       "      <td>0.848565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.843735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.957424</td>\n",
       "      <td>0.841187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.988442</td>\n",
       "      <td>0.840710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>0.984263</td>\n",
       "      <td>0.845154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.101288</td>\n",
       "      <td>0.853457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>1.173393</td>\n",
       "      <td>0.845631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.267266</td>\n",
       "      <td>0.845119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>1.246678</td>\n",
       "      <td>0.848581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90660, training_loss=0.2082019666105395, metrics={'train_runtime': 4823.4519, 'train_samples_per_second': 93.978, 'train_steps_per_second': 18.796, 'total_flos': 3732714617587200.0, 'train_loss': 0.2082019666105395, 'epoch': 10.0})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with over sampling and with Dropout\n",
    "output_dir = './twitter_miniLM_over'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir,          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    learning_rate = 5e-5,           # learning rate\n",
    "    weight_decay = 0.01,            # weight decay\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_steps= 5000\n",
    ")\n",
    "\n",
    "model_2 =  AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_1,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90660' max='90660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90660/90660 1:20:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.055600</td>\n",
       "      <td>0.561738</td>\n",
       "      <td>0.802271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.976800</td>\n",
       "      <td>0.500918</td>\n",
       "      <td>0.823375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.637750</td>\n",
       "      <td>0.811165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.816200</td>\n",
       "      <td>0.617228</td>\n",
       "      <td>0.845782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.845567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.791700</td>\n",
       "      <td>0.593966</td>\n",
       "      <td>0.848639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>0.750554</td>\n",
       "      <td>0.852685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.767525</td>\n",
       "      <td>0.853920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.789659</td>\n",
       "      <td>0.856165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.853287</td>\n",
       "      <td>0.854617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90660, training_loss=0.5959931900007529, metrics={'train_runtime': 4839.9947, 'train_samples_per_second': 93.657, 'train_steps_per_second': 18.731, 'total_flos': 3732714617587200.0, 'train_loss': 0.5959931900007529, 'epoch': 10.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with over sampling and without Dropout\n",
    "\n",
    "output_dir = './twitter_miniLM_over'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir,          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    learning_rate = 5e-5,           # learning rate\n",
    "    weight_decay = 0.01,            # weight decay\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_steps= 5000\n",
    ")\n",
    "\n",
    "model_2 =  AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_2,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 4 1 4 1]\n",
      "Predictions: [2 0 4 2 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 2 2 2 1]\n",
      "Predictions: [2 2 4 1 2]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [2 4 0 2 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 0 1 4 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [0 2 2 0 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 0 2 1 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [1 4 2 2 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 4 1 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 2 1 0 1]\n",
      "Predictions: [2 1 1 0 0]\n",
      "Predictions: [1 1 1 0 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [4 0 0 1 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 2 1 0 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 2 1 0 1]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [2 4 1 1 1]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [4 2 2 2 2]\n",
      "Predictions: [1 3 1 4 2]\n",
      "Predictions: [1 1 1 1 0]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [1 2 1 0 1]\n",
      "Predictions: [1 1 2 4 2]\n",
      "Predictions: [1 1 2 1 4]\n",
      "Predictions: [1 2 0 2 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 1 0 4 1]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [2 2 2 2 0]\n",
      "Predictions: [1 0 1 2 1]\n",
      "Predictions: [0 2 2 1 2]\n",
      "Predictions: [2 2 2 2 0]\n",
      "Predictions: [1 1 4 1 4]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [0 1 2 2 2]\n",
      "Predictions: [2 2 2 4 1]\n",
      "Predictions: [1 2 4 2 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 2 4 0 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 2 1 0 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 0 1 0 1]\n",
      "Predictions: [1 2 2 4 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 4 1 4 1]\n",
      "Predictions: [1 1 0 2 4]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [0 2 1 2 2]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [0 1 2 1 1]\n",
      "Predictions: [0 2 1 1 2]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 2 2 1 4]\n",
      "Predictions: [0 2 1 2 2]\n",
      "Predictions: [1 0 2 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 0 2 1]\n",
      "Predictions: [1 3 1 1 1]\n",
      "Predictions: [1 4 1 1 2]\n",
      "Predictions: [2 0 2 1 2]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [0 2 1 1 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [4 1 2 1 2]\n",
      "Predictions: [2 1 0 1 1]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 1 2 4]\n",
      "Predictions: [1 1 2 0 4]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 1 0 0 1]\n",
      "Predictions: [2 0 4 1 2]\n",
      "Predictions: [0 1 2 1 2]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 4 4 2 1]\n",
      "Predictions: [2 4 2 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [0 1 2 1 2]\n",
      "Predictions: [1 0 2 4 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 2 1 4 1]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 2 0 1 0]\n",
      "Predictions: [0 1 4 1 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [3 2 1 1 2]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 1 4 2 2]\n",
      "Predictions: [1 2 2 1 0]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [1 2 0 4 1]\n",
      "Predictions: [2 2 3 2 1]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [1 4 1 1 2]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 1 1 0 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 4 0 0 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 1 2 0 2]\n",
      "Predictions: [2 1 2 4 1]\n",
      "Predictions: [4 2 1 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [2 2 2 2 4]\n",
      "Predictions: [0 2 1 1 1]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [0 0 1 0 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 1 0 1]\n",
      "Predictions: [1 1 2 0 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [2 2 1 0 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [0 1 3 2 1]\n",
      "Predictions: [2 2 2 1 0]\n",
      "Predictions: [4 1 0 1 2]\n",
      "Predictions: [1 1 1 2 0]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 3 4 2]\n",
      "Predictions: [0 2 1 2 2]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [0 1 1 1 1]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [1 2 2 1 4]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 0 1 1 2]\n",
      "Predictions: [1 0 2 2 2]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [2 4 2 2 1]\n",
      "Predictions: [2 1 1 0 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [0 2 2 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 2 4 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [4 1 1 2 0]\n",
      "Predictions: [3 1 2 4 2]\n",
      "Predictions: [1 1 0 4 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 2 2 2 0]\n",
      "Predictions: [0 2 2 1 2]\n",
      "Predictions: [1 0 0 1 2]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [2 0 0 2 4]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [0 2 1 1 2]\n",
      "Predictions: [1 2 2 0 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 2 0 2 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 2 4 1 0]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [2 1 1 2 0]\n",
      "Predictions: [2 4 4 1 1]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [0 1 3 0 1]\n",
      "Predictions: [2 3 1 1 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [4 2 1 2 2]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [4 2 2 2 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 2 1 4]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 0 2 1 2]\n",
      "Predictions: [2 2 2 0 1]\n",
      "Predictions: [4 1 2 2 1]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 4 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [1 1 2 0 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 2 0 2]\n",
      "Predictions: [0 2 1 1 2]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 0 4 0 1]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [1 2 1 1 4]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 4 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 0 2 2 1]\n",
      "Predictions: [0 2 1 1 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 1 2 0 1]\n",
      "Predictions: [1 1 1 2 0]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 1 0 4]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 0 1 2 1]\n",
      "Predictions: [1 2 0 4 2]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [4 2 2 1 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 0 1 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [0 2 2 2 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 0 1 1 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 2 1 0]\n",
      "Predictions: [0 1 0 1 0]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [2 0 2 2 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 2 1 4 0]\n",
      "Predictions: [1 0 4 1 0]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 1 0 1]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 4 0 0 1]\n",
      "Predictions: [1 2 0 1 1]\n",
      "Predictions: [1 2 0 4 1]\n",
      "Predictions: [1 4 1 0 2]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [1 2 0 1 2]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [0 1 1 1 1]\n",
      "Predictions: [4 1 1 4 1]\n",
      "Predictions: [1 4 1 2 2]\n",
      "Predictions: [0 1 1 0 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [0 2 0 1 2]\n",
      "Predictions: [1 2 1 2 0]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 1 0 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 4 2 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 1 2 0 2]\n",
      "Predictions: [0 1 1 2 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [0 1 1 2 1]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 2 4 1]\n",
      "Predictions: [1 1 1 0 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 4 2 1 2]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [1 0 1 1 2]\n",
      "Predictions: [0 2 1 2 1]\n",
      "Predictions: [2 1 2 2 0]\n",
      "Predictions: [1 0 2 1 1]\n",
      "Predictions: [1 2 0 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 2 0 1 2]\n",
      "Predictions: [1 2 1 2 4]\n",
      "Predictions: [2 1 2 2 0]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [2 4 2 1 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 0 1 2 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 1 2 1 0]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 1 0 2 2]\n",
      "Predictions: [1 1 4 2 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [0 1 1 1 1]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [4 1 2 2 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 0 1 2 2]\n",
      "Predictions: [2 2 1 4 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 1 2 4 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 4 1 1 2]\n",
      "Predictions: [1 1 0 2 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [2 1 0 2 1]\n",
      "Predictions: [2 2 1 0 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [4 0 2 2 2]\n",
      "Predictions: [4 1 2 1 1]\n",
      "Predictions: [3 1 1 2 1]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 1 2 1 0]\n",
      "Predictions: [1 2 1 4 0]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [0 2 0 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 4 2 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 1 1 4 2]\n",
      "Predictions: [2 2 2 4 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 0 2 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 0 1 2 2]\n",
      "Predictions: [1 2 1 3 1]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 4 1 2 2]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 1 2 0 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [2 1 1 4 2]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 1 0 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 0 2 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [1 0 2 1 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 2 2 0 1]\n",
      "Predictions: [2 2 0 1 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 1 0 1 3]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [0 1 4 1 0]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [0 1 3 1 1]\n",
      "Predictions: [1 0 1 1 2]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 2 4 1]\n",
      "Predictions: [2 0 1 1 2]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 0 2 2 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 1 2 4]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 1 4 1 2]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [3 1 1 2 2]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 2 0 1 4]\n",
      "Predictions: [1 2 2 0 1]\n",
      "Predictions: [0 1 4 2 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 2 1 1 0]\n",
      "Predictions: [0 1 2 1 0]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [1 0 1 2 2]\n",
      "Predictions: [1 4 2 1 4]\n",
      "Predictions: [0 2 1 2 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [4 1 1 1 2]\n",
      "Predictions: [1 4 0 1 2]\n",
      "Predictions: [0 1 1 1 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 0 1 1 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [0 1 2 1 1]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [0 1 1 4 1]\n",
      "Predictions: [1 0 3 1 1]\n",
      "Predictions: [2 1 2 4 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 0 1 1 1]\n",
      "Predictions: [4 2 1 2 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [4 2 1 1 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 4 4 3 0]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [4 2 2 4 0]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 2 1 2 4]\n",
      "Predictions: [1 1 2 0 4]\n",
      "Predictions: [1 0 2 2 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 2 2 0 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [0 1 1 2 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [3 1 1 1 2]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 1 1 4]\n",
      "Predictions: [3 2 1 2 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [2 1 0 2 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 1 2 0]\n",
      "Predictions: [1 2 1 0 2]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [2 2 1 0 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [4 1 1 2 2]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [4 1 1 0 0]\n",
      "Predictions: [1 2 0 2 1]\n",
      "Predictions: [2 4 1 2 2]\n",
      "Predictions: [2 1 0 2 1]\n",
      "Predictions: [1 4 1 2 4]\n",
      "Predictions: [2 1 2 4 1]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [0 0 1 2 1]\n",
      "Predictions: [2 0 1 0 2]\n",
      "Predictions: [2 0 1 2 2]\n",
      "Predictions: [0 2 1 2 2]\n",
      "Predictions: [2 1 2 4 2]\n",
      "Predictions: [2 0 1 4 1]\n",
      "Predictions: [1 1 1 1 0]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 0 1 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 0 1 1]\n",
      "Predictions: [2 1 4 2 1]\n",
      "Predictions: [2 2 2 2 4]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 2 2 1 0]\n",
      "Predictions: [2 2 4 3 0]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [0 1 1 0 0]\n",
      "Predictions: [2 0 2 4 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 3 1 2 1]\n",
      "Predictions: [2 0 0 2 0]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 2 0 1 1]\n",
      "Predictions: [1 0 2 2 2]\n",
      "Predictions: [0 1 2 2 2]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 4 1 2 1]\n",
      "Predictions: [0 1 1 1 2]\n",
      "Predictions: [2 1 3 2 4]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [4 2 1 2 1]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [1 0 1 2 1]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [2 4 1 2 1]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [1 2 2 2 0]\n",
      "Predictions: [1 1 4 2 1]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [2 1 4 0 2]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [2 2 2 1 3]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [0 1 4 1 1]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 0 1 1 0]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 0 2 1 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [4 1 2 2 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 1 1 0 2]\n",
      "Predictions: [4 2 2 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 0 2 0]\n",
      "Predictions: [4 2 2 1 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 0 1 1 0]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [0 2 1 2 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 1 2 0 0]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 1 2 4]\n",
      "Predictions: [2 2 2 0 2]\n",
      "Predictions: [1 2 1 0 1]\n",
      "Predictions: [4 2 1 2 0]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [0 1 1 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 4 2 0 1]\n",
      "Predictions: [0 2 1 1 0]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [2 0 2 1 2]\n",
      "Predictions: [1 1 0 2 2]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [4 1 1 2 1]\n",
      "Predictions: [0 2 1 1 1]\n",
      "Predictions: [2 1 1 4 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 1 4 1]\n",
      "Predictions: [1 1 1 0 2]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 1 2 0 0]\n",
      "Predictions: [2 1 0 1 1]\n",
      "Predictions: [4 2 1 2 1]\n",
      "Predictions: [1 1 1 0 2]\n",
      "Predictions: [2 2 4 2 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 2 1 2 4]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 1 0 1 2]\n",
      "Predictions: [1 1 1 2 4]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 1 1 1 0]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 0 2 1 2]\n",
      "Predictions: [2 1 2 2 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 2 4 2 4]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 2 0 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 1 4 3 1]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 2 1 2 3]\n",
      "Predictions: [4 2 1 2 2]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 4 2 1]\n",
      "Predictions: [1 2 2 0 1]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [0 1 1 1 1]\n",
      "Predictions: [1 2 0 1 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 4 1 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [2 4 1 1 1]\n",
      "Predictions: [1 2 4 2 2]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [2 2 2 3 0]\n",
      "Predictions: [2 1 2 0 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 1 4 1 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [1 4 4 2 1]\n",
      "Predictions: [1 4 1 2 2]\n",
      "Predictions: [2 1 2 0 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [4 2 4 2 3]\n",
      "Predictions: [1 2 1 2 0]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [4 2 1 4 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [4 2 4 2 1]\n",
      "Predictions: [1 0 1 4 1]\n",
      "Predictions: [1 1 0 1 4]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 1 1 3 0]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 2 0 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 1 1 4 1]\n",
      "Predictions: [2 4 2 2 2]\n",
      "Predictions: [1 4 2 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 0 0 1]\n",
      "Predictions: [2 1 1 2 4]\n",
      "Predictions: [2 1 0 2 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 1 2 4 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 1 0 2]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [2 2 4 2 1]\n",
      "Predictions: [3 1 1 2 2]\n",
      "Predictions: [1 0 3 1 1]\n",
      "Predictions: [1 4 1 1 2]\n",
      "Predictions: [2 2 0 1 1]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [1 2 0 1 1]\n",
      "Predictions: [0 1 0 2 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [4 1 2 2 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 1 0 2 2]\n",
      "Predictions: [2 4 1 2 2]\n",
      "Predictions: [3 1 1 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 0 2 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [0 1 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 1 0 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 1 2 4 1]\n",
      "Predictions: [1 2 1 4 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 2 0 1 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 1 0 2 1]\n",
      "Predictions: [3 1 1 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 4 1 1 0]\n",
      "Predictions: [2 0 2 1 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 1 2 1 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 1 1 2 0]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 0 2 1 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [0 2 1 2 1]\n",
      "Predictions: [1 2 4 1 1]\n",
      "Predictions: [1 1 1 3 1]\n",
      "Predictions: [1 2 0 2 2]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [4 2 1 1 2]\n",
      "Predictions: [1 2 2 2 4]\n",
      "Predictions: [0 2 1 1 2]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [0 2 1 2 2]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [0 1 2 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [2 2 1 2 0]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 4 0 2 1]\n",
      "Predictions: [1 1 3 2 0]\n",
      "Predictions: [1 0 1 2 2]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [1 2 2 2 0]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [1 2 4 1 1]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [0 4 2 1 0]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [0 1 2 0 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [0 1 2 1 2]\n",
      "Predictions: [4 1 4 4 1]\n",
      "Predictions: [2 1 1 4 2]\n",
      "Predictions: [0 1 1 4 0]\n",
      "Predictions: [2 2 1 0 1]\n",
      "Predictions: [1 1 2 2 4]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [1 2 1 4 1]\n",
      "Predictions: [2 1 2 1 0]\n",
      "Predictions: [2 1 0 2 2]\n",
      "Predictions: [4 1 2 4 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [3 2 2 2 2]\n",
      "Predictions: [1 1 2 0 0]\n",
      "Predictions: [1 1 0 1 2]\n",
      "Predictions: [1 2 3 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 2 1 2 2]\n",
      "Predictions: [2 1 0 1 2]\n",
      "Predictions: [1 2 1 1 0]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [4 4 1 1 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 1 1 0 0]\n",
      "Predictions: [3 2 2 1 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [2 2 0 1 2]\n",
      "Predictions: [2 2 2 2 1]\n",
      "Predictions: [1 0 1 0 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 0 1 2 0]\n",
      "Predictions: [1 0 2 2 2]\n",
      "Predictions: [2 1 4 2 1]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [1 0 1 2 2]\n",
      "Predictions: [1 0 1 1 4]\n",
      "Predictions: [2 1 1 0 1]\n",
      "Predictions: [1 1 2 1 0]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [1 1 1 2 0]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [0 2 1 2 1]\n",
      "Predictions: [0 1 1 2 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 1 4 2 2]\n",
      "Predictions: [2 1 1 3 1]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 1 2 0 0]\n",
      "Predictions: [1 1 2 1 4]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 1 1 0 1]\n",
      "Predictions: [4 2 1 2 1]\n",
      "Predictions: [1 2 2 1 2]\n",
      "Predictions: [2 2 1 0 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 2 2 2 1]\n",
      "Predictions: [2 2 1 4 4]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [4 2 0 2 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [0 1 2 1 2]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 4 0 1 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 1 4 2]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [1 1 2 2 2]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [2 1 1 1 4]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [1 1 0 0 4]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [1 2 4 1 1]\n",
      "Predictions: [1 1 2 1 0]\n",
      "Predictions: [0 1 2 2 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [0 4 1 1 0]\n",
      "Predictions: [2 1 2 0 2]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [3 2 2 2 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [1 4 1 1 2]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 0 2 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [3 1 0 2 1]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [2 1 0 2 1]\n",
      "Predictions: [0 1 4 1 1]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 2 2 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [1 2 1 1 0]\n",
      "Predictions: [1 1 2 0 1]\n",
      "Predictions: [0 2 1 1 1]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 2 2 2 2]\n",
      "Predictions: [1 1 0 1 2]\n",
      "Predictions: [0 2 1 2 1]\n",
      "Predictions: [2 1 2 2 0]\n",
      "Predictions: [1 1 1 1 4]\n",
      "Predictions: [2 0 1 1 0]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 0 2 1]\n",
      "Predictions: [0 1 2 1 1]\n",
      "Predictions: [2 1 1 0 1]\n",
      "Predictions: [2 0 0 0 2]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [2 1 2 4 2]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [4 2 2 1 2]\n",
      "Predictions: [1 1 2 2 3]\n",
      "Predictions: [1 2 1 4 1]\n",
      "Predictions: [4 1 1 2 2]\n",
      "Predictions: [2 2 2 4 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [1 0 0 1 2]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [0 1 2 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 1 1 1 2]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [4 1 1 1 2]\n",
      "Predictions: [2 2 1 1 1]\n",
      "Predictions: [2 1 2 2 1]\n",
      "Predictions: [2 1 0 1 1]\n",
      "Predictions: [1 0 1 4 1]\n",
      "Predictions: [1 2 1 2 4]\n",
      "Predictions: [2 2 1 1 4]\n",
      "Predictions: [0 1 1 1 0]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 1 0 2 1]\n",
      "Predictions: [1 0 1 2 1]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [2 1 4 1 1]\n",
      "Predictions: [1 2 1 1 2]\n",
      "Predictions: [1 2 2 1 0]\n",
      "Predictions: [2 4 4 2 0]\n",
      "Predictions: [2 2 2 2 2]\n",
      "Predictions: [1 1 4 2 1]\n",
      "Predictions: [2 0 0 1 1]\n",
      "Predictions: [2 1 2 1 1]\n",
      "Predictions: [2 0 1 1 1]\n",
      "Predictions: [2 0 1 1 1]\n",
      "Predictions: [1 0 1 1 1]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 1 1 1 1]\n",
      "Predictions: [2 2 2 1 0]\n",
      "Predictions: [2 1 1 3 1]\n",
      "Predictions: [0 2 4 1 1]\n",
      "Predictions: [2 1 2 1 2]\n",
      "Predictions: [2 1 4 1 1]\n",
      "Predictions: [2 0 2 1 2]\n",
      "Predictions: [1 2 2 0 1]\n",
      "Predictions: [1 0 2 1 2]\n",
      "Predictions: [1 1 4 1 1]\n",
      "Predictions: [1 1 1 1 4]\n",
      "Predictions: [2 2 1 1 2]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [2 1 1 2 1]\n",
      "Predictions: [2 2 4 0 1]\n",
      "Predictions: [2 4 1 1 2]\n",
      "Predictions: [2 1 3 2 0]\n",
      "Predictions: [1 0 0 2 4]\n",
      "Predictions: [1 1 2 1 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 0 2 2 1]\n",
      "Predictions: [2 2 1 2 1]\n",
      "Predictions: [2 2 2 1 1]\n",
      "Predictions: [1 1 1 2 2]\n",
      "Predictions: [2 0 2 2 1]\n",
      "Predictions: [1 1 0 4 4]\n",
      "Predictions: [1 2 1 1 1]\n",
      "Predictions: [1 1 1 1 2]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [2 2 2 0 1]\n",
      "Predictions: [2 1 1 0 0]\n",
      "Predictions: [1 1 0 1 1]\n",
      "Predictions: [2 2 1 0 2]\n",
      "Predictions: [4 2 2 1 0]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [2 1 0 1 2]\n",
      "Predictions: [1 0 1 1 2]\n",
      "Predictions: [2 1 1 2 2]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [1 1 2 2 1]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [2 2 2 1 2]\n",
      "Predictions: [1 2 1 2 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 2 2 4 4]\n",
      "Predictions: [2 1 4 4 1]\n",
      "Predictions: [1 2 4 2 1]\n",
      "Predictions: [1 4 1 1 1]\n",
      "Predictions: [1 1 1 2 1]\n",
      "Predictions: [2 1 1 1 1]\n",
      "Predictions: [1 1 1 4 0]\n",
      "Predictions: [1 2 1 2 2]\n",
      "Predictions: [4 1 2 1 0]\n",
      "Predictions: [2 2 2 2 1]\n",
      "Predictions: [1 1 1 2 0]\n",
      "Predictions: [2 1 1 1 0]\n",
      "Predictions: [1 2 1 4 2]\n",
      "Predictions: [2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8610150991449882}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making predictions\n",
    "\n",
    "model_2.to(device)\n",
    "\n",
    "# Move the input tensor to the device\n",
    "for i, batch in enumerate(test_loader):\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model_2(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    # Display the predictions\n",
    "    print(\"Predictions:\", predictions.cpu().numpy())\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2175,  2000,  3109,   999,   102,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2200,  2172,  4078, 18136,  2063,  2108,  2182,   102],\n",
      "        [  101,  2023,  2003,  2025,  1037,  2204,  4132,   102,     0,     0],\n",
      "        [  101,  1045,  1005,  1049,  8295,  2009,  2182,   102,     0,     0],\n",
      "        [  101,  6919,  2801,   999,   102,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
      "[3 1 1 2 2]\n",
      "['very negative', 'neutral', 'neutral', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "#Predicting one sentence at a time\n",
    "\n",
    "import pickle\n",
    "with open('sentiment_model.pkl', 'rb') as file:\n",
    "    \n",
    "     model = pickle.load(file)\n",
    "        \n",
    "text = [\"Go to hell!\",\"I very much despise being here\",\"This is not a good platform\",\"I'm loving it here\",\"Wonderful idea!\"]\n",
    "tokenized_text = tokenize_text(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = tokenized_text['input_ids'].to(device)\n",
    "attention_mask = tokenized_text['attention_mask'].to(device)\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "logits = outputs.logits\n",
    "prediction_indxs = torch.argmax(logits, dim=-1)\n",
    "prediction_indxs = prediction_indxs.cpu().numpy()\n",
    "print(prediction_indxs)\n",
    "prediction_labels = [class_labels[i] for i in prediction_indxs]\n",
    "print(prediction_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "\n",
    "import pickle\n",
    "with open('sentiment_model.pkl', 'wb') as file:\n",
    "pickle.dump(model_2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data from reddit for proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ideally we want human annotated comments not done by vader sentiment\n",
    "\n",
    "json_data = {'users': []}\n",
    "user_list = ['BantedHam', 'piconese', 'MikGusta']\n",
    "\n",
    "for i in range(6):  # over 6 weeks\n",
    "    \n",
    "    timestamp_month_ago = int((datetime.utcnow() - timedelta(weeks=i + 1)).timestamp())\n",
    "    timestamp_current = int((datetime.utcnow() - timedelta(weeks=i)).timestamp())\n",
    "\n",
    "    # Convert timestamp_current to datetime format\n",
    "    datetime_current = datetime.utcfromtimestamp(timestamp_current)\n",
    "\n",
    "    # Format the datetime to your desired format\n",
    "    date_formatted = datetime_current.strftime('%m/%d/%y')\n",
    "    time_formatted = datetime_current.strftime('%I:%M %p')\n",
    "\n",
    "    for username in user_list:\n",
    "        user_data = {'username': username, 'date': date_formatted, 'time': time_formatted, 'comments': []}\n",
    "\n",
    "        user = reddit.redditor(username)\n",
    "        comments = user.comments.new(limit=None)\n",
    "\n",
    "        for comment in comments:\n",
    "            if timestamp_month_ago <= comment.created_utc <= timestamp_current:\n",
    "                user_data['comments'].append(comment.body)\n",
    "\n",
    "        json_data['users'].append(user_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty json file for keeping scores\n",
    "\n",
    "score_data = {'users': []}\n",
    "\n",
    "current_time = int(datetime.utcnow().timestamp())\n",
    "\n",
    "for username in user_list:\n",
    "    user_score = {'username':username, 'time': current_time, 'score': 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'username': 'BantedHam',\n",
       "  'date': '01/24/24',\n",
       "  'time': '03:52 AM',\n",
       "  'comments': []},\n",
       " {'username': 'piconese',\n",
       "  'date': '01/24/24',\n",
       "  'time': '03:52 AM',\n",
       "  'comments': ['T-flex is a total noob, just roll a beast master and go beast mode on him. Best class in the game, *trust me*.',\n",
       "   'My kid stole his first bass last year during his little league season, I was so proud ',\n",
       "   'You dont, thats why Jim morrigan ended up drinking himself to death',\n",
       "   'This is technically libel, but yeah, the sentiment checks out lol',\n",
       "   'Great, theyre gonna lick the gem flavored artifact too? ',\n",
       "   'I hate you both, I upvoted TIMES TWO!!!',\n",
       "   'She can use the power winch to trigger a controlled explosion to escape.',\n",
       "   'Super Sentai! \\n\\n*cue dbza super sentai theme*',\n",
       "   'That is dope, thank you!',\n",
       "   'Wait, can you find us after the intro? I wanted to keep the little creep but I never saw them again once I landed on the beach \\u200d',\n",
       "   'As much as I agree with op, the idea that they have an exclusive form in the demon realm is kind of a cool idea. Take my upvote.']},\n",
       " {'username': 'MikGusta',\n",
       "  'date': '01/24/24',\n",
       "  'time': '03:52 AM',\n",
       "  'comments': ['The shape of the Earth, how germs spread, the safety of vaccines, the cause and impact of global warming, the positive affect of strict gun control. I dont want to keep going.',\n",
       "   'https://preview.redd.it/56r7s6pei8ec1.jpeg?width=1241&format=pjpg&auto=webp&s=e91913b681cdfc98113248d4947f9d14bfd6c236',\n",
       "   'OH MY GOD!',\n",
       "   'Thats horrible! I hope you and your baby are doing well and that your (hopefully ex) husband gets what he deserves!',\n",
       "   'She was incredibly rude and shouldnt be anywhere near phones. But it is usually common practice for clinics to only call about results if theres something to be concerned about. No news is good news.',\n",
       "   'Thats the sickest shit Ive ever heard',\n",
       "   'https://preview.redd.it/kiml419534ec1.jpeg?width=1920&format=pjpg&auto=webp&s=7fe60725a12833a496e20d8cf833cf7488650fa9',\n",
       "   'https://preview.redd.it/jjjkaa2234ec1.jpeg?width=3024&format=pjpg&auto=webp&s=d77fa748635e3205ceb5e13a9c8321187e95c94e',\n",
       "   'https://preview.redd.it/i269luuz24ec1.jpeg?width=3024&format=pjpg&auto=webp&s=e979c2c8b97882a9c14d9ff1fa1ece1e4baccf29',\n",
       "   'https://preview.redd.it/9iqnb9kv24ec1.jpeg?width=3024&format=pjpg&auto=webp&s=46c43e472e1856b70d75f0c601879c6a98d4eec2',\n",
       "   \"I'm gonna trade this life for fortune and fame! I'd even cut my hair and change my name!\",\n",
       "   'Me too! Ill sing it after I hear someone else say it also lmao',\n",
       "   'Well at least he got to keep his ball.',\n",
       "   'My dad always boils hotdogs because his mom always boiled hotdogs and she did it because of her mom. That hotdog water nastiness ends with me!',\n",
       "   'My mom let her shitty ex bfs mom babysit me. She made me a basic sandwich and forced me to eat the whole thing when I wasnt hungry and I threw up. I still hate crappy homemade sandwiches.',\n",
       "   'I love your style!',\n",
       "   'Thats so cute! Itd make a great tattoo!',\n",
       "   'https://preview.redd.it/kt11no0jgxdc1.jpeg?width=1149&format=pjpg&auto=webp&s=f5af85f4d4992641c0ea7e8c4b8d2191865f7a4a\\n\\nMy little origami elephant. Im not actually an artist, I just love art, but Im still proud of this little guy.',\n",
       "   'Oh! There was a purpose to that! I thought he was just having a good time lol',\n",
       "   'Blue with brown central heterochromia. Very beautiful!',\n",
       "   'https://preview.redd.it/hbj6zzjjfxdc1.jpeg?width=3024&format=pjpg&auto=webp&s=333927f60b05257cce6b5e1397a5ca4cd9b37333\\n\\nThis is he',\n",
       "   'Do you wanna go outside? \\n\\nIm asking my dog this, but my cat will jump up from a dead sleep so he can come with us. His food bowl is near the back door so he wants me to watch him eat and then get pets while waiting for my dog.\\n\\nHe also knows no bite because hes one of those cats that loves so much he wants to give a love bite and he doesnt understand that people are not cats and it hurts.',\n",
       "   'I justI canthow didwow',\n",
       "   'Damn. Beautiful teeth though!',\n",
       "   'Yeah like I said, you have to do dna testing to know for sure.',\n",
       "   'Eggs and ALL fish. No, not even when its scrambled, and no, not even salmon.',\n",
       "   'According to a quick [google](https://vgl.ucdavis.edu/test/albino-cat) Feline albinism is a rare inherited condition characterized by a lack of pigment resulting in cats with a white coat and blue eyes. \\n\\nSo yeah you technically have an albino cat BUT it can only be determined for sure through DNA testing.',\n",
       "   'Saving this to come back to when Im sad. This made my whole day ',\n",
       "   'Drunk History and Minecraft. Could be something there.',\n",
       "   'You mean left handed people???',\n",
       "   'https://preview.redd.it/mgnslwoxvndc1.jpeg?width=1920&format=pjpg&auto=webp&s=4dc55cb5615a4834d3c11818c67aa45ad7d310a6\\n\\nHeres my handsome man!',\n",
       "   'His surfer dude accent trying to be British is hilarious',\n",
       "   'My twin was miscarried at 4 months. I couldve easily gone out with them. My mom didnt know she was pregnant and was rearranging all of the furniture by herself. Also my mom was told she was no longer pregnant when she went to the hospital. In the two months it took her to realize she was still pregnant she couldve done something that wouldve miscarried me.',\n",
       "   'I agree with others here that Mylo and Violet are super cute names for a boy and girl. Ill also suggest Ever (as in Everglow) for a girl. And Sky (Sky Full of Stars) is a beautiful unisex name.',\n",
       "   'I wouldnt recommend naming a girl something that sounds like him. Unless youre doing a bending gender norms thing.',\n",
       "   'I LOVE that couch!!',\n",
       "   'The first time I watched Airplane! my friend and I had to take a break 2/3 of the way in because we were crying laughing so much we couldnt see and our cheeks hurt.',\n",
       "   'Yeaaaaaah. ants.',\n",
       "   'Disease X. Idk wtf it is but were gonna be fucked.',\n",
       "   'I broke my tail bone twice rollerblading. Then I fucked it up again when I slipped on ice. But I only walked away with have a malformed coccyx and I cant sit on anything without cushion for very long. Im sorry it caused so many more issues for you. I hope you get the treatment and relief you need soon! ',\n",
       "   'Yeah, and I interpret the rest of the lyrics to mean that despite what happened Im still able to love and be loved. I was actually looking for other songs like that when I found this post. Ill have a listen to your song suggestions!',\n",
       "   'Clean up that litter. Were you guys born in a barn?',\n",
       "   'I wonder what they were going to use those for. Must not have been important.',\n",
       "   \"To Be Alone by Hozier is my go to.  \\n\\nNever feel too good in crowds. With folks around. When they're playing the anthems of rape culture loud. Crude and proud.\\n\\nBut you don't know what hell you put me through. To have someone kiss the skin that crawls from you.\",\n",
       "   'Nice',\n",
       "   'Youre very very lucky getting diagnosed so quickly. I started having issues when I was nine in 2010, and was just diagnosed last November. Im getting surgery February 16th. Im expecting a 7 month recovery. I was told that physical therapy wont be necessary. These other comments are kind of scaring me because I dont qualify for any financial aid. I wont have an income while Im recovering and my savings will only last me for 8 months. If Im not able to walk normally and stand for a full 8 hour shift by the time my savings are used up idk what Ill do.',\n",
       "   'Thats so freaking cute! When my cat jumps into my lap I grab a blanket and bunch it up so he has a little pillow too. Otherwise hell use my hand and Ill just be stuck there lol',\n",
       "   'All I could think about is how wild birds cant control their bowel movements. Such a beautiful video couldve been ruined by an unfortunate bm.',\n",
       "   'What is it about bikes that make the drivers not look at whats in from of them?',\n",
       "   'I think they were Bud, Canberra, Klaus, Flora, Annalisa, and Punchy. I still have Annalisa, shes never leaving I love her so much. I wish I still had Punchy, I really miss him :(',\n",
       "   'My cat is the same way! I told the vets he loves people, and Im sure theyve heard that before and things went awry. So I watched them get prepared to give him a shot and get into position and then the vet with the needle said, Oh, hes purring! And he gave a little head butt lmao.',\n",
       "   'I was like 8 when that movie came out. I remember thinking, Oh, theyre gonna die. Pixar is crazy!']},\n",
       " {'username': 'BantedHam',\n",
       "  'date': '01/17/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': []},\n",
       " {'username': 'piconese',\n",
       "  'date': '01/17/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': ['They call it higher specs because you have to high af to think its anywhere near as good as the perfection that is the swatch.',\n",
       "   'Ive never had thrown dagger range be short, although the last I tried was in bg1ee, not 2ee. I dont see why it would be different though. Ive consistently used throwing daggers from the edge of my field of vision. They move a bit slower than an arrow, but that doesnt really affect much.',\n",
       "   'Im allergic to tropical fruit, has got to be one of the stupidest things Ive ever heard ',\n",
       "   'In the provided frame, vegeta says hell never fight again \\n\\nI feel like he was having a moment, fully felt like he would truly give up on catching up to Goku, then got over it.',\n",
       "   'It wasnt *that* bad',\n",
       "   'In civ5, diplomatic victories are rather easy to achieve, even on deity (provided you can successfully fend off any wars).',\n",
       "   'Friggin smart ass',\n",
       "   'Its a play on moyashi, which means bean sprout',\n",
       "   'Raditz?',\n",
       "   'Jokes you on! My mom is my dick and we have a very strained relationship',\n",
       "   'My favorite solo run was as a dwarf magic user',\n",
       "   'I havent tried out the photography mod. In your experience, is it better to go with headshots and portraits or commercial photography?']},\n",
       " {'username': 'MikGusta',\n",
       "  'date': '01/17/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': ['According to Spotify stats\\n\\n1. Work Song\\n\\n2. As It Was\\n\\n3. Sunlight\\n\\n4. Shrike \\n\\n5. Movement\\n\\n6. Like Real People Do\\n\\n7. Someone New\\n\\n8. NFWMB\\n\\n9. From Eden \\n\\n10. Wasteland Baby\\n\\nFrancesca and All Things End are creeping up there too.',\n",
       "   'I was hoping Zack Snyders Justice League would feel shorter than it is, but nope. You feel every single one of those 240 minutes.']},\n",
       " {'username': 'BantedHam',\n",
       "  'date': '01/10/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': []},\n",
       " {'username': 'piconese',\n",
       "  'date': '01/10/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': ['Copyright laws are powerful with this one',\n",
       "   'I once saw an infant chimp kill a champion mma fighter, one hit ko. dont mess with the monkeys son ',\n",
       "   'Those are some good stats! Get a str belt and youll be pelting bullets and daggers like a wild animal!',\n",
       "   'I think they were referring to the musician',\n",
       "   'He died on the hill that day\\n\\n*small violin plays*',\n",
       "   'Whoop whoop, my ninja!',\n",
       "   'Once you leave to get imoen all the quests will be waiting for you to finish upon your return, so feel free to rush after her. You might wish you had gotten more exp and items before going ahead with the main plot, but you should be fine regardless.',\n",
       "   'This fucking amateur didnt even fry them in duck fat ',\n",
       "   'Whats the lore behind arkhamization? Is it stupid?',\n",
       "   'You can hide, but you cannot run!\\n\\nLilarcor is a favorite of mine too, I love his ridiculous comments. I LOVE THE SMELL OF DAISIES IN THE MORNING.  The story is fun too. \\n\\nThere are so many little bits in the lore on items, its hard to think of the ones I really like off the top of my head!',\n",
       "   'My wifes bf got me a baby Taylor so I just stuff that in the overhead compartment \\u200d',\n",
       "   'Toan is in the trinkets ',\n",
       "   'Woooah woah woah, hold up, you telling me I can play folk-jazz fusion on a schecter hellraiser?? ',\n",
       "   'Thats not a nose, that nose shaped thing is genitals. He smells through his ear holes. Cmon guy. Seriously, dragon ball fans really cant read.',\n",
       "   'First and second scenario he opens a food truck or small restaurant. Geetz has cooking feats!\\n\\nThird scenario he simply blows up mountains looking for gold veins and other precious metals and stones.',\n",
       "   'Press tab on your keyboard to highlight intractable objects. Or, click the magnifying glass image on the bottom right of your hud. This will make it a lot easier to find things in the environment.',\n",
       "   'A thief that dual classes to a mage can use thief weapons, provided youve reactivated the class. Shes all set at the beginning of bg2 to use thief weapons and skills.',\n",
       "   'Captain ginyu? Wut?',\n",
       "   'I think that was the filler scene of krillin saving that mom and kid?',\n",
       "   'lol fair! Ive havent tried on mobile, I can see that being a must have',\n",
       "   'You can toggle it on pc too, if you dont mind floating boxes over everyones heads ',\n",
       "   'Theyre playing bg2, imoen is already dual classed to a mage. \\n\\nu/tuna_96\\n\\nYellow means you *can* use it. If the char portrait is greyed out then youre not getting any love.\\n\\nEdit: sorry, misread part of the prompt. Im a little confused, is it showing red or yellow? And she wont equip it at all? You sure its a short bow and not a long bow? Her portrait may be red because of damage youve taken?',\n",
       "   'It comes down to toon hacks or ultra powerful dudes, like mewtwo \\u200d',\n",
       "   'Next time you can skip the whole trilogy, its only one scene in ep 4 ',\n",
       "   'Why no respect threads for kagami and kuroko? \\n\\nI dont know who these chars are, but seeing as to how Goku and geetz generally scale higher than most universes, Im going to tentatively go with the DBoyZ.',\n",
       "   'No, they shouldnt. Unless your rep takes a huge hit they should be fine.',\n",
       "   'More like his fat fuckin hog  (zarbon has seen bigger)',\n",
       "   'If you pick a good alignment (neutral or chaotic) youll have access to a nice axe at the start of bg2. Other than that, I cant think of a reason to necessarily play as good. If you play as evil you can eventually get a nice set of armor towards the end of bg2, but by then youll probably be wanting to sport something heavier so its a toss up \\u200d',\n",
       "   'Yeeeah, gareth Edwards isnt the brightest director to have lived ',\n",
       "   'I love that youre still trying to make this work despite no one agreeing with you  \\n\\nWhat does being fat or underfed in the modern era have to do with your prompt? 100 21st century *average* humans is literally the title. I think we can safely put aside outliers and assume everyone is an average joe from todays era. That goes for the Neanderthals too: average dudes from their era, which means they likely do not have the best nutrition (at least when compared to todays standards). Again, having sufficient food so as to focus on weaponry is *not* the same as good diet. \\n\\nOn weapons, we have an *insane* advantage in knowing what ranged weapons even are. We can build bows and arrows over that week. If we cant manage to build a lot of those, or training is too difficult to implement in that short of time, we can simply make spears and atlatls (which have a range of 20+ meters and can get moving over 130kph). \\n\\nThe humans will always take a few casualties, but theres no way wed lose with a weeks prep time.',\n",
       "   'The crying ',\n",
       "   'Sneak attacks are pretty good in dragon ball, but the moment half goes tui, canon will know *something* is up; he wouldnt be completely caught off guard.',\n",
       "   'Other than butterfly effect or something, no.',\n",
       "   'Even if half Goku managed to blitz canon Goku, canon would up his power to match *very* quickly. Canon can tank a few hits at base, I think hed be surprised and maybe a little winded, but half Goku will always lose.',\n",
       "   'Thats tab tho innit',\n",
       "   'Hear hear',\n",
       "   'I think if anything youre *over*estimating them  also, getting enough food is very different than having a nutritional diet.',\n",
       "   'Neanderthals were 5-5.5 tall and likely had far poorer nutrition than someone that grew up in the 21st century. Humans win, low diff.',\n",
       "   'Huh?',\n",
       "   'Talkin like you (two tall mountains) by Connie converse. \\n\\nHello its me by Lou reed and John Cale \\n\\nIm sorry for your loss. I also had someone close to me commit suicide within the last year and these songs are a beautiful tribute.',\n",
       "   'Yeah, theyre def baiting',\n",
       "   'This happened back in August 23, nothing came of it. Who knows at this point \\u200d',\n",
       "   'This is bait lol',\n",
       "   'Hey, you guys wanna watch a couple geriatrics fight? ',\n",
       "   'Pack ',\n",
       "   'Last time I asked this question on reddit a night hag told me I had to sign up to their onlyfans for a rating \\u200d',\n",
       "   'Guy in video would like a chat',\n",
       "   'Sorry, just listening to the space duck. What a majestic creature.',\n",
       "   'I like it! \\n\\nHow does skull trap fit in? Charms? I imagine charms are out, but not sure if skull trap would be kosher ',\n",
       "   'That would be dope! I mean, I already have it on steam and switch, but Im cool with my box having it too ']},\n",
       " {'username': 'MikGusta',\n",
       "  'date': '01/10/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': ['I have a huge list of movies I have to watch. I was born in 2001 so Ive missed a lot. Im making progress on my list but theres a lot left to go and Im always adding to it.\\n\\nFor sure, I have to watch The Godfather, The Terminator, Goodfellas, Die Hard, Beetlejuice, Rocky Horror, Mrs Doubtfire, Phantom of the Opera, Fast Times at Ridgemont High, Bill & Ted, No Country for Old Men, and Requiem for a Dream. I could keep going but Ill leave it there lol',\n",
       "   'I just had to throw out to shirts that I loved because every time I washed them more hair would get woven into it. I literally had to pull the front of the shirt away from the back and pull out all the threads of my hair for 15 minutes every time I wanted wear one of the shirts.',\n",
       "   'The more I learn about politics and the more I hear about the opinions and decisions of conservative politicians the more I want to get out of this god damned country. I cant stand being here. I cant stand the regression of society. Were supposed to be the best country in the world and we have ludicrous policies. Im tired of it.',\n",
       "   'Oh god. Id trauma dump right to that guys face and make him uncomfortable as hell. Old people love to tell me how hard it is to be an adult or elderly and theyre talking to the wrong person. As soon as I give them a taste of what Im going through they immediately back off. Dont assume what somebody has been through and dont tell them theyre unqualified or not allowed to talk about life and happiness just because of age or gender or anything.',\n",
       "   '1. Warhammer and MTG. I cant be with someone living paycheck to paycheck because they spend over half of it on a game. Also Id like someone who wants to spend time with me and doesnt take off every day to go play those games for hours or cancels going out with me to go to every tournament.\\n\\n2. Going to the shooting range. I personally cant be compatible with someone who loves guns and loves shooting them off. I cant be with someone who loves their guns so much they wouldnt give them up in order to make the country safer.',\n",
       "   'That is so fucking sad. Fuck those people',\n",
       "   'He was mine too. I freaking hated him. He didnt leave for like 8 months and I was going crazy',\n",
       "   'I understand both of their perspectives and needs, but I really think I would have done the exact same thing if I was in Marcus shoes.']},\n",
       " {'username': 'BantedHam',\n",
       "  'date': '01/03/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': []},\n",
       " {'username': 'piconese',\n",
       "  'date': '01/03/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': ['Talk to one of the slaves in the prison area of the floor above the boss level.',\n",
       "   'But since when has being simply in the presence of magic weakened him? Its not kryptonite lol',\n",
       "   'Them some free range chickens, damn! They get to keep their beaks?? That farmer deserves a medal for being so, so humane ',\n",
       "   'And even after all that time theyre still not ripe ',\n",
       "   'Ah, foxy freeza and sexy cell have arrived!',\n",
       "   'B-b-b-baka na!',\n",
       "   'I await responses to this with baited breath',\n",
       "   'Sunlight and burying your bare feet in soil. You might get worms, but youll be *way* healthier than a normie vegoon.',\n",
       "   'Its great on a tablet, the touch screen is sorely missed in the switch version. On a phone I can only imagine youll feel scrunched up. Probably plays fine though \\u200d',\n",
       "   'I had a bunch of clams at a party and a cheeky nandos after, been shitting like crazy  *love* the new year! ',\n",
       "   'Did anime cell go to space? \\n\\nHe has freezas genes: if he can regrow limbs cuz piccolo, he can survive in space cuz freeza \\u200d',\n",
       "   '[Black people, you know why? Cuz we hate black people too](https://m.youtube.com/watch?v=cBEwQHjdw14)\\n\\n- Chris rock',\n",
       "   'Robo Max',\n",
       "   'I dont know if theyd include the line if it didnt serve the story / push things forward. Its an indication by the author that cell max is *burly*. I take it at face value, Im not trying to read into minutia with dragon ball lol \\n\\nPlus, piccolo has shown he can *kind* of sense god ki. He knows shin is something different, and he has a similar thing with beerus and whis.',\n",
       "   'Darts are great, I agree. They have some excellent effects, and the 3 apr is sexy af. Get that crimson dart in watchers keep  youre laughing',\n",
       "   'Everyones saying win for Moro, but during super hero they made a point of saying they werent even sure if Goku and geetz could beat cell max (as of the last time they checked in on the duo). I think if cell had full mental capacity, as indicated by the prompt, he would take the fight. Cell was *smart*, I dont see why a fully matured cell max wouldnt be too.',\n",
       "   'Og cell could so I assume yes: cell max can survive in space.',\n",
       "   'In my experience, once a monk gets rolling, they can chunk dudes with ease. This is mostly in soa tho: bg1 monks are pretty weak, and in tob it can be hard to tank / take crits from enemies. Its important to get the right gear and its very helpful to have a tank around. Monk fists have the range of a two handed sword, so you can still pummel someone while your tank stands between you and the enemy.',\n",
       "   'Having a menu like that really helps with overhead. You can run a skeleton crew at an inn serving dishes like that: no prep work at all!',\n",
       "   'Magic\\n\\nKa.',\n",
       "   'Can I get bussy at kfc too?  might need to rethink this whole vegan thing',\n",
       "   'I dunno, we see Goku tapping into his emotions with tui in the granolah arc \\u200d if he can manage to still use ui even when allowing his emotions to run free then its only a matter of training to use ego mindset along with ui. I expect beerus is capable of using both simultaneously.',\n",
       "   'Thats Steamboat Willie',\n",
       "   'Thats what I was asking, what other source is there other than the cartoon? If everyone is basing their shit off the cartoon why is the respect thread nothing like what youre talking about?',\n",
       "   'If you view all comments youll see the respect thread.',\n",
       "   'You scallywag ',\n",
       "   'Doubling down \\n\\nHappy new year!',\n",
       "   'A good thing too! Id have a pack of inept bounty hunters on their trail in no time ',\n",
       "   'Yeah, sure, if the plot calls for it I dont see why not \\u200d they do what they gonna do',\n",
       "   'Is there an echo in here? ',\n",
       "   'Which isnt R A W',\n",
       "   'Nah, its def been cooked. They either get it precooked (likely) or they cook a bunch in the am and reheat as the day goes on. No way theyre cooking bacon from raw to order. At a Dunkins.',\n",
       "   'Dang! So that respect thread is based off the comics or something? My he-man is poor at best, although I did grow up with an action figure of a guy with three eyes that you can spin',\n",
       "   'Am I thinking of Arena then? One of those two lets you wander about, right?',\n",
       "   'Having worked at dicks for a number of years, heres my recommendation: get a cheeseburger and a special. Throw the top bun of the special away. Put the cheeseburger on top of the special. Enjoy.',\n",
       "   'Huh, the respect thread suggests otherwise',\n",
       "   'Ah, but have you seen the jestress? ',\n",
       "   'Is that limmy?',\n",
       "   'Skyrim is just Skyrim. ESO or daggerfall let you explore more regions.',\n",
       "   'Yeah, problem with darts is you dont get the str boost on damage. Although dagger stacks are a pain, I agree, theyre def a better option. If youre on pc you can always console command a 100+ stack of regular throwing daggers \\u200d fixes the inventory management issue',\n",
       "   'Ive never had range issues with daggers. They move a little more slowly during their animation, but Ive been able to hit enemies from just inside the range of vision.',\n",
       "   'The one Luther church in my town was taken over by some guy who called himself Martin Luther king jr (I guess he was heir to the throne or something?). Anyways, its a Baptist church now.',\n",
       "   'Finding her sexually attractive at all is fucking weird, yo ',\n",
       "   'At what age do you find Pan to be the most sexually attractive?',\n",
       "   'Thankfully he can get kithixx pretty early in bg2!',\n",
       "   '/uj i met jimmy Carr once while i was working at a high end jewelry store. He looked at the charms for charm bracelets and said to me, sweet 16? Sounds like a goodbye present from a pedophile. ',\n",
       "   'You wouldnt steal a house!',\n",
       "   'In a top style tournament, gonzo gets flattened at first, everyone thinks hes out, like 17. Ms piggy and Kermit do the heavy lifting until Kermit sacrifices himself to save ms piggy. This causes Ms piggy to go super swine and she wrecks house, but stamina issues kick in and she gets chucked. Thats when gonzo goes full rage mode super gonzo god super gonzo and utterly destroys the z fighters.',\n",
       "   'Good break down, I agree with this entirely.',\n",
       "   'Ah yes, the gold standard for debunking ',\n",
       "   'Broly!',\n",
       "   'For any class able, Id pick throwing daggers over slings any day \\u200d hell, you even have great choices for axes and hammers!',\n",
       "   'Correct. Read your class info on the records page and it should tell you how long it lasts, what exactly it does, how often you can use it, etc. \\n\\nIts a major perk for being a dwarven defender, although the limited movement speed can be a real hamper.',\n",
       "   'More of a school cafeteria, but yeah, essentially',\n",
       "   'Dibs on the diaphragm!',\n",
       "   'Ah, so youre a dwarven defender? Defensive stance is special ability that slows you way down but grants you some resistance to physical attacks and such.',\n",
       "   'Yoda was vegan confirmed!',\n",
       "   'Im pretty sure that if bees make honey from a lion, thats considered vegan.',\n",
       "   'Lentils eat birds and snakes too??',\n",
       "   'They all have cloven hooves though, demons and pigs. That makes it vegan.',\n",
       "   'Exactly why I keep only him around: he doesnt bitch at me lol \\n\\nHonorable mention to adoring fan, sometimes you need a yes man \\u200d',\n",
       "   'Advanced prince?',\n",
       "   'Their writing and alphabet are so uncouth \\u200d poor little bastards.',\n",
       "   'Just head on down to your public library!',\n",
       "   'Not to mention apeman! ',\n",
       "   'King missile has some funny ass songs, like Jesus was way cool. Definitely check them out, they have a lot of weird tracks across their catalogue.\\n\\nDead milkmen have some funny songs too, like beach party Vietnam or bitchin Camaro.\\n\\nViolent femmes can also be pretty funny with their lyrics, particularly on their first album.',\n",
       "   'In studio jack can pull off decent stuff, but he sometimes struggles live. Neil is a legend, I cant critique him lol',\n",
       "   'A pure fighter can naturally achieve 3 attacks per round with a sling: 1 from base, .5 from level 7, .5 from level 13, .5 from 2 pips, and .5 from 5 pips. \\n\\nThere are some gauntlets that will grant an additional .5 per round, bringing us up to 3.5 (7/2). \\n\\nHaste adds 1 attack per round and improved haste doubles your current apr (up to a max of 10). \\n\\nWhirlwind and greater whirlwind also grant 10 apr for a round. \\n\\nAs a fighter/thief, youll hit a natural max of 2.5 (5/2), 3 if you have the gauntlets. \\n\\nAs far as Im aware, thats as high as you can get without haste or whirlwind. I really wish there was a sling with 2 attacks per round, like the light crossbow you can find, but alas.',\n",
       "   'Shenwrong. \\n\\nIts shenroff.',\n",
       "   'The cloak is for ceremony only. As soon as battle commences he brandishes the cloak as would a bull fighter, taunting and betraying his foes with whimsical grace. Quite extraordinary for one so heavily armored!']},\n",
       " {'username': 'MikGusta',\n",
       "  'date': '01/03/24',\n",
       "  'time': '03:53 AM',\n",
       "  'comments': [\"Oh my god that sucks!! Hopefully, they restock it for next Christmas even though that's a long time away now. I hope you still had a good time at MOCCA anyway. I'd love to say hi to them at a con.\",\n",
       "   'The LGBT+ community holds such a large place in my heart. I have never felt more safe and loved than with the community.',\n",
       "   \"Yeah I waited over a year for them to restock this hoodie. As soon as I found out about the Christmas sale and restock I went to their site and ordered it. They should really keep it in stock. Anyone could wear it, Drawfee fan or not. I've gotten a lot of compliments on it from people who have no idea what it's from.\",\n",
       "   'National Lampoons Christmas Vacation. I really dont find it funny at all. Youre meant to laugh at someone for being poor, children are told Santa wont be able to give them presents for the past two years, a cat is literally electrocuted and dies, an old man damn near sets the whole house on fire, a boss is greedy and screws over his employees, a man is kidnapped and threatened, and the entire goal is the main character wants a large enough bonus so he can get a swimming pool while his cousins are *literally* homeless.',\n",
       "   'Big orange fluffy cats have a big place in my heart. Theyre the sweetest and most caring creatures.',\n",
       "   'Ive literally never noticed it was the colors of the lesbian flag. I think its just 70s retro considering the hoodie is navy blue. But Ive gotten used to everyone thinking Im a lesbian so its okay.',\n",
       "   'My friend Collin. Were not super close, but I feel so safe around him. I know he would fight anyone who would try to bother me or hurt me. Hes not only a protector but also really kind and hilarious. Im really grateful that we recently reconnected after high school.',\n",
       "   'Theres a woman who lives in Illinois named Marijuana Pepsi. She never changed her name or went by Mary. She has a Ph.D and is a college professor now.',\n",
       "   'If I was a boy my mom would have named me Jaden. I kinda wish she still did. I think its a really cool name for girls. Instead she got a cat and named it Jaden.',\n",
       "   'Yeah I had a neighbor named Eric. He was really off putting and I kept my distance from him. He once snapped at my mom for bothering him and it really scared her.\\n\\nBut I had a best friend name Erik and hes great. Hes the kinda guy you could go to an be real for a second and just talk about shit going on and it wouldnt kill the mood. You could just go right back to goofing around.',\n",
       "   'A kid named Bryce transferred to my middle school. He was annoying as fuck and was a master at pissing people off. One time we were out in the field for gym class and Bryce wanted to forcefully take a ball from another kid who was already almost 6 ft tall. Bryce decided to not be patient and kicked the big kid in the dick. Big kid immediately picked up Bryce by his head and slammed him into the dirt and Bryce had to wear an arm sling for weeks. Big kid was the king for the rest of the year and Bryce didnt come back for 8th grade.',\n",
       "   'I have a cousin named Tiffany too. Shes a stuck up biotch and tries to get all of her Facebook friends to buy into all of the pyramid schemes she gets into.',\n",
       "   'I watched it for the first time a couple of years ago and never knew the twist. It was an amazing viewing experience because of that.',\n",
       "   'I wish they had asked her why \"mother god\" was blue if colloidal silver, if made right, doesn\\'t turn you blue. I think the answer would have been something like all the negative energy of the world had diseased her skin or whatever, but I still would have liked to hear the question and answer.',\n",
       "   'Yeah, I thought they figured out how to quicken the process and she was in an advanced state of mummification. I was shocked it had only been days and her skin already looked black and had a tight pulled back face.',\n",
       "   'How do you fuck up a quesadilla that bad? I wish they showed the supposed monstrosity.',\n",
       "   'The way he pretended to fully believe in them is sickening. These people were his family. They loved and trusted him, and he took everything from them. They had nothing, no money, no home, and no support after the police got involved.',\n",
       "   \"I just finished watching it as well. I was really sad about how much they thought Robin Williams was involved. His memory shouldn't be connected with them at all.\",\n",
       "   'Beautiful! Id put that porch back up immediately!',\n",
       "   'I think thats just how British people look',\n",
       "   'God damn its like someone needs to post the difference between the breed and color pattern image everyday so people quit asking this question.',\n",
       "   'Yeah your bites look very different to bed bug bites, but maybe your body didnt have an inflammatory response. If you have bed bugs or not you figure it out soon enough.',\n",
       "   'Yeah I didnt find that on max. During the credits theres some music, and then a man is talking about being in prison but hes happy to still be alive and able to see his wife and children, then they play a bit of music again and then its over.',\n",
       "   \"I cant find the call in that one. Are you sure thats the right one?\\n\\nEdit: It's before the credits at around 2:57 left of the episode, played at a conference\",\n",
       "   'Theyre amazing! Now I want cards of their little guys!',\n",
       "   'A heart on my inner left wrist. I have chronic depression and in middle school I had a lot of friends who also struggled and they turned to self harm. Around that time I started drawing a heart on my wrist to remind myself to not harm my body. Its a tribute to my old friends and its a reminder to love myself.',\n",
       "   'In reality, no. In their version of reality, yes.',\n",
       "   'I wouldnt say its my favorite but I do actually like that song',\n",
       "   'I wanna know how flat earthers feel about global warming. Whats gonna happen to the ice wall? Or do they believe global warming doesnt exist?',\n",
       "   'Youre probably feeling way less satisfied now  after reading these comments',\n",
       "   'That looks like the stuff you find when you open a pecan',\n",
       "   'My dad did that to me as a baby all the time just to see my funny sour face. Now my favorite candy is Lemon Heads']},\n",
       " {'username': 'BantedHam',\n",
       "  'date': '12/27/23',\n",
       "  'time': '03:54 AM',\n",
       "  'comments': [\"Go full road warrior. Balls to the wall embrace the old shit car chic aesthetic and put like spikes and a fatty bumber and shit on that bad boy. Cut out your wheel wells and put on bigger tires next time you need to get new ones. Spray paint the banner for your future war party on the side and then the next time someone talks shit, just run em over lol, that'll realy show em\"]},\n",
       " {'username': 'piconese',\n",
       "  'date': '12/27/23',\n",
       "  'time': '03:54 AM',\n",
       "  'comments': ['Just learn to play bistro fada with only two fingers, those hairy French chicks that dont wear bras will be all over you',\n",
       "   'The real winner\\n\\nhttps://preview.redd.it/ekm24bweer8c1.jpeg?width=960&format=pjpg&auto=webp&s=5d4865b911b6fadef9cb847b12ad3ce18729ff21',\n",
       "   'https://i.redd.it/vjhi9ax4er8c1.gif\\n\\nWow, first Daima reference',\n",
       "   'Bows yes, but pick throwing daggers over slings. Hell, even throwing axes are equal to or greater than slings. Slings are for non-dwarven clerics and those that really want to rp a halfling ',\n",
       "   'Search is also a helpful tool. Not trying to be a darsh, but a lot of great advice is def hanging around on this sub.',\n",
       "   ' trims out a lot of the fat \\n\\nSo, super buu? ',\n",
       "   'I thought xv2 was better but kakarot was ok \\u200d',\n",
       "   'A mouths a mouth \\u200d\\u200d',\n",
       "   'Dang, two dollars! I want my two dollars!!',\n",
       "   'So not entirely. Just, you know, kinda entirely.',\n",
       "   'No I dont want to view popular, I want to view home ',\n",
       "   'Im so sorry for your loss ',\n",
       "   'Truck bed boys',\n",
       "   'There. There wolf. There, castle.',\n",
       "   'Im more into that buuty, but cell is def a hunk.',\n",
       "   'I have no advice. \\n\\nFair weather, traveler!',\n",
       "   'Well, except for raditz ',\n",
       "   'Its been a few months, but I dont remember that being an issue at all. I feel like I wouldve been very annoyed about it and Id remember, but maybe Im forgetting something. \\n\\nDo you have everything updated? Game and console? Thats the only thing I can think of. Hopefully it doesnt happen again!',\n",
       "   'If vegeta managed to wreck his rival as easily as he did, why is weird that 1000 was too small of a gap in the turles v piccolo case? ',\n",
       "   'My good sir knight, thy long blade appears most heavy: may I hold it for thee when next thy void? ',\n",
       "   'Weirdly enough, the first time I saw anything about it was an hour or two ago! Someone else was saying they also had their save wiped. \\n\\nAs someone that has played quite a bit on the switch, Ive never encountered any game breaking issues. Nothing quitting and restarting wouldnt fix \\u200d \\n\\nIve even gone so far as to make arbitrary export characters just for certain items. My saves have never been wiped.',\n",
       "   'If youre enjoying it as much as you imply, then youll soon be in the club. This game has a *lot* or replayability for those that vibe with it. Sucks about the save, tho.',\n",
       "   'Hahah that advice about stats is probably the sole reason Steam counts me for playing as many hours as I have  just one more roll ',\n",
       "   'The intro to genesis is just *so boring* ',\n",
       "   'Fair enough. Odyssey was fun, I really enjoyed it. Havent played it much since beating it, but I went ham on it during that one play through ',\n",
       "   'This also took me a minute on Mac as Im much more used to playing on pc  glad it still works, anyway! Console is just too much fun as a tool to go without.',\n",
       "   'If you install the live another life mod you can start over with one.',\n",
       "   'Im cool with it, so long as they deliver! Those two Zelda games are top notch.',\n",
       "   'Yamucha is one is the most respected and cherished characters in the entire saga. He even gets confused for Goku in a pivotal moment, something that excites him so much his chest is bursting. The man is a legend.',\n",
       "   'Ugh, my roommate left the door open to their stinky ass room *again*?? \\n\\nBetter start playing until they get annoyed enough to shut their door \\u200d',\n",
       "   'Ive never heard of gibsons, is that some kind of animal? ',\n",
       "   'Wrong armor though, and Im pretty sure gohans hair is bushier in that scene than seen here (which is likely when freeza and king cold arrive).',\n",
       "   'Mamario',\n",
       "   'Easy fix: tell him this is training so he can become stronger then beerus. Done, Goku goes ham and saves Christmas.',\n",
       "   'In my experience, all dlc is included with the enhanced edition. It was part of their selling point, honestly. \\n\\nI own it on Steam and switch; everything is included in my copies.',\n",
       "   'Shucks, sorry to hear that. Maybe Ill uninstall everything else and try it out again well see if I get the gumption \\u200d',\n",
       "   'Especially when you consider the fact that Nintendo is over here pumping out *amazing* games like botw and totk without any major hiccups.',\n",
       "   'This is heartening! You reckon my 2021 MacBook Air has a chance at running it? Last time I tried was while it was still in early access and it was nigh unplayable  fingers crossed they addressed mac support since then  if so I will be reinstalling asap',\n",
       "   'I loved getting into the crazy world and setting, but the online community is sometimes really lame, and the end game is suuuper grindy',\n",
       "   'I love the attacks instead of trying to defend your dumb ass take on how evidence works  now run along and play with your cucky dolls ',\n",
       "   'And youre basing that assumption on probably the stupidest premise Ive read in a while  they didnt show goku using ui, it CANT be the same universe!!!  fer sher',\n",
       "   'Bulmas hair cut in the Daima trailer is closer to her hair at the end of buu / rof than it is to super hero, but thats a tenuous link at best.',\n",
       "   'Loool whos crying? Youre having a tantrum, relax, its Christmas \\n\\nI like how you feel like retconning means it simply cant be the same timeline. What about all the shit they retconned *during* dragon ball z? Are we just universe hopping the whole time?? ',\n",
       "   'So the buu saga ends and we time skip, right? Nothing is mentioned after that. About anything. A lack of evidence is not evidence lol just because we dont see Goku turn ssj3 during eoz doesnt mean he forgot he had the form or something  but thats exactly the line of thinking youre applying here: unless its deliberately spelled out, it simply didnt happen. You gotta learn to accept that super is the official continuation of the story, not some spin off like gt or the non-super movies. Super is canon through and through and is filling us in on what happened directly after the buu saga.',\n",
       "   'How do you know Goku *doesnt* have ui or ssb during end of z? Theres not enough information one way or another so thats a weak argument.',\n",
       "   'Nah, the armor gohan wears during the cell saga does not include shoulder pads. This is definitely post namek, hence the long hair, but before he was given his new armor for training with Goku.',\n",
       "   'Sadly you are indeed incorrect. I too thought it was from the cell saga, but the give away is the shoulder pads on the armor. When freeza and king cold arrive gohans armor has shoulder pads, whereas the armor he wears during training with Goku does not. This is def from when freeza and king cold arrive, not from the cell saga.',\n",
       "   'Source?',\n",
       "   'I have no hope, but not in some pessimistic way. Its a fun sort of soap opera if anything, theyre gonna do what makes fun. I love it and I doubt Ill cancel my db subscription any time soon.',\n",
       "   'I know super hero didnt bring back *perfect* cell, but we *just* got a cell redo ',\n",
       "   'Some would go so far as to say, fear is the mind-killer. \\n\\nWrong franchise, I know, but it felt apt.',\n",
       "   'Honestly love this song ',\n",
       "   'Paladins, particularly those that can cast spells, will continue to scale into throne of bhaal due to the way armor of faith works (first level spell). Plus, you get one of *the* best two handed swords relatively early on in bg2, a sword that is paladin exclusive (save for those that can use any item).',\n",
       "   'Its also worth mentioning that throwing daggers get an extra attack, similar to bows, while also adding strength to the damage. Darts do not get a strength boost, but they do get two additional attacks (for a total of three per round).',\n",
       "   'Troll or be trolled, my friend. Yo ho ho, the pirates life for me! ',\n",
       "   'Dang, whis looking like he inherited some genetic disease ',\n",
       "   'Pretty sure super is a direct continuation of the story post buu saga but before eoz. I dont think anyone that works on it has said otherwise, unlike how toriyama directly said that gt is a side story.',\n",
       "   'So you *dont* like it? ',\n",
       "   'No, not in any meaningful way. You may fiddle with some things in some environments, but its nothing like what you can do in bg3 or pnp.',\n",
       "   'I think theyre actually telling you to leave \\u200d happens to the best of us.',\n",
       "   'Fie! Those dwarves are always one step ahead of me! Sneaky little curs, the lot of them! ',\n",
       "   'Oh certainly, sword and shield style is next to useless unless youre *really* worried about arrows hitting you while casting  I never put pips into that garbage',\n",
       "   'Truckbed boys',\n",
       "   'The style, yes. Thankfully you dont need to put any points into that garbage tier to make use of the more useful shields in the game.',\n",
       "   'Shields can offer some good bonuses, like how shield of harmony gives you mind shield (immunity to certain effects). Unless Im trying to run a very specific two handed weapon, its always better to sword and board.',\n",
       "   'How old do you reckon the Dragonborn should be, if you apply this to their lifespan?',\n",
       "   'I think the math reflects the release of Skyrim and how many days have passed since then, not how much anyone has played the game.',\n",
       "   'Siege of dragonspear, Baldurs gate 3, Baldurs gate 2, then the first one. Its the only way to enjoy the series. Trust me.',\n",
       "   '',\n",
       "   'Yourself in I: find meaning, speak for chaos',\n",
       "   'Wait a second\\n\\nIs u/culturaljello296 the alt account for u/newtaltruistic8820 ???',\n",
       "   'How far do you reckon hulk makes it? Genuinely curious, not trying to be snarky!',\n",
       "   'Speak for yourself, I find chaos in meaning.',\n",
       "   'Duh, its a karma farm ',\n",
       "   'This is def the answer  happy trails!',\n",
       "   'Try it out and let us know! It is a funny idea, I like it',\n",
       "   'Yeah yeah, we can all edit comments and pretend they made sense the whole time ',\n",
       "   'If popo wanted Zenos job and power that would be a step down for popo, get real',\n",
       "   'I think its super popo god super popo (aka spgsp or super popo blue)',\n",
       "   'The largest and most prominent is def not gt lol its super. Gt was a brick, super is bringing home the bacon.',\n",
       "   'Oh no, not as a millennial; its a gen x thing.',\n",
       "   'Once you level up some stuff and have more know how, hades is fairly easy to get through in about 30min, +/- 10min. Same with dead cells, honestly. At first though, yeah, an hour or sometimes even longer sounds about right (in my experience, anyway).',\n",
       "   'Extra 50 magic (essentially 5 free level ups) and being the tallest (read: fastest) is def the way to play!',\n",
       "   'I just awkwardly stare and dont make eye contact \\u200d',\n",
       "   'Bouldering ',\n",
       "   'Was looking for this',\n",
       "   'Nah, thalmor was nice dressing in Skyrim, theyre not gonna elevate that much in any big way.\\n\\nItd be cool if the bad guy was a plague. Maybe you scour the realm ridding the sickness as you go, or peryite goes rogue or something.',\n",
       "   'Yeah, you can stop using that term once youre a *business* *owner*.',\n",
       "   'Good point, they were def cooking at that point. Theyd be a lot weaker though.',\n",
       "   'Vegetas galic gun was also higher than his power level of 18k. Sure, it was an attack, but it shows that he can certainly manipulate his power. Imagine if he focused all that energy into a punch. its def a burst boost, but it counts for something.',\n",
       "   'All hail piccolo, the demon king of no one',\n",
       "   '10th level spells',\n",
       "   'Im doing a custom party with classes I dont usually run with: an elf sorc, human monk, and half-orc shadow dancer. So far the sorc is carrying the team ',\n",
       "   'Uh, which part?',\n",
       "   'I feel like he was going for some kind of intimidation \\u200d']},\n",
       " {'username': 'MikGusta',\n",
       "  'date': '12/27/23',\n",
       "  'time': '03:54 AM',\n",
       "  'comments': ['Probably getting sent to the er and then transferred to a mental hospital.\\n\\nThe last time I took an edible I learned that marijuana really slows down the metabolizing of antidepressants and anti anxiety medication. I discovered that if those arent working properly my anxiety nausea and bad thoughts come back and I have physical tics like Tourettes (which anxiety may cause). Gonna have a little chat about all that with my GP',\n",
       "   'Why should I get all done up to be in a store for 10 minutes? Id want to change back into comfy clothes right when I get back home anyway. \\n\\nIm not at the grocery store for your viewing pleasure. Why would I get dressed up just to please the eyes of strangers and to not upset anyone offended by leisure wear? \\n\\nAnd the whole thing about women need to get ready everyday otherwise they have no self respect is bs. You think my self worth is determined by my clothing and makeup? Uh-uh.',\n",
       "   'I got like 5 pairs of different kinds of knock off uggs from my aunt. I have never owned, worn, or liked uggs.',\n",
       "   'A lick bomb bomb down?\\n\\nA tongue boom boom bottom?\\n\\nA mouth explosive explosive ground?',\n",
       "   'What is less interesting than something youve seen a hundred times?',\n",
       "   'Name this shape on my floor',\n",
       "   'I know mine does. When I lived with my mom and I would go on trips with my dad she would tell me about how my cat would spend many hours of the day sitting in front of my bedroom door waiting for me to come out. And every night when Im asleep he runs around the house getting all of my dogs and his toys and brings them to my bedroom door.',\n",
       "   'My skin is really soft',\n",
       "   'My other favorite channel Drawfee says it all the time too. They draw things based on prompts so its an indefinite amount of little guys every video',\n",
       "   'I just learned about a giant millipede from 326 million years ago that was 9 feet long and weighed 100 lbs. The earth at the time had 30% more oxygen in the air than today so things were absolutely massive.',\n",
       "   'CRAWLING IN MY SKIN THESE WOUNDS THEY WILL NOT HEAL',\n",
       "   'Could I get a link to the reference? I cant remember what this is from and I wanna rewatch it.',\n",
       "   'Its not food but my cat loves to lick condensation off of everything. He will stand there and lick the side of cold cups, bottles, and  ice cream containers like its his job.',\n",
       "   'Mirror -> Mrrr',\n",
       "   'To Noise Making (Sing). I used to like the song but Spotify made me listen to it a bunch and now the chorus annoys me.',\n",
       "   'Give that guy a plate of crab legs. I wanna see what he would do.',\n",
       "   'My dog has the saddest face too. Everyone tells me she looks depressed ',\n",
       "   'Thats amazing!! I hope I can get the surgery as soon as possible! I want to be able to work more  and travel without all the pain. I cant wait',\n",
       "   'Whys there only one slice of cheese?',\n",
       "   'I want to see someone use this technique on someone with CVG (deep scalp wrinkles). Im curious how it would work.',\n",
       "   'This is so good ',\n",
       "   'I did this with my cat. Over the week he learned that if he relaxed his body I would think he successfully swallowed the pill and I would let him go. Then he would calmly walk to the other side of the room and then cough up the pill. I couldnt even be mad, he played me good ',\n",
       "   'My dad also hates cats. I moved in with him five years ago bringing my cat that is the best in the world. Super tolerant, super affectionate, hes never been aggressive, and he loves everybody and everything. My dad somehow still hates him. My cat will follow him around and my dad will yell at him for sitting next to him. Lately every time my dad sits down my cat will go up to him and lay his head on my dads boots and my dad will get annoyed. One of these days I might snap and bring home another cat so my cat can have a friend and my dad can have a hissy fit.',\n",
       "   'Woah. Do you feel like your ankle is secure enough and the jerking isnt causing harm? Id definitely message a doctor to ask about it, and see if one of the medications youre taking has side effects of involuntary muscle contractions.',\n",
       "   'Yeah Im kinda in the same boat. The cortisone shot doesnt help me unfortunately and regular pain meds dont help either. Im only 22 and I havent been able to be active since I was 15 because of the pain. Rn I can only work weekends because my job requires I stand for 9 hours shifts. When I get off work I cant put any weight on my foot it hurts so bad. So idk what Im gonna do, I just know I need to get the surgery for a chance at a more active life while Im young. On the 27th Im going to talk to my podiatrist again to discuss my ct scan and mri. Hopefully I can get the surgery soon so I can recover sooner.',\n",
       "   'Thank you so much!!! I appreciate the recommendation! Have you gotten the surgery?',\n",
       "   'I work in customer service and I went out to eat with my dad once. After the waitress took my order, instead of saying thank you I said have a good one. She also thankfully didnt react.',\n",
       "   'Why is it peeking?',\n",
       "   'Apparently people dont understand sarcasm',\n",
       "   'Youre awesome!! Where did you get those shirts?',\n",
       "   'I went to DC with my dad a couple years ago and it was agony. There were many times that the pain was so bad that my legs started shaking because they couldnt hold me up anymore. It was the most pain Ive ever experienced. I had to keep chanting in my mind you have to walk, dad cant carry you, you have to walk, dad cant carry you\\n\\nMy best advice is to baby it as much as possible. Bring crutches or a knee scooter to keep weight off the foot. Try going on FB market place and see if anyone near you is selling something like that. Im currently saving up to get a knee scooter myself. Im also considering the hands free knee crutch that basically gives you a peg leg lol',\n",
       "   'It took 13 years for a doctor to find arthritis in my joint. It took another 7 months after that for a good doctor to finally find the coalition that caused my arthritis and causes all my pain. \\n\\nFind a better doctor!! Find a podiatrist that specializes in ankles!! Dont settle with a doctor you dont agree with and stay persistent!',\n",
       "   'Honestly I use a pair of fuzzy crocs lol. My ankle still will start to hurt after a while but the crocs help me stay on my feet for longer.',\n",
       "   'Other people have correctly answered but I just wanna let you know that you can slow down and speed up the speed of youtube videos. Hit the gear, hit Playback Speed, and then select the speed. Dropping the speed down to .75 you can hear the intro perfectly  :)',\n",
       "   'A lawyer, and then the bank, and then my dad. Im living with him rn and if I won the lottery there would be a lot of changes happening around here. \\n\\nWe need new floors and windows, new paint on everything, a new garage door, a new back patio, and a new balcony. And we need to remodel the kitchen and living room, a spare room that got flooded, and the whole basement (including the laundry room and office). \\n\\nAfter all that Id get my dad his dream home in Germany. Then start traveling and looking into college and eventually Id find a country outside of the US to call home.',\n",
       "   'Oh Jesus! That took me a second ']},\n",
       " {'username': 'BantedHam',\n",
       "  'date': '12/20/23',\n",
       "  'time': '03:54 AM',\n",
       "  'comments': ['Auto parts',\n",
       "   'Nah put it on a stock civic with no exhaust and a crooked spoiler to really show em whos boss',\n",
       "   'Just long enough for them to start sweating.\\n\\nThen you turn it off.\\n\\nThen you hit strobe mode LMAO',\n",
       "   'Get a light bar.\\n\\nFlash oncoming traffic the right way with your brights.\\n\\nIf they dont turn them off...\\n\\nBlind them with 10,000 lumens.']},\n",
       " {'username': 'piconese',\n",
       "  'date': '12/20/23',\n",
       "  'time': '03:54 AM',\n",
       "  'comments': ['No. But it does sound UNREAL.',\n",
       "   'This reminds me of a time long ago when I was playing the first mass effect game. I had some encounter with a reporter that was implying some things about my character I didnt like. One of the prompts read something like, I dont like what youre implying, so I chose that. Next thing I know commander shepherd says, time for you to shut up, and straight up punches the chick \\n\\nThat was NOT what I had in mind, despite it being very funny at the time. Sometimes games are just scripted badly, regardless of how much agency you feel you may have \\u200d',\n",
       "   'You tell em, clone of kp!',\n",
       "   'I dont need to go look at what some virgins thought\\n\\n Honestly made me laugh. Screw those uppity bald virgins! ',\n",
       "   'Based ',\n",
       "   'Dignity is *so* overrated',\n",
       "   'Yeah but one piece is not nearly as iconic, nor did it have wonderful artwork done in the original run. Og db stands the test of time, it still looks good. Trim some filler? For sure. Redo those backgrounds and old school animation? Nah. \\n\\nDaima looks fun, Im not mad about it.',\n",
       "   'Mental effort? ',\n",
       "   'Its pretty hardcore, they dont show you much in game and they leave you to figure it out. I think the monks in candlekeep might tell you some basics on how magic works, but nothing they give away is very helpful in the long run. \\n\\nOnce youve played through it a few times you start to memorize how fights will go and you come up with novel strategies. Its actually pretty satisfying to show up to a big fight with a wonky crew and kick ass all the same.',\n",
       "   'Isnt this track from wire? On chairs missing?',\n",
       "   'I like that it can be done, although I doubt Ill ever bother  Id just as soon set xp with console \\u200d but yours is a fun experiment / exhibition of whats possible in engine, so top marks ',\n",
       "   'And here I am still waiting for my hoverboard! Screw you, Spielberg ',\n",
       "   'In planescape: torment there are devils (abishai) wandering about sigil getting along ok \\u200d they get pissed off if you chat with them for too long, but I think theyre doing alright for themselves.',\n",
       "   'Solved!',\n",
       "   'Thank you! Brilliant, well done ',\n",
       "   'Necessary comment',\n",
       "   'Fingolfin Baldur was a great point guard',\n",
       "   'If you think in terms of a year, plant a seed; if in terms of ten years, plant trees; if in terms of 100 years, teach the people.\\n\\nConfucius',\n",
       "   'Loool joined',\n",
       "   'Youre gonna need to stock up on darts!',\n",
       "   'No one should really try this combo out, its redundant and purposefully overkill  a successful blindness spell is more than enough for a single target encounter.',\n",
       "   'You gotta cast grease, blindness, and sleep on the wolf. Then you throw darts at it for 20 min.',\n",
       "   'Needs more upvotes ',\n",
       "   'Ah, one of *those* gate shops ',\n",
       "   'Informed consumers are the best consumers ',\n",
       "   'Pretty sure r2 is actually cooking up some popcorn in this scene: luke and leia asked for sweet popcorn while they watch the hutts die.',\n",
       "   'The cum accelerates',\n",
       "   'Tu casa es aqui, mantequilla!']},\n",
       " {'username': 'MikGusta',\n",
       "  'date': '12/20/23',\n",
       "  'time': '03:54 AM',\n",
       "  'comments': ['Mid',\n",
       "   'Thats better than what my dad and I have rn. I built a lego bonsai and thats all we got lmao',\n",
       "   'I  DONT  TRUST  LADDERS ',\n",
       "   'I dont like how uncentered it is',\n",
       "   'Its not a good convo unless I end up upside down in my chair or sofa.',\n",
       "   'Same. My grandma is 83 and in a rehabilitation center. She broke her femur, had a stroke, her right arm became paralyzed, and was just diagnosed with dementia. We were just told she was never going to be able to come home because she cant walk or take care of herself at all. We have her funeral paid for in advance so Medicaid doesnt take all her money before we could plan that.',\n",
       "   'My dad was 7, just about to turn 8, years old when we landed on the moon. He was sitting in his living room with his little sister and watched the landing on tv. I told him about this conspiracy once and he just said, No.',\n",
       "   '![gif](giphy|2jGSqfGQKUyDzmp9oa)',\n",
       "   'Anxious Roast Beef Sandwich\\n\\n',\n",
       "   'Slotherhouse \\n\\nIts about a sloth going on a killing spree',\n",
       "   'I remember going to see that in theaters. My dad is like Matt Damons character and finds disco annoying but I love it. So I started singing along while getting my jacket and my dad goes, Whats wrong with you? ',\n",
       "   'So I just checked WebMD and I think he has cancer. ',\n",
       "   'https://preview.redd.it/yw0m1it31r6c1.jpeg?width=3024&format=pjpg&auto=webp&s=3d421a9e337953fd9b10b244a0e5b5f32270d79a\\n\\nHe loves Christmas deliveries',\n",
       "   'Ive stared at similar images when getting ct scans. Last one I saw was a beautiful tree in autumn. But yeah, extremely boring.',\n",
       "   'This reminds me of the time my dad said Im hungry and I got so excited I just yelled HI HUNGRY IM MIKA!',\n",
       "   'Nice',\n",
       "   'All the pretty girls like Samuel. Oh he really doesnt share.',\n",
       "   'Is that necessary?',\n",
       "   'https://preview.redd.it/e2b29n4ime6c1.jpeg?width=3024&format=pjpg&auto=webp&s=fd5a88f29013799c4fb449c6a909fe1591b0990c\\n\\nRemember to wash your paws!',\n",
       "   'Oh god. If I absolutely had to choose I guess Id say Parachutes but I really like their first 7 albums equally. The last two took some getting used to but I love them now too.',\n",
       "   'That all of his songs are pure bangers and he is absolutely not a one hit wonder. I know Im not alone in this though.',\n",
       "   'A kid was asking for a Thesaurus but didnt know how to say it. He asked for a Thesaurasorus.',\n",
       "   'Im so glad my crazy mom didnt put me in a Christian school',\n",
       "   'I was just looking for tickets in WI in August of next year and I would have to pay like $300 for one ticket because of scalpers',\n",
       "   'Cuttlefish are aliens?',\n",
       "   'Finally a poster includes their dream address!! Im totally gonna check it out! That bathroom is magical!',\n",
       "   'That was a fuckin jump scare ',\n",
       "   'Grandmother Willow is looking rough',\n",
       "   'Theres a handful of bits they do that I dont personally find funny but I like to hear them laugh.']}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data['users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'BantedHam', 'date': '01/24/24', 'time': '03:52 AM', 'comments': ['This is cool']}\n"
     ]
    }
   ],
   "source": [
    "user_data = next((user for user in json_data['users'] if user['username'] == 'BantedHam'), None)\n",
    "\n",
    "user_data['comments'].append('This is cool')\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in file on desktop\n",
    "import os\n",
    "import json\n",
    "desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "output_file_path = os.path.join(desktop_path, 'json_data.json')\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(json_data, output_file, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNxEulALZ5aabXFI6KzsUCw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03444d6dacbd41a7a330d88f59f36a0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "041c3964da5e4e9d9efca42fc1954682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4fc76271408443b8a93df3c3be6c83f1",
       "IPY_MODEL_795325372d114e0c8cc1e0ace2960157",
       "IPY_MODEL_aaca48d8c396489b8ea52fd3470b0398"
      ],
      "layout": "IPY_MODEL_b2a177a76c38410c9259fd56100b795a"
     }
    },
    "13b9e96e48014aaa906a3fcf71c7c587": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ac16ae47ca346bda57576a73fbc0af9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b9ac9ec1f2f4f4ebd24d5053a6ae629": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c48c5032f204b56b8ad1956711898aa",
      "placeholder": "",
      "style": "IPY_MODEL_d841fecffcf64b039e4c71daf487424c",
      "value": "config.json: 100%"
     }
    },
    "1e7913c384584213988ceb6a1c55e6f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "225fa135e9db42e0b80aa3e08ba793c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2517147907d34e1f834bb125083352af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_225fa135e9db42e0b80aa3e08ba793c4",
      "placeholder": "",
      "style": "IPY_MODEL_60f0b21eed3e454fa558c76de1f7deb6",
      "value": "vocab.txt: 100%"
     }
    },
    "2592064fe24d415eb78b4040c86a2564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb7965a426794118b711e8ff49edd42a",
       "IPY_MODEL_fb39abcadc94494abf05b8dcc47a5a13",
       "IPY_MODEL_2cda3feb8d674332965c23888b76403b"
      ],
      "layout": "IPY_MODEL_9178d42c5ed74afd9b7385c73f904ce9"
     }
    },
    "2c48c5032f204b56b8ad1956711898aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cda3feb8d674332965c23888b76403b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7adcfbc0562488f9512a5d64c2f8570",
      "placeholder": "",
      "style": "IPY_MODEL_7a1c4a816da447e68cf42ea62207334d",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.73kB/s]"
     }
    },
    "3047f874f7054f5e96c68ee5e9534d30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3801bd5c4dee4810a7eb8840238d1886": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "417f8ba232714811928c736e617530cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f69ad8d59be40e18a1d6d25cc047761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fc76271408443b8a93df3c3be6c83f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b55b21ddc804f6e9480663302481ceb",
      "placeholder": "",
      "style": "IPY_MODEL_fbf9f26329c749148657dafa229843fc",
      "value": "tokenizer.json: 100%"
     }
    },
    "5ea4d137498a41bea805495d1201826f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc606197bb9c4c12bc0d9f0e9b8a2740",
      "placeholder": "",
      "style": "IPY_MODEL_eb394e2ce4924daf9e504dbfae49bd15",
      "value": " 232k/232k [00:00&lt;00:00, 4.72MB/s]"
     }
    },
    "60f0b21eed3e454fa558c76de1f7deb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "635197abb4f84e628cb123fc492ea067": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b9ac9ec1f2f4f4ebd24d5053a6ae629",
       "IPY_MODEL_fa8c3535774f4905867ade9e58d86d24",
       "IPY_MODEL_bec1aa75071044938f19dea5b809321e"
      ],
      "layout": "IPY_MODEL_1ac16ae47ca346bda57576a73fbc0af9"
     }
    },
    "72eff2bb1c524cf6974e6d6582503b6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "795325372d114e0c8cc1e0ace2960157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7086473aead4d689312917764fe307a",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03444d6dacbd41a7a330d88f59f36a0e",
      "value": 466062
     }
    },
    "7a1c4a816da447e68cf42ea62207334d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7affc0ab30d546e081711abe8b8e0bc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b55b21ddc804f6e9480663302481ceb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9178d42c5ed74afd9b7385c73f904ce9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96db522da48c4f84bb974936f371b8a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7086473aead4d689312917764fe307a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7adcfbc0562488f9512a5d64c2f8570": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7dcb913bf234d7d9c5b6a281f8a984b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaca48d8c396489b8ea52fd3470b0398": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96db522da48c4f84bb974936f371b8a0",
      "placeholder": "",
      "style": "IPY_MODEL_4f69ad8d59be40e18a1d6d25cc047761",
      "value": " 466k/466k [00:00&lt;00:00, 6.73MB/s]"
     }
    },
    "ae081453cf034841b8e3ab2014913039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6e9bf2c66b449b688c9046a0c3ae5db",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3801bd5c4dee4810a7eb8840238d1886",
      "value": 231508
     }
    },
    "b2a177a76c38410c9259fd56100b795a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bec1aa75071044938f19dea5b809321e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6bfbaf3a5df494b871a13917185a4f0",
      "placeholder": "",
      "style": "IPY_MODEL_1e7913c384584213988ceb6a1c55e6f0",
      "value": " 483/483 [00:00&lt;00:00, 34.5kB/s]"
     }
    },
    "c6e9bf2c66b449b688c9046a0c3ae5db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d01f9c0cd8454e19a3bbaaf094c8b781": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6bfbaf3a5df494b871a13917185a4f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d841fecffcf64b039e4c71daf487424c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb394e2ce4924daf9e504dbfae49bd15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa8c3535774f4905867ade9e58d86d24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d01f9c0cd8454e19a3bbaaf094c8b781",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3047f874f7054f5e96c68ee5e9534d30",
      "value": 483
     }
    },
    "fb39abcadc94494abf05b8dcc47a5a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7affc0ab30d546e081711abe8b8e0bc0",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_72eff2bb1c524cf6974e6d6582503b6a",
      "value": 28
     }
    },
    "fb7965a426794118b711e8ff49edd42a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7dcb913bf234d7d9c5b6a281f8a984b",
      "placeholder": "",
      "style": "IPY_MODEL_417f8ba232714811928c736e617530cf",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "fbf9f26329c749148657dafa229843fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc606197bb9c4c12bc0d9f0e9b8a2740": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe3cf435b53e4bca83ef333de8660be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2517147907d34e1f834bb125083352af",
       "IPY_MODEL_ae081453cf034841b8e3ab2014913039",
       "IPY_MODEL_5ea4d137498a41bea805495d1201826f"
      ],
      "layout": "IPY_MODEL_13b9e96e48014aaa906a3fcf71c7c587"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
